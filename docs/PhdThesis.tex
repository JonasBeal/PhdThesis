\documentclass[a4paper,12pt,twoside,onecolumn,openright,final,oldfontcommands]{memoir}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={From the mechanistic modeling of signaling pathways in cancer to the interpretation of models and their contributions: clinical applications and statistical evaluation},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{plainnat}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Start preamble
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{./cover/psl-cover} % specifies the path to the cover page template
%\usepackage{psl-cover} % specifies the path to the cover page template

%\makeatletter
%\def\thm@space@setup{%
%  \thm@preskip=8pt plus 2pt minus 4pt
%  \thm@postskip=\thm@preskip
%}
%\makeatother


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Add a lettrine to the very first character of the content %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{lettrine} % supports various dropped capitals styles

\newcommand{\initial}[1]{
	\lettrine[lines=3,lhang=0.33,nindent=0em]{
		\color{gray}
     		{\textsc{#1}}}{}}
     		

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Links box
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=blue,      
    urlcolor=blue,
    citecolor=black
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Text box
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\usepackage{xcolor}
%\usepackage[dvipsnames]{xcolor}
%\PassOptionsToPackage{usenames,dvipsnames}{xcolor}
\usepackage{xcolor}
\definecolor{steelblue}{rgb}{0.27, 0.51, 0.71}
\definecolor{maroon}{rgb}{0.5, 0.0, 0.0}
\definecolor{indianred}{rgb}{0.8, 0.36, 0.36}

\usepackage{tcolorbox}
\newtcolorbox{summarybox}{
  colback= black!10!white,
  colframe=steelblue,
  coltext=black,
  boxsep=5pt,
  arc=4pt}
  
\newtcolorbox{conclubox}{
  colback= black!10!white,
  colframe=indianred,
  coltext=black,
  boxsep=5pt,
  arc=4pt}
     		
%%%%%%%%%%%%%%%%%%%%%%%%
% Insert an empty page %
%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{afterpage} % executes command after the next page break

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    % \addtocounter{page}{-1}% % uncomment to increase page counter
    \newpage
    }

\newcommand{\clearemptydoublepage}{\newpage{\thispagestyle{empty}\cleardoublepage}}

%%%%%%%%%%%%%%%%%%
% Epigraph style %
%%%%%%%%%%%%%%%%%%

\usepackage{epigraph} % provides commands to assist in the typesetting of a single epigraph

\setlength\epigraphwidth{1\textwidth}
\setlength\epigraphrule{0pt} % no line between
\setlength\beforeepigraphskip{1\baselineskip} % space before and after epigraph
\setlength\afterepigraphskip{2\baselineskip}
\renewcommand*{\textflush}{flushright}
\renewcommand*{\epigraphsize}{\normalsize\itshape}

%%%%%%%%%%%%%%%%%%%%%%%%
% Formatting
%%%%%%%%%%%%%%%%%%%%

\usepackage{calc} % simple arithmetics in latex commands
\usepackage{soul} % hyphenation for letterspacing, underlining, etc.
\renewcommand{\topfraction}{.85}
\setlength{\parskip}{1em}

\makeatletter
\newlength\dlf@normtxtw
\setlength\dlf@normtxtw{\textwidth}
\newsavebox{\feline@chapter}
\newcommand\feline@chapter@marker[1][4cm]{%
	\sbox\feline@chapter{%
		\resizebox{!}{#1}{\fboxsep=1pt%
			\colorbox{gray}{\color{white}\thechapter}%
		}}%
		\rotatebox{90}{%
			\resizebox{%
				\heightof{\usebox{\feline@chapter}}+\depthof{\usebox{\feline@chapter}}}%
			{!}{\scshape\so\@chapapp}}\quad%
		\raisebox{\depthof{\usebox{\feline@chapter}}}{\usebox{\feline@chapter}}%
}

\newcommand\feline@chm[1][4cm]{%
	\sbox\feline@chapter{\feline@chapter@marker[#1]}%
	\makebox[0pt][c]{% aka \rlap
		\makebox[1cm][r]{\usebox\feline@chapter}%
	}}

\makechapterstyle{daleifmodif}{
\renewcommand\chapnamefont{\normalfont\Large\scshape\raggedleft\so}
\renewcommand\chaptitlefont{\normalfont\Large\bfseries\scshape}
\renewcommand\chapternamenum{} \renewcommand\printchaptername{}
\renewcommand\printchapternum{\null\hfill\feline@chm[2.5cm]\par}
\renewcommand\afterchapternum{\par\vskip\midchapskip}
\renewcommand\printchaptertitle[1]{\color{gray}\chaptitlefont\raggedleft
  \renewcommand\chaptername{Chapter}
  ##1\par}
}

\makeatother
\chapterstyle{daleifmodif}

% The pages should be numbered consecutively at the bottom centre of the page
\makepagestyle{myvf}
\makeoddfoot{myvf}{}{\thepage}{}
\makeevenfoot{myvf}{}{\thepage}{}
\makeheadrule{myvf}{\textwidth}{\normalrulethickness}
\makeevenhead{myvf}{\small\textsc{\leftmark}}{}{}
\makeoddhead{myvf}{}{}{\small\textsc{\rightmark}}
\pagestyle{myvf}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Define cover page settings
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{My title}

\author{Jonas BEAL}

\institute{l'Institut Curie}
\doctoralschool{Complexite du Vivant}{515}

\institute{l'Institut Curie}
\doctoralschool{Complexité du Vivant}{515}
\specialty{Génomique}
\date{23 septembre 2020}

%% cotutelle
% \entitle{Thesis Subject in English}
% \otherinstitute{CEA Saclay}
% \logootherinstitute{logo-institute}

\jurymember{1}{Adeline LECLERQ-SAMSON}{Professeur, Université Grenoble Alpes}{Rapporteur}
\jurymember{2}{Lodewyk WESSELS}{Professeur, Netherlands Cancer Institute}{Rapporteur}
\jurymember{3}{Émilie LANOY}{Ingénieure de recherche, Gustave Roussy}{Examinateur}
\jurymember{4}{Denis THIEFFRY}{Professeur, ENS, PSL}{Examinateur}
\jurymember{5}{Emmanuel BARILLOT}{Directeur de Recherche, Institut Curie, PSL}{Directeur de thèse}
\jurymember{6}{Aurélien LATOUCHE}{Professeur, Institut Curie, Cnam}{Directeur de thèse}
\jurymember{7}{Laurence CALZONE}{Ingénieure de recherche, Institut Curie, PSL}{Membre invitée, co-encadrante de thèse}


\frabstract{
  Au delà de ses mécanismes génétiques, le cancer peut être compris comme une maladie de réseaux qui résulte souvent de l’interaction entre différentes perturbations dans un réseau de régulation cellulaire.  La dynamique de ces réseaux et des voies de signalisation associées est complexe et requiert des approches intégrées. Une d’entre elles est la conception de modèles dits mécanistiques qui traduisent mathématiquement la connaissance biologique des réseaux afin de pouvoir simuler le fonctionnement moléculaire des cancers informatiquement. Ces modèles ne traduisent cependant que les mécanismes généraux à l’oeuvre dans certains cancers en particulier.  
  

Cette thèse propose en premier lieu de définir des modèles mécanistiques personnalisés de cancer. Un modèle générique est  d’abord défini dans un formalisme logique (ou Booléen), avant d’utiliser les données omiques (mutations, ARN, protéines) de patients ou de lignées cellulaires afin de rendre le modèle spécifique à chacun. Ces modèles personnalisés peuvent ensuite être confrontés aux données cliniques de patients pour vérifier leur validité. Le cas de la réponse clinique aux traitements est exploré en particulier dans ce travail. La représentation explicite des mécanismes moléculaires par ces modèles permet en effet de simuler l’effet de différents traitements suivant leur mode d’action et de vérifier si la sensibilité d’un patient à un traitement est bien prédite par le modèle personnalisé correspondant. Un exemple concernant la réponse aux inhibiteurs de BRAF dans les mélanomes et cancers colorectaux est ainsi proposé.  
  

La confrontation des modèles mécanistiques de cancer, ceux présentés dans cette thèse et d’autres, aux données cliniques incite par ailleurs à évaluer rigoureusement leurs éventuels bénéfices dans la cadre d’une utilisation médicale. La quantification et l’interprétation de la valeur pronostique des biomarqueurs issus de certains modèles méchanistiques est brièvement présentée avant de se focaliser sur le cas particulier des modèles capables de sélectionner le meilleur traitement pour chaque patient en fonction des ses caractéristiques moléculaires. Un cadre théorique est proposé pour étendre les méthodes d’inférence causale à l’évaluation de tels algorithmes de médecine de précision. Une illustration est fournie à l’aide de données simulées et de xénogreffes dérivées de  patients.  
  

L’ensemble des méthodes et applications décrites tracent donc un chemin, de la conception de modèles mécanistiques de cancer à leur évaluation grâce à des modèles statistiques émulant des essais cliniques, proposant ainsi un cadre pour la mise en oeuvre de la médecine de précision en oncologie.

}

\enabstract{
  Beyond its genetic mechanisms, cancer can be understood as a network disease that often results from the interactions between different perturbations in a cellular regulatory network.  The dynamics of these networks and associated signaling pathways are complex and require integrated approaches. One approach is to design mechanistic models that translate the biological knowledge of networks in mathematical terms to simulate computationally the molecular features of cancers. However, these models only reflect the general mechanisms at work in cancers.  
  

This thesis proposes to define personalized mechanistic models of cancer. A generic model is first defined in a logical (or Boolean) formalism, before using omics data (mutations, RNA, proteins) from patients or cell lines in order to make the model specific to each one profile. These personalized models can then be compared with the clinical data of patients in order to validate them. The response to treatment is investigated in particular in this thesis. The explicit representation of the molecular mechanisms by these models allows to simulate the effect of different treatments according to their targets and to verify if the sensitivity of a patient to a drug is well predicted by the corresponding personalized model. An example concerning the response to BRAF inhibitors in melanomas and colorectal cancers is thus presented.  
  

The comparison of mechanistic models of cancer, those presented in this thesis and others, with clinical data also encourages a rigorous evaluation of their possible benefits in the context of medical use. The quantification and interpretation of the prognostic value of outputs of some mechanistic models is briefly presented before focusing on the particular case of models able to recommend the best treatment for each patient according to his molecular profile. A theoretical framework is defined to extend causal inference methods to the evaluation of such precision medicine algorithms. An illustration is provided using simulated data and patient derived xenografts.  
  

All the methods and applications put forward a possible path from the design of mechanistic models of cancer to their evaluation using statistical models emulating clinical trials. As such, this thesis provides one framework for the implementation of precision medicine in oncology.

}

\frkeywords{ Modélisation, Cancer, Modèle mécanistique, Biostatistiques, Inférence causale, Médecine de précision.}
\enkeywords{ Modeling, Cancer, Mechanistic model, Biostatistics, Causal inference, Precision medicine.}
  
\pagenumbering{roman}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\title{From the mechanistic modeling of signaling pathways in cancer to the
interpretation of models and their contributions: clinical applications
and statistical evaluation}
\date{23/09/2020}

\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

\vspace*{\fill}

\begin{center}
\begin{quote}

\emph{Ce sont les sévères artistes}\\
\emph{Que l’aube attire à ses blancheurs,}\\
\emph{Les savants, les inventeurs tristes,}\\
\emph{Les puiseurs d’ombre, les chercheurs,}\\
\emph{Qui ramassent dans les ténèbres}\\
\emph{Les faits, les chiffres, les algèbres,}\\
\emph{Le nombre où tout est contenu,}\\
\emph{Le doute où nos calculs succombent,}\\
\emph{Et tous les morceaux noirs qui tombent}\\
\emph{Du grand fronton de l’inconnu !}\\
Victor Hugo (\emph{Les mages})

\end{quote}
\end{center}

\vfill

\chapter*{Abstract}

\initial{B}eyond its genetic mechanisms, cancer can be understood as a
network disease that often results from the interactions between
different perturbations in a cellular regulatory network. The dynamics
of these networks and associated signaling pathways are complex and
require integrated approaches. One approach is to design mechanistic
models that translate the biological knowledge of networks in
mathematical terms to simulate computationally the molecular features of
cancers. However, these models only reflect the general mechanisms at
work in cancers.

This thesis proposes to define personalized mechanistic models of
cancer. A generic model is first defined in a logical (or Boolean)
formalism, before using omics data (mutations, RNA, proteins) from
patients or cell lines in order to make the model specific to each one
profile. These personalized models can then be compared with the
clinical data of patients in order to validate them. The response to
treatment is investigated in particular in this thesis. The explicit
representation of the molecular mechanisms by these models allows to
simulate the effect of different treatments according to their targets
and to verify if the sensitivity of a patient to a drug is well
predicted by the corresponding personalized model. An example concerning
the response to BRAF inhibitors in melanomas and colorectal cancers is
thus presented.

The comparison of mechanistic models of cancer, those presented in this
thesis and others, with clinical data also encourages a rigorous
evaluation of their possible benefits in the context of medical use. The
quantification and interpretation of the prognostic value of outputs of
some mechanistic models is briefly presented before focusing on the
particular case of models able to recommend the best treatment for each
patient according to his molecular profile. A theoretical framework is
defined to extend causal inference methods to the evaluation of such
precision medicine algorithms. An illustration is provided using
simulated data and patient derived xenografts.

All the methods and applications put forward a possible path from the
design of mechanistic models of cancer to their evaluation using
statistical models emulating clinical trials. As such, this thesis
provides one framework for the implementation of precision medicine in
oncology.

\vspace{\baselineskip}

\textbf{Key-words}: Modeling, Cancer, Mechanistic model, Biostatistics,
Causal inference, Precision medicine

\chapter*{Résumé}

\initial{A}u delà de ses mécanismes génétiques, le cancer peut être
compris comme une maladie de réseaux qui résulte souvent de
l'interaction entre différentes perturbations dans un réseau de
régulation cellulaire. La dynamique de ces réseaux et des voies de
signalisation associées est complexe et requiert des approches
intégrées. Une d'entre elles est la conception de modèles dits
mécanistiques qui traduisent mathématiquement la connaissance biologique
des réseaux afin de pouvoir simuler le fonctionnement moléculaire des
cancers informatiquement. Ces modèles ne traduisent cependant que les
mécanismes généraux à l'oeuvre dans certains cancers en particulier.

Cette thèse propose en premier lieu de définir des modèles mécanistiques
personnalisés de cancer. Un modèle générique est d'abord défini dans un
formalisme logique (ou Booléen), avant d'utiliser les données omiques
(mutations, ARN, protéines) de patients ou de lignées cellulaires afin
de rendre le modèle spécifique à chacun. Ces modèles personnalisés
peuvent ensuite être confrontés aux données cliniques de patients pour
vérifier leur validité. Le cas de la réponse clinique aux traitements
est exploré en particulier dans ce travail. La représentation explicite
des mécanismes moléculaires par ces modèles permet en effet de simuler
l'effet de différents traitements suivant leur mode d'action et de
vérifier si la sensibilité d'un patient à un traitement est bien prédite
par le modèle personnalisé correspondant. Un exemple concernant la
réponse aux inhibiteurs de BRAF dans les mélanomes et cancers
colorectaux est ainsi proposé.

La confrontation des modèles mécanistiques de cancer, ceux présentés
dans cette thèse et d'autres, aux données cliniques incite par ailleurs
à évaluer rigoureusement leurs éventuels bénéfices dans la cadre d'une
utilisation médicale. La quantification et l'interprétation de la valeur
pronostique des biomarqueurs issus de certains modèles méchanistiques
est brièvement présentée avant de se focaliser sur le cas particulier
des modèles capables de sélectionner le meilleur traitement pour chaque
patient en fonction des ses caractéristiques moléculaires. Un cadre
théorique est proposé pour étendre les méthodes d'inférence causale à
l'évaluation de tels algorithmes de médecine de précision. Une
illustration est fournie à l'aide de données simulées et de xénogreffes
dérivées de patients.

L'ensemble des méthodes et applications décrites tracent donc un chemin,
de la conception de modèles mécanistiques de cancer à leur évaluation
grâce à des modèles statistiques émulant des essais cliniques, proposant
ainsi un cadre pour la mise en oeuvre de la médecine de précision en
oncologie.

\vspace{\baselineskip}

\textbf{Mots-clés}: Modélisation, Cancer, Modèle mécanistique,
Biostatistiques, Inférence causale, Médecine de précision

\afterpage{\blankpage}

\chapter*{Remerciements}

\initial{O}n ne se lance pas dans une thèse dans le seul but d'en écrire
les remerciements. C'est peut être un tort tant l'occasion est belle
d'en profiter pour exprimer sa gratitude à tous ceux qui la méritent
sans que l'on pense toujours à le leur dire. Je vais donc essayer de
remercier tous ceux qui occupent mon esprit à l'heure où j'écris ces
lignes, d'un bout à l'autre du spectre, de ceux sans qui cette thèse
n'aurait pas été la même à ceux sans qui je serais bien différent.

Merci tout d'abord aux membres du jury, Denis Thieffry, Émilie Lanoy, et
en particulier aux rapporteurs Adeline Leclerq-Samson et Lodewyk
Wessels, d'avoir accepté d'évaluer ce travail, contribuant ainsi à le
rendre plus enrichissant pour moi. Merci également aux membres de mes
comités de suivi de thèse Benno Schwikowski, Mélanie Prague et Xavier
Paoletti pour leur regard extérieur et leurs remarques pertinentes et
constructives.

Mes remerciements suivants, à la fois professionnels et personnels vont
à Laurence, Aurélien et Emmanuel pour avoir supervisé cette thèse en
m'orientant sans me diriger, dans un juste équilibre d'encadrement et de
liberté, chacun suivant ses spécificités. Merci en particulier à
Laurence pour son inépuisable bienveillance, prodiguée avec générosité.
Merci à Aurélien pour sa disponibilité et sa positivité, mot piégé s'il
en est. Et enfin merci à Emmanuel pour sa capacité à prendre du recul.

Merci ensuite à tous les collègues de l'équipe de biologie des systèmes,
présents ou passés, particulièrement ceux avec lesquels j'ai pu
collaborer directement. Merci à Arnau pour m'avoir inoculé son souci
excessif de certains détails, à Vincent pour m'avoir enseigné certaines
bonnes pratiques, à Pauline pour m'avoir accueilli à l'Institut et à
Lorenzo pour m'avoir supporté. Merci plus largement à tous les membres
de l'équipe qui ont fait de cette thèse une période d'échanges,
scientifiques ou non : Nicolas, Mihaly, Cristobal, Andrei, Urczula,
Céline, Laura, Loredana, Jane, Om, Marianyela, Christine, Maria, Inna.
Mention spéciale à mes collègues de bureau successifs Loïc et Jonathan
pour leur humour bienvenu et pour avoir toléré le mien. Merci de la même
manière aux collègues de l'équipe STAMPM, tout spécialement à Bassirou
et Alessandra pour m'avoir accueilli dans leur bureau mouvant de
Saint-Cloud, mais aussi Christophe et Xavier (à nouveau) pour leur
écoute et leurs conseils. Merci aux autres membres de l'U900, avec une
pensée particulière pour Caroline, Kati et Yasmina dont le sens de
l'organisation a contribué à me faciliter la vie.

Merci également à tous ces collègues inconnus qui partagent librement
connaissances, tutoriels, code informatique ou modèles typographiques.
Ils forment une communauté virtuelle à l'impact bien réel.

Merci aussi à ceux qui ont contribué à orienter mon parcours au fil des
années, de Polytechnique à la biologie computationnelle. Je pense
notamment à François et Fred chez Novadiscovery ainsi qu'à Vassily et
Misko à l'EPFL.

De manière toujours plus personnelle, je me dois aussi de remercier tous
les amis qui ont fait de ces trois années un voyage agréable. Au sein de
l'Institut tout d'abord, pour quelques repas avec Élise et Anne, au
Collège des Ingénieurs, pour des discussions intellectuellement variées
et rafraichissantes. Merci ensuite à tous ces amis, connus à
Saint-Étienne, Lyon, Palaiseau, Paris ou Lausanne, que se reconnaitront
sans qu'il soit nécessaire de tous les nommer exhaustivement. Mention
spéciale à Guillaume, Laurent, Reda, Michaël, Pierre, Etienne, Florian
et Amélie pour m'avoir aidé ou réjoui plus souvent qu'à leur tour.

Une grande partie de la gratitude restante à ce stade revient à ma
famille pour son soutien sans faille. Merci à mes parents, frère, soeur
et grand-mère pour m'avoir accompagné tout au long de mes études de leur
curiosité et de leur fierté, autant de vent dans mes voiles sans lequel
je n'aurais pas tant avancé. Merci tout spécialement à mes parents, je
dois à votre tranquille mais tenace confiance de n'avoir jamais douté de
ma capacité à choisir ma voie et à la tracer avec succès.

Enfin merci à Honorine pour tout le reste, c'est à dire pour
l'essentiel. Si ``\emph{on ne possède d'un être que ce qu'on change en
lui}''\footnote{André Malraux, \emph{La condition humaine}}, ne doute
jamais qu'une large portion de ce travail t'appartient. Son auteur quant
à lui ne s'appartient déjà plus tout à fait\ldots{}

\clearemptydoublepage

\chapter*{Preface}

\initial{T}he understanding of cancer has progressed dramatically in
recent decades, fueled in particular by the contribution of increasingly
precise and abundant biological data. These advances have taken cancer
modeling from theory to practice: it is now possible to simulate the
molecular mechanisms of cancer in great detail. \textbf{But are cancer
models more than objects of scientific investigation? Can they be
considered as patient-oriented clinical tools?} These questions are at
the heart of the present thesis which will focus on specific and limited
examples to bring some partial answers to these general questions. The
following chapters will thus propose \textbf{a comprehensive journey,
from the design of a cancer model to its clinical application}.

In more detail, this thesis is structured in three parts, each
subdivided into three chapters. Since the whole thesis is about cancer
modeling, the first part aims at defining the type of model to be
referred to, and in particular models that will be called mechanistic,
as well as the object of the modeling, \emph{i.e.}, the molecular
networks involved in cancer. So the first part answers the question:
\textbf{what is a cancer model and what is its purpose?} It is thus
essentially an \textbf{introduction}, intended to outline the concepts
and objects studied afterwards.

The second part will be devoted to the methods developed during this
thesis to transform qualitative models of molecular networks, known as
logical models, into personalized models that can be interpreted
clinically. In short, \textbf{how can a mathematical representation of
biological knowledge be transformed into a tool that contributes to the
understanding of the clinical manifestations of cancer?} This part
therefore focuses on the first part of the journey, which is
\textbf{systems biology}, starting from the biological question and
seeking to model it while beginning to evaluate the clinical
perspectives of such a model.

Finally, the third and last part will look at how the clinical relevance
of all the above-mentioned models can be rigorously evaluated, both in
their ability to predict the evolution of the disease and in their
ability to recommend the most appropriate treatments for each patient.
\textbf{How to quantify and interpret the value of the clinical
information delivered by these models?} So this is the last part of the
path, where the cancer model is considered to be complete, and the aim
is to quantify its clinical relevance, using \textbf{statistical
methods}.

As for the form of this thesis, it exists both in PDF format and in an
online HTML version
(\url{https://jonasbeal.github.io/files/PhdThesis/}). The two versions
are strictly identical but the second one contains some additional
interactive graphs or applications. Both documents have been generated
with R from the same source files that also include the data and code
required for the provided figures\footnote{The thesis document, both in
  its PDF version and its online HTML version, is based on the R package
  \emph{bookdown} and inspired by the architecture proposed by Ladislas
  Nalborczyk in his
  \href{https://github.com/lnalborczyk/phd_thesis}{GitHub repository}}.
All materials are available on the dedicated GitHub repository
(\url{https://github.com/JonasBeal/PhdThesis}).

\BeginKnitrBlock{summarybox}
\subsubsection*{Scientific content}\label{scientific-content}
\addcontentsline{toc}{subsubsection}{Scientific content}

Except for the first part, essentially introductory and based on
scientific literature, the different chapters are based on original
scientific work done during this thesis (2017-2020) and mentioned at the
beginning of each chapter in a box similar to the this one.

The main articles behind this thesis are indicated below with one
published article and two pre-prints currently under review:

\begin{itemize}
\tightlist
\item
  Béal, Jonas, Arnau Montagud, Pauline Traynard, Emmanuel Barillot, and
  Laurence Calzone. ``Personalization of logical models with multi-omics
  data allows clinical stratification of patients.'' Frontiers in
  physiology 9 (2019): 1965.
  \href{https://www.frontiersin.org/articles/10.3389/fphys.2018.01965/full}{Link}.
\item
  Béal, Jonas, Lorenzo Pantolini, Vincent Noël, Emmanuel Barillot, and
  Laurence Calzone. ``Personalized logical models to investigate cancer
  response to BRAF treatments in melanomas and colorectal cancers.''
  bioRxiv (2020).
  \href{https://www.biorxiv.org/content/10.1101/2020.05.27.119016v2}{Link}.
\item
  Béal, Jonas, and Aurélien Latouche. ``Causal inference with multiple
  versions of treatment and application to personalized medicine.''
  arXiv preprint arXiv:2005.12427 (2020).
  \href{https://arxiv.org/abs/2005.12427}{Link}.
\end{itemize}

These three articles were described or completed in oral presentations,
respectively in International Conference of Systems Biology 2018,
conference on Intelligent Systems for Molecular Biology (ISMB/ECCB 2019,
\href{https://www.youtube.com/watch?v=6EMBycoR0Ow}{Video}) and
conference of International Society of Clinical Biostatistics (ISCB41
2020).
\EndKnitrBlock{summarybox}

\clearemptydoublepage

\renewcommand{\contentsname}{Table of contents}

\maxtocdepth{subsection}

\tableofcontents*
\addtocontents{toc}{\par\nobreak \mbox{}\hfill{\bf Page}\par\nobreak}
\newpage

\listoftables
\addtocontents{lot}{\par\nobreak\textbf{{\scshape Table} \hfill Page}\par\nobreak}
\newpage

\listoffigures
\addtocontents{lof}{\par\nobreak\textbf{{\scshape Figure} \hfill Page}\par\nobreak}
\newpage

\blankpage

\pagenumbering{arabic}

\part{Cells and their
models}\label{part-cells-and-their-models}

\chapter{Scientific modeling: abstract the
complexity}\label{scientific-modeling-abstract-the-complexity}

\epigraph{"Ce qui est simple est toujours faux. Ce qui ne l'est pas est inutilisable."}{Paul Valéry (Mauvaises pensées et autres, 1942)}

\initial{T}he notion of modeling is embedded in science, to the point
that it has sometimes been used to define the very nature of scientific
research. What is called a model can, however, correspond to very
different realities which need to be defined before addressing the
object of this thesis which will consist, if one wants to be
mischievous, in analyzing models with other models. This semantic
elucidation is all the more necessary as this thesis is
interdisciplinary, suspended between systems biology and biostatistics.
In order to convince the reader of the need for such a preamble, he is
invited to ask a statistician and a biologist how they would define what
a model is.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/orrery} 

}

\caption[A scientist and his model]{\textbf{A scientist and his model.} Joseph Wright
of Derby, \emph{A Philosopher Giving a Lecture at the Orrery (in which a
lamp is put in place of the sun)}, c. 1763-65, oil on canvas, Derby
Museums and Art Gallery}\label{fig:orrery}
\end{figure}






\section{What is a model?}\label{what-is-a-model}

\subsection{In your own words}\label{in-your-own-words}

A model is first of all an ambiguous object and a polysemous word. It
therefore seems necessary to start with a semantic study. Among the many
meanings and synonymous proposed by the dictionary (Figure
\ref{fig:visual-thesaurus}), while some definitions are more related to
art, several find echoes in scientific practice. It is sometimes a
question of the physical representation of an object, often on a reduced
scale as in Figure \ref{fig:orrery}, and sometimes of a theoretical
description intended to facilitate the understanding of the way in which
a system works \citep{dictionnarymodel}. It is even sometimes an ideal
to be reached and therefore an ambitious prospect for an introduction.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/visualThesaurus} 

}

\caption[Network visualization of \emph{model} thesaurus entries]{\textbf{Network visualization of
\emph{model} thesaurus entries.} Generated with the
\href{https://www.visualthesaurus.com}{`Visual Thesaurus'} ressource}\label{fig:visual-thesaurus}
\end{figure}





The narrower perspective of the scientist does not reduce the
completeness of the dictionary's description to an unambiguous object
\citep{bailer2002scientists}. In an attempt to approach these
multi-faceted objects that are the models, Daniela Bailer-Jones
interviewed different scientists and asked them the same question: what
is a model? Across the different profiles and fields of study, the
answers vary but some patterns begin to emerge (Figure
\ref{fig:interviews}). A model must capture the essence of the
phenomenon being studied. Because it eludes, voluntarily or not, many
details or complexity, it is by nature a simplification of the
phenomenon. These limitations may restrict its validity to certain cases
or suspend it to the fulfilment of some hypotheses. They are not
necessarily predictive, but they must be able to generate new
hypotheses, be tested and possibly questioned. Finally, and
fundamentally, they \textbf{must provide insights about the object of
study and contribute to its understanding}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{01-Models_files/figure-latex/interviews-1} 

}

\caption[Scientists talk about their models: words cloud.]{\textbf{Scientists talk about their models:
words cloud.} Cloud of words summarizing the lexical fields used by
scientists to talk about their models in dedicated interviews reported
by \citet{bailer2002scientists}.}\label{fig:interviews}
\end{figure}






These definitions circumscribe the \emph{model} object, its use and its
objectives, but they do not in any way describe its nature. And for good
reason, because even if we agree on the described contours, the
biodiversity of the models remains overwhelming for taxonomists:

\begin{quote}
\emph{Probing models, phenomenological models, computational models,
developmental models, explanatory models, impoverished models, testing
models, idealized models, theoretical models, scale models, heuristic
models, caricature models, exploratory models, didactic models, fantasy
models, minimal models, toy models, imaginary models, mathematical
models, mechanistic models, substitute models, iconic models, formal
models, analogue models, and instrumental models are but some of the
notions that are used to categorize models.}\\
\citep{frigg2020models}
\end{quote}

\subsection{Physical world and world of
ideas}\label{physical-world-and-world-of-ideas}

Without claiming to be exhaustive, we can make a \textbf{first simple
dichotomy between physical/material and formal/intellectual models}
\citep{rosenblueth1945role}. The former consists in replacing the object
of study by another object, just as physical but nevertheless simpler or
better known. These may be models involving a change of scale such as
the simple miniature replica placed in a wind tunnel, or the metal
double helix model used by Watson and Crick to visualize DNA. In all
these cases the model allows to visualize the object of study (Figure
\ref{fig:planets} A and B), to manipulate it and play with it to better
understand or explain a phenomenon, just like the scientist with his
orrery (Figure \ref{fig:orrery}). In the case of biology, there are
mainly model organisms such as drosophila, zebrafish or mice, for
example. We then benefit from the relative simplicity of their genomes,
a shorter time scale or ethical differences, usually to elucidate
mechanisms of interest in humans. Correspondence between the target
system and its model can sometimes be more conceptual, such as that ones
relying on mechanical--electrical analogies: a mechanical system (e.g.~a
spring-mass system) can sometimes be represented by an electric network
(e.g.~a RLC circuit with a resistor, a capacitor and an inductor).

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{01-Models_files/figure-latex/planets-1} 

}

\caption[Orrery, planets and models]{\textbf{Orrery, planets and models}. Physical
models of planetary motion, either geocentric (Armillary sphere from
\emph{Plate LXXVII} in
\href{https://commons.wikimedia.org/wiki/File:EB1711_Armillary_Sphere.png}{\emph{Encyclopedia
Britannica}}, 1771) or heliocentric in panel B (Bion, 1751,
\href{https://gallica.bnf.fr/ark:/12148/btv1b2600252q/f8.item.r=Bion}{catalogue
Bnf}) and some geometric representations by Johannes Kepler in panel C
(in
\href{https://commons.wikimedia.org/wiki/File:Kepler_astronomia_nova.jpg}{\emph{Astronomia
Nova}}, 1609)}\label{fig:planets}
\end{figure}












The model is then no longer simply a mimetic replica but is based on an
intellectual equivalence: we are gradually moving into the realm of
formal models \citep{rosenblueth1945role}. These are of a more symbolic
nature and they \textbf{represent the original system with a set of
logical or mathematical terms}, describing the main driving forces or
similar structural properties as geometrical models of planetary motions
summarized by Kepler in Figure \ref{fig:planets}C. Historically these
models have often been expressed by sets of mathematical equations or
relationships. Increasingly, these have been implemented by computer.
Despite their sometimes less analytical and more numerical nature, many
so-called computational models could also belong to this category of
formal models. There are then many formalisms, discrete or continuous,
deterministic or stochastic, based on differential equations or Boolean
algebra \citep{fowler1997mathematical}. Despite their more abstract
nature, they offer similar scientific services: it is possible to play
with their parameters, specifications or boundary conditions in order to
better understand the phenomenon. One can also imagine these formal
models from a different perspective, which starts from the data in a
bottom-up approach instead of starting from the phenomenon in a top-down
analysis. These models will then often be called statistical models or
models of data \citep{frigg2020models}. This distinction will be further
clarified in section \ref{stat-mech}.

To summarize and continue a little longer with the astronomical
metaphor, the study of a particularly complex system (the solar system)
can be broken down into a variety of different models. Physical and
mechanical models such as armillary spheres (\ref{fig:planets}A and B)
make it possible to touch the object of study. In addition, we can
observe the evolution of models which, when confronted with data, have
progressed from a geocentric to a heliocentric representation to get
closer to the current state of knowledge. Sometimes, models with more
formal representations are used to give substance to ideas and
hypotheses (\ref{fig:planets}C). One of the most conceptual forms is
then the mathematical language and one can thus consider that the
previously mentioned astronomical models find their culmination in
Kepler's equations about orbits, areas and periods that describe the
elliptical motion of the planets. We refer to them today as Kepler's
laws. The model has become a law and therefore a paragon of mathematical
modeling \citep{wan2018mathematical}.

\subsection{Preview about cancer
models}\label{preview-about-cancer-models}

As we get closer to the subject of our study, and in order to illustrate
these definitions more concretely, we can take an interest in the
meaning of the word \emph{model} in the context of cancer research. For
this, we restrict our corpus to scientific articles found when searching
for ``cancer model'' in the PubMed article database. Among these, we
look at the occurrences of the word \emph{model} and the sentences in
which it is included. This cancer-related context of model is
represented as a tree in Figure \ref{fig:pubmed-tree}. Some of the
distinctions already mentioned can be found here. The \emph{mouse} and
\emph{xenograft} models, which will be discussed later in this thesis,
represent some of the most common physical models in cancer studies.
These are animal models in which the occurrence and mechanisms of
cancer, usually induced by the biologist, are studied. On the other
hand, \emph{prediction}, \emph{prognostic} or \emph{risk score} models
refer to formal models and borrow from statistical language.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/pubmed-tree} 

}

\caption[Tree visualization of \emph{model} semantic context in cancer-related literature]{\textbf{Tree visualization of \emph{model}
semantic context in cancer-related literature} Generated with the
\href{https://esperr.github.io/pub-trees/}{`PubTrees'} tool by Ed Sperr,
and based on most relevant PubMed entries for ``cancer model'' search.}\label{fig:pubmed-tree}
\end{figure}






Another way to classify cancer models may be to group them into the
following categories: \emph{in vivo}, \emph{in vitro} and \emph{in
silico}. The first two clearly belong to the physical models but one
uses whole living organisms (e.g.~a human tumor implanted in an
immunodeficient mouse) and the other separates the living from its
organism in order to place it in a controlled environment (e.g.~tumor
cells in growth medium in a Petri dish). \textbf{In the thesis, data
from both \emph{in vivo} and \emph{in vitro} models will be used.
However, unless otherwise stated, a model will always refer to a
representation \emph{in silico}.} This third category, however, contains
a very wide variety of models \citep{deisboeck2009silico}, to which we
will come back in chapter \ref{mechanistic-cancer}. A final ambiguity
about the nature of the formal models used in this thesis needs to be
clarified beforehand.

\section{Statistics or mechanistic}\label{stat-mech}

A rather frequent metaphor is to compare formal models to black boxes
that take in input \(X\) predictors, or independent variables, and
output response variable(s) \(Y\), also named dependent variables. The
models then split into two categories (Figure \ref{fig:boxes}) depending
on the answer to the question: are you modeling the inside of the box or
not?

\subsection{The inside of the box}\label{the-inside-of-the-box}

The purpose of this section is to present in a schematic, and therefore
somewhat caricatural, manner the two competing formal modeling
approaches that will be used in this thesis and that we will call
mechanistic modeling and statistical modeling. Assuming the unambiguous
nature of the predictors and outputs we can imagine that the natural
process consists in defining the result Y from the inputs X according to
a function of a completely unknown form (Figure \ref{fig:boxes}A).

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{fig/boxes} 

}

\caption[Different modeling strategies.]{\textbf{Different modeling strategies.} (A) Data
generation from predictors \(X\) to response \(Y\) in the natural
phenomenon. (B) Mechanistic modeling defining mechanisms of data
generation inside the box. (C) Statistical modeling finding the function
\(f\) that gives the best predictions. Adapted from
\citet{breiman2001statistical}.}\label{fig:boxes}
\end{figure}








The first modeling approach, that we will call \textbf{mechanistic},
consists in \textbf{building the box by imitating what we think is the
process of data generation}, or in other words, by representing the
mechanisms at work (Figure \ref{fig:boxes}B). This integration of prior
knowledge can take different forms. In this thesis it will often come
back to presupposing certain relations between entities according to
what is known about their behaviour. \(X_1\) which acts on \(X_3\) may
correspond to the action of one biological entity on another, supposedly
unidirectional; just as the joint action of \(X_2\) and \(X_3\) may
reflect a known synergy in the expression of genes or the action of
proteins. Mathematically this is expressed here with a perfectly
deterministic model defined \emph{a priori}. All in all, in a purely
mechanistic approach, the nature of the relations between entities
should be linked to biological processes and the parameters in the model
all have biological definitions in such a way that it could even be
considered to measure them directly. For example, the coefficient \(2\)
multiplying \(X_2X_3\) can correspond to a stoichiometric coefficient or
a reaction constant which have a theoretical justification or are
accessible by experimentation. In some fields of literature these models
are sometimes called mathematical models because they propose a
mathematical translation of a phenomenon, which does not start from the
data in a bottom-up approach but rather from a top-down theoretical
framework. In this thesis we will adhere to the \emph{mechanistic model}
name, which is more transparent and less ambiguous compared to other
approaches also based on mathematics, without necessarily the other
characteristics described above.

The second approach, often called \textbf{statistical modeling}, or
sometimes machine learning depending on the precise context and
objective, does not necessarily seek to reproduce the natural process of
data generation but to \textbf{find the function allowing the best
prediction of \(Y\) from \(X\)} (Figure \ref{fig:boxes}C). Pushed to the
limit, they are an ``idealized version of the data we gain from
immediate observation'' \citep{frigg2020models}, thus providing a
phenomenological description. The methods and algorithms used are then
intended to be sufficiently flexible and to make the fewest possible
assumptions about the relationships between variables or the
distribution of data. Without listing them exhaustively, the approaches
such as simple linear regressions or more complex support vector
machines \citep{cortes1995support} or random forests
\citep{breiman2001random}, which will sometimes be mentioned in this
thesis, fall into this category which contains many others
\citep{hastie2009elements}.

Several discrepancies result from this difference in nature between
mechanistic and statistical models, some of which are summarized in the
Table \ref{tab:mechstat}. In a somewhat schematic way, we can say that
the \textbf{mechanistic model first asks the question of \emph{how} and
then looks at the result for the output}. Conversely, the statistical
model first tries to approach the Y and then possibly analyses what can
be deduced from it, regarding the importance of the variables or their
relationships in a \emph{post hoc} approach
\citep{ishwaran2007variable, manica2019toward}. The greater flexibility
of statistical methods makes it possible to better accept the
heterogeneity of the variables, but this is generally done at the cost
of a larger number of parameters and therefore requires more data.
Moreover, statistical models can be considered as inductive, since they
are able to use already generated data to identify patterns in it.
Conversely, mechanistic models are more deductive and they can
theoretically allow to \textbf{extrapolate beyond the original data or
knowledge used to build the model} \citep{baker2018mechanistic}.
Finally, the most relevant way of assessing the value or adequacy of
these models may be quite different. A statistical model is measured by
its ability to predict output in a validation dataset different from the
one used to train its parameters. The mechanistic model will also be
evaluated on its capacity to approach the data but also to order it, to
give a meaning. If its pure predictive performance is generally
inferior, \textbf{how can the value of understanding be assessed?} This
question will be one of the threads of the dissertation.

\begin{table}

\caption{\label{tab:mechstat}\textbf{Some pros and cons for mechanistic and
statistical modeling}. Adapted from \citet{baker2018mechanistic}.}
\centering
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{15em}||>{\raggedright\arraybackslash}p{15em}}
\hline
\rowcolor[HTML]{808080}  \multicolumn{1}{>{\centering\arraybackslash}p{15em}}{\textcolor{white}{\textbf{Mechanistic modeling}}} & \multicolumn{1}{>{\centering\arraybackslash}p{15em}}{\textcolor{white}{\textbf{Statistical modeling}}}\\
\hline
\multicolumn{2}{l}{\textbf{Definition}}\\
\hline
\hspace{1em}Seeks to establish a mechanistic relationship between inputs and outputs & Seeks to establish statistical relationships between inputs and outputs\\
\hline
\multicolumn{2}{l}{\textbf{Pros and cons}}\\
\hline
\hspace{1em}Presupposes and investigates causal links between the variables & Looks for patterns and establishes correlations between variables\\
\hline
\hspace{1em}Capable of handling small datasets & Requires large datasets\\
\hline
\hspace{1em}Once validated, can be used as a predictive tool in new situations possibly difficult to access through experimentation & Can only make predictions that relate to patterns within the data supplied\\
\hline
\hspace{1em}Difficult to accurately incorporate information from multiple space and time scales due to constrained specifications & Can tackle problems with multiple space and time scales thanks to flexible specifications\\
\hline
\hspace{1em}Evaluated on closeness to data and ability to make sense of it & Evaluated based on predictive performance\\
\hline
\end{tabular}
\end{table}




Mechanistic and statistical models are not perfectly exclusive and
rather form the two ends of a spectrum. The definitions and
classification of some examples is therefore still partly personal and
arbitrary. For instance, the example in \ref{fig:boxes}B can be
transformed into a model with a more ambiguous status:

\[logit(P[Y=1])=\beta_1X_1 + \beta_{23}X_2X_3\]

This model is deliberately ambiguous. As a logistic model, it is
therefore naturally defined as a statistical model. But the definition
of the interaction between \(X_2\) and \(X_3\) denotes a mechanistic
presupposition. The very choice of a logistic and therefore parametric
model could also result from a knowledge of the phenomenon, even if in
practice it is often a default choice for a binary output. Finally, the
nature of the parameters \(\beta_{1}\) and \(\beta_{23}\) is likely to
change the interpretation of the model. If they are deduced from the
data and therefore optimized to fit Y as well as possible, one will
think of a statistical model whose specification is nevertheless based
on knowledge of the phenomenon. On the other hand, one could imagine
that these parameters are taken from the biochemistry literature or
other data. The model will then be more mechanistic. The boundary
between these models is further blurred by the different possibilities
of combining these approaches and making them complementary
\citep{baker2018mechanistic, salvucci2019machine}.

\subsection{A tale of prey and predators}\label{lotkasection}

The following is a final general illustration of the concepts and
procedures introduced with respect to statistical and mechanistic models
through a famous and characteristic example: the Lotka-Volterra model of
interactions between prey and predators. This model was, like many
students, my first encounter with what could be called mathematical
biology. The Italian mathematician Vito Volterra states this system for
the first time studying the unexpected characteristics of fish
populations in the Adriatic Sea after the First World War.
Interestingly, Alfred Lotka, an American physicist deduced the exact
same system independantly, starting from very generic process of
redistribution of matter among the several components derived from law
of mass action \citep{knuuttila2017modelling}. A detailed description of
their works and historical formulation can be found in original articles
\citep{lotka1925principles, volterra1926fluctuations} or dedicated
reviews \citep{knuuttila2017modelling}.

The general objective is to understand the evolution of the populations
of a species of prey and its predator, reasonably isolated from outside
intervention. Here we will use Canada lynx (\emph{Lynx canadensis}) and
snowshow hare (\emph{Lepus americanus}) populations for which an
illustrative data set exists \citep{hewitt1917conservation}. In fact,
commercial records listing the quantities of furs sold by trappers to
the Canadian Hudson Bay Company may represent a proxy for the
populations of these two species as represented in Figure
\ref{fig:lotka}A. Denoting the population of lynx \(L(t)\) and the
population of hare \(H(t)\) it can be hypothesized that prey, in the
absence of predators, would increase in population, while predators on
their own would decline in the absence of prey. A prey/predator
interaction term can then be added, which will positively impact
predators and negatively impact prey. The system can then be formalized
with the following differential questions with all coefficients
\(a_1, a_2, b_1, b_2 >0\):

\[\dfrac{dH}{dt}=a_1H-a_2HL\]

\[\dfrac{dL}{dt}=-b_1L+b_2HL\]

\(a_1H\) represents the growth rate of the hare population (prey), i.e.,
the population grows in proportion to the population itself according to
usual birth modeling. The main losses of hares are due to predation by
lynx, as represented with a negative coefficient in the \(-a_2HT\) term.
It is therefore assumed that a fixed percentage of prey-predator
encounters will result in the death of the prey. Conversely, it is
assumed that the growth of the lynx population depends primarily on the
availability of food for all lynxes, summarized in the \(b_2HL\) term.
In the absence of hares, the lynx population decreases, as denoted by
the coefficient \(-b_1L\). Important features of mechanistic models are
illustrated here: the equations are based on \emph{a priori} knowledge
or assumptions about the structure of the problem and the parameters of
the model can be interpreted. \(a_1\), for example, could correspond to
the frequency of litters among hares and the number of offspring per
litter.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{01-Models_files/figure-latex/lotka-1} 

}

\caption[Some analyses around Lotka-Volterra model of a prey-predator system]{\textbf{Some analyses around Lotka-Volterra model of
a prey-predator system}. (A) Evolution of lynx and hares populations
based on Hudson Bay Company data about fur pelts. (B) and (C) Linear
regression for estimation of parameters. (D) Evolution of lynx and hare
populations as predicted by the model based on inferred parameters and
initial conditions.}\label{fig:lotka}
\end{figure}








This being said, the structure of the model having been defined \emph{a
priori}, it remains to determine its parameters. Two options would
theoretically be possible: to propose values based on the interpretation
of the parameters and ecological knowledge, or to fit the model to the
data in order to find the best parameters. For the sake of simplicity,
and because this example has only a pedagogical value in this
presentation, we propose to determine them approximately using the
following Taylor-based approximation:

\[\dfrac{1}{y(t)} \dfrac{dy}{dt} \simeq \dfrac{1}{y(t)} \dfrac{y(t+1)-y(t-1)}{2}\]

By applying this approximation to the two equations of the differential
system and plotting the corresponding linear regressions (Figures
\ref{fig:lotka}B and C), we can obtain an evaluation of the parameters
such as \(a_1=0.82\), \(a_2=0.0298\), \(b_1=0.509\), \(b_2=0.0129\). By
matching the initial conditions to the data, the differential system can
then be fully determined and solved numerically (Figures
\ref{fig:lotka}D). Comparison of data and modeling provides a good
illustration of the virtues and weaknesses of a mechanistic model.
Firstly, based on explicit and interpretable hypotheses, the model was
able to recover the cyclical behaviour and dependencies between the two
species: the increase in the lynx population always seems to be preceded
by the increase in the hare population. However, the amplitude of the
oscillations and their periods are not exactly those observed in the
data. This may be related to approximations in the evaluation of
parameters, random variation in the data or, of course, simplifications
or errors in the structure of the model itself.

Besides, if one tries to carry out a statistical modeling of these data,
it is very likely that it is possible to approach the curve of
populations evolution much closer, especially for the hares. But should
it be expressed simply as a function of time or should a joint modeling
be proposed? The nature of the causal link between prey and predators
will be extremely difficult to establish without strong hypotheses such
as those of the mechanistic model. On the other hand, if populations in
later years had to be predicted as accurately as possible, it is likely
that a sufficiently well-trained statistical model would perform better.
Finally, and this is a fundamental difference, the \textbf{mechanistic
model enables to test cases or hypotheses that go beyond the scope of
the data}. Quite simply, by playing with the variables or parameters of
the model, we can predict the exponential decrease of predators in the
absence of prey and the exponential growth of prey in the absence of
predator. More generally, it is also possible to study analytically or
numerically the bifurcation points of the system in order to determine
the families of behaviours according to the relative values of the
parameters \citep{flake1998computational}. It is not possible to infer
these new or hypothetical behaviours directly from the data of the
statistical model. This is theoretically possible on the basis of the
mechanistic model, provided that it is sufficiently relevant and that
its operating hypotheses cover the cases under investigation. Now that
the value of mechanistic models has been illustrated in a fairly
theoretical example, all that remains is to explore in the next chapters
how they can be built and used in the context of cancer.

\section{Simplicity is the ultimate
sophistication}\label{simplicity-is-the-ultimate-sophistication}

Before concluding this modeling introduction, it is important to
highlight one of the most important points already introduced in a
concise manner by the poet Paul Valéry at the beginning of this chapter.
\textbf{Whatever its nature, a model is always a simplified
representation of reality and by extension is always wrong to a certain
extent}. This is a generally well-accepted fact, but it is crucial to
understand the implications for the modeler. This simplification is not
a collateral effect but an intrinsic feature of any model:

\begin{quote}
\emph{No substantial part of the universe is so simple that it can be
grasped and controlled without abstraction. Abstraction consists in
replacing the part of the universe under consideration by a model of
similar but simpler structure. Models, formal and intellectual on the
one hand, or material on the other, are thus a central necessity of
scientific procedure.}\\
\citep{rosenblueth1945role}
\end{quote}

Therefore, a model exists only because we are not able to deal directly
with the phenomenon and simplification is a necessity to make it more
tractable \citep{potochnik2017idealization}. This simplification
appeared many times in the studies of frictionless planes or
theoretically isolated systems, in a totally deliberate strategy.
However, this idealization can be viewed in several ways
\citep{weisberg2007three}. One of them, called Aristotelian or minimal
idealization, is to eliminate all the properties of an object that we
think are not relevant to the problem in question. This amounts to lying
by omission or making assumptions of insignificance by focusing on key
causal factors only \citep{frigg2020models}. We therefore refer to the
\emph{a priori} idea that we have of the phenomenon. The other
idealization, called Galilean, is to deliberately distort the theory to
make it tractable as explicited by Galileo himself:

\begin{quote}
\emph{We are trying to investigate what would happen to moveables very
diverse in weight, in a medium quite devoid of resistance, so that the
whole difference of speed existing between these moveables would have to
be referred to inequality of weight alone. Since we lack such a space,
let us (instead) observe what happens in the thinnest and least
resistant media, comparing this with what happens in others less thin
and more resistant.}
\end{quote}

This fairly pragmatic approach should make it possible to evolve
iteratively, reducing distortions as and when possible. This could
involve the addition of other species or human intervention into the
Lotka-Volterra system described above. A three-species Lotka-Volterra
model can however become chaotic \citep{flake1998computational}, and
therefore extremely difficult to use and interpret, thus underlining the
importance of simplifying the model.

We will have the opportunity to come back to the idealizations made in
the course of the cancer models but it is already possible to give some
orientations. The biologist who seeks to study cancer using cell lines
or animal models is clearly part of Galileo's lineage. The mathematical
or \emph{in silico} modeler has a more balanced profile. The design of
qualitative mechanistic models based on prior knowledge, which is the
core of the second part of the thesis, is more akin to minimal
idealization, which seeks to highlight the salient features of a system.
The Galilean consistins in studying mathematically tractable systems was
also important. To take the example of prey-predator interactions, a
differential system with more variables quickly becomes impossible to
solve by hand. The development of more and more powerful computers has
apparently pushed back the limits of the computationally tractable
systems and thus of Galilean idealization. However, this is always
necessary, for example in high-dimensional statistical approaches
(thousands of variables) where the modelers decide to consider the
variables independently while neglecting their interactions.

Because of the complexity of the phenomena, simplification is therefore
a necessity. The objective then should not necessarily be to make the
model more complex, but to \textbf{match its level of simplification
with its assumptions and objectives}. Faced with the temptation of the
author of the model, or his reviewer, to always extend and complicate
the model, it could be replied with Lewis Carrol words\footnote{More
  concisely stated by \citet{rosenblueth1945role}: ``best material model
  for a cat is another cat, or preferably the same cat.''}:

\begin{quote}
\emph{``That's another thing we've learned from your Nation,'' said Mein
Herr, ``map-making. But we've carried it much further than you. What do
you consider the largest map that would be really useful?''}\\
\emph{``About six inches to the mile.''}\\
\emph{``Only six inches!'' exclaimed Mein Herr. ``We very soon got to
six yards to the mile. Then we tried a hundred yards to the mile. And
then came the grandest idea of all! We actually made a map of the
country, on the scale of a mile to the mile!''}\\
\emph{``Have you used it much?'' I enquired.}\\
\emph{``It has never been spread out, yet,'' said Mein Herr: ``the
farmers objected: they said it would cover the whole country, and shut
out the sunlight! So we now use the country itself, as its own map, and
I assure you it does nearly as well.''}\\
Lewis Carroll, \emph{Sylvie and Bruno} (1893)
\end{quote}

\BeginKnitrBlock{conclubox}
\subsubsection*{Summary}\label{summary}
\addcontentsline{toc}{subsubsection}{Summary}

Any scientific model has the vocation to allow the understanding of its
object of study by proposing a simplification simpler to handle. A model
can be physical or purely intellectual. This thesis will focus on the
latter, in particular models that explicitly highlight the mechanisms
and relationships between the underlying entities of the system. These
models will be called mechanistic and sometimes opposed to statistical
models or machine learning models which do not presuppose \emph{a
priori} knowledge on the internal mechanisms of the system.
\EndKnitrBlock{conclubox}

\chapter{Cancer as deregulation of complex
machinery}\label{cancer-as-deregulation-of-complex-machinery}

\epigraph{"Does not the entireness of the complex hint at the perfection of the simple?"}{Edgar Allan Poe (Eureka, 1848)}

\initial{A}rmed with all these models, whether statistical or
mechanistic, we are going to look at cancer, a particularly complex
system that fully justifies their use. Since the first chapter recalled
how important prior knowledge of the phenomenon under study is for
designing models, whatever their nature, this chapter will briefly
summarize some of the most important characteristics of this disease
before returning to the models themselves in the next chapter. Without
aiming for exhaustiveness, and after an epidemiological and statistical
description, we will focus on the most useful information for the
modeler, i.e., the underlying biological mechanisms and available data.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/bath} 

}

\caption[Cancer is an old disease]{\textbf{Cancer is an old disease.} Rembrandt,
\emph{Bathsheba at Her Bath}, c. 1654, oil on canvas, Louvre Museum,
Paris}\label{fig:bath}
\end{figure}





\section{What is cancer?}\label{what-is-cancer}

Cancer can be described as a group of diseases characterized by
\textbf{uncontrolled cell divisions and growth which can spread to
surrounding tissues}. Descriptions of this disease, especially when
associated with solid tumors, have been found as far back as ancient
Egyptian documents, at least 1600 BC and we know from the first century
A.D. with Aulus Celsus that it is better to remove the tumors and this
as soon as possible \citep{hajdu2011note}. Progress will accelerate
during the Renaissance with the renewed interest in medicine, and
anatomy in particular, which will advance the knowledge of tumor
pathology and surgery \citep{hajdu2011note2}. The progress of anatomical
knowledge has also left brilliant testimonies in the field of painting,
which make the renown of the Renaissance today. The precision of these
artists' traits has also allowed some retrospective medical analyses,
some of them going so far as to identify the signs of a tumor in some of
the subjects of these paintings \citep{bianucci2018earliest}. Such is
the bluish stain on the left breast of the Bathsheba painted by
Rembrandt (Figure \ref{fig:bath}) which has been subject to
controversial interpretations, sometimes described as an example of
``skin discolouration, distortion of symmetry with axillary fullness and
peau d'orange'' \citep{braithwaite1983rembrandt} and sometimes spared by
photonic and computationnal analyses \citep{heijblom2014monte}. The
mechanisms of the disease only began to be elucidated with the
appearance of the microscope in the 19th century, which revealed its
cellular origin \citep{hajdu2012note}. The classification and
description of cancers is then gradually refined and the first
non-surgical treatments appear with the discovery of ionising radiation
by the Curies \citep{hajdu2012note2}. The 20th century is then the
century of understanding the causes of cancer
\citep{hajdu2013note, hajdu2013note2}. Some environmental exposures are
characterized as asbestos or tobacco. Finally, the biological mechanisms
become clearer with the identification of tumor-causing viruses and
especially with the discovery of DNA \citep{watson1953molecular}. The
foundations of our current understanding of cancer date back to this
period, which marks the beginning of the molecular biology of cancer. It
is this branch of biology that contains the bulk of the knowledge that
will be used to build our mechanistic models, and it will be later
detailed in Section \ref{molecular-biology}.

One of the ways to read this brief history of cancer is to see that
theoretical and clinical progresses have not followed the same
timeframes. The medical and clinical management of cancers initially
progressed slowly but surely, and this in the absence of an
understanding of the mechanisms of cancer. Conversely, the theoretical
progress of the last century has not always led to parallel medical
progress, except on certain specific points. The interaction between the
two is therefore not always obvious. The \textbf{transformation of
fundamental knowledge into clinical impact is therefore of particular
importance}. This is what is called \emph{translational medicine}, the
aim of which is to go from laboratory bench to bedside
\citep{cohrs2015translational}. It is in this perspective that we will
analyze the mechanistic models studied in this thesis. Their objective
is to integrate biological knowledge, or at least a synthesis of this
knowledge, in order to transform it into a relevant clinical
information.

\section{Cancer from a distance: epidemiology and main
figures}\label{epidemio}

Before going down to the molecular level, it is important to detail some
figures and trends in the epidemiology of cancer today. Following the
description in the previous section, cancer is first and foremost
defined as a disease. Considered to be a unique disease, it caused 18.1
million new cancer cases and 9.6 million cancer deaths in 2018 according
to the Global Cancer Observatory affiliated to World Health Organization
\citep{bray2018global}. However, these aggregated data conceal
disparities of various kinds. The first one is geographical. Indeed,
mortality figures make cancer one of the leading causes of premature
death in most countries of the world but its importance relative to
other causes of death is even greater in the more developed countries
(Figure \ref{fig:globocan-map}). All in all, cancer is the first or
second cause of premature death in almost 100 countries worldwide
\citep{bray2018global}. These differences call for careful consideration
of the impact of population age structures and health-related
covariates.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{fig/globocan-map} 

}

\caption[World map and national rankings of cancer as a cause of premature death]{\textbf{World map and national rankings of
cancer as a cause of premature death.} Classification of cancer as a
cause of death before the age of 70, based on data for the year 2015.
Original Figure, data and methods from \citet{bray2018global}.}\label{fig:globocan-map}
\end{figure}






A second disparity lies in the different types of cancer. If we classify
tumors solely according to their location, i.e., the organ affected
first, we already obtain very wide differences. First of all, the
incidence varies considerably (Figure \ref{fig:cancer-tissues}A)).
Cancers do not occur randomly anywhere in the body and certain
environments or cell types appear to be more favourable
\citep{tomasetti2015variation}. Mortality is also highly variable but is
not directly inferred from incidence. Not all types of cancer have the
same prognosis (Figure \ref{fig:cancer-tissues}A and B) and survival
rates \citep{liu2018integrated}. Although breast cancer is much more
common than lung cancer, it causes fewer deaths because its prognosis
is, on average, much better. The mechanisms at work in the emergence of
cancer are therefore not necessarily the same as those that will govern
its evolution or its response to treatment. And still on the response to
treatment, Figure \ref{fig:cancer-tissues}B highlights another
disparity: not only are the survival prognosis associated with each
cancer very different, but the evolution (and generally the improvement)
of these prognoses has been very uneven over the last few decades. This
means that theoretical and therapeutic advances have not been applied to
all types of cancer with the same success. It is one more indication of
the \textbf{diversity of cancer mechanisms in different tissues and
biological contexts}, which make it impossible to find a panacea, and
which, on the contrary, encourage us to carefully consider the
particularities of each tumor, both to understand them and to treat
them. Under a generic name and in spite of common characteristics, the
cancers thus appear as extremely heterogeneous. And to understand the
sources of this heterogeneity, it is necessary to consider the disease
on a smaller scale.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{02-Cancer_files/figure-latex/cancer-tissues-1} 

}

\caption[Incidence, mortality and survival per cancer types]{\textbf{Incidence, mortality and survival
per cancer types}. (A) World incidence and mortality for the 19 most
frequent cancer types in 2018, expressed with age-standardized rates
(adjusted age structure based on world population); data retrieved from
\href{https://gco.iarc.fr/today/home}{Global Cancer Observatory}. (B)
Evolution of 5-years relative survival for the same cancer types based
on US data from SEER registries in 1975-1977 and 2006-2012; data
retrieved from \citet{jemal2017annual}.}\label{fig:cancer-tissues}
\end{figure}










\section{Basic molecular biology and cancer}\label{molecular-biology}

If it is not possible and desirable to summarize here the state of
knowledge about the biology of cancer, we are going to give a very
partial vision focused on the main elements used in this thesis, thus
aiming to make it a self-sufficient document. The details necessary for
a finer and more general understanding can be found in dedicated
textbooks such as \citet{alberts2007molecular} and
\citet{weinberg2013biology}.

\subsection{Central dogma and core
principles}\label{central-dogma-and-core-principles}

Some of the principles that govern biology can be described at the level
of one of its simplest element, the cell. Let us consider for the moment
a perfectly healthy cell. It must ensure a certain number of functions
necessary for its survival and, sometimes, for its
division/reproduction. These functions are encoded to a large extent in
its genetic information in the form of DNA, which is stable and shared
by the different cells since it is defined at the level of the
individual. Most biological functions, however, are not performed by DNA
itself which remains in the nucleus of the cell. The DNA is thus
transcribed into RNA, another nucleic acid which, in addition to
performing some biological functions, becomes the support of the genetic
information in the cell. The RNA is then itself translated into new
molecules composed of long chains of amino acid residues and called
proteins. They are the ones that execute most of the numerous cellular
functions: DNA replication, physical structuring of the cell, molecule
transport within the cell etc. A rather simplistic but fruitful way to
understand this functioning is to consider it as a \textbf{progressive
transfer of biological information from DNA to proteins}, which has
sometimes been summarized as the central dogma of the molecular biology
(\ref{fig:central-dogma}), first stated Francis Crick
\citep{crick1970central}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{fig/central-dogma} 

}

\caption[Central dogma of molecular biology]{\textbf{Central dogma of molecular biology.}
Schematic representation of the information flow within the cell, from
DNA to proteins through RNA, more precisely described in this
\href{https://www.youtube.com/watch?v=J3HVVi2k2No}{video} (Image credit
\emph{Genome Research Limited}).}\label{fig:central-dogma}
\end{figure}







However, many changes would be necessary to clarify this scheme and the
uni-directional nature was questioned early on. Above all, a large
number of regulations interact with and disrupt this master plan. The
genes are not always all transcribed, or at least not at constant
intensities, interrupting or varying the chain upstream. This modulation
in the transcription of genes can be induced by proteins, called
transcription factors. After a gene transcription, its expression can
still be regulated at various stages. RNAs can also be degraded more or
less rapidly. RNAs can be reshaped in their structure during their
maturation by a process called splicing, which varies the genetic
information they carry. Finally, proteins are subject to all kinds of
modifications referred to as post-translational, which can change the
chemical nature of certain groups or modify the three-dimensional
structure of the whole protein. For instance, some proteins perform
their function only if a specific amino acid residue is phosphorylated.
In addition, these modifications can be transmitted between proteins,
further complicating the flow of information. \textbf{All these
possibilities of regulation play an absolutely essential role in the
life of the cell by allowing it to adapt to different contexts,
situations and development stages}. From the same genetic material, a
cell of the eye and a cell of the heart can thus perform different
functions. Similarly, the same cell subjected to different stimuli at
different times can provide different responses because these molecular
stimuli trigger a regulation of its programme. But all these regulatory
mechanisms can be corrupted.

\subsection{A rogue machinery}\label{a-rogue-machinery}

With the above knowledge we can now return to the definition of cancer
as an uncontrolled division of cells that can lead to the growth of a
tumor that eventually spreads to the surrounding tissues. Therefore,
this corresponds to normal processes, like cell division and
reproduction, that are no longer regulated as they should be and are out
of control. Experiments on different model organisms have gradually
identified genetic mutations as a major source of these deregulations
\citep[\citet{reddy1982point}]{nowell1976clonal} until cancer was
clearly considered as a \textbf{genetic disease} making Renato Dulbecco,
Nobel Laureate in Medicine for his work on oncoviruses, say:

\begin{quote}
\emph{If we wish to learn more about cancer, we must now concentrate on
the cellular genome.}\\
\citep{dulbecco1986turning}.
\end{quote}

However, cancer is not a Mendelian disease for which it would be
sufficient to identify the one and only gene responsible for
deregulation. Indeed, the cell has many protective mechanisms. For
example, if a genetic mutation appears in the DNA, it has a very high
chance of being repaired by dedicated mechanisms. And if it is not
repaired, other mechanisms will take over to trigger the programmed
death of the cell, called apoptosis, before it can proliferate wildly.
So a cancer cell is probably a cell that has learned to resist this cell
death. Similarly, in order to generate excessive growth, a cell will
need to be able to replicate itself many times. However, there are
pieces of sequences on chromosomes called telomeres that help to limit
the number of times each cell can replicate. A cancer cell will
therefore have to manage to bypass this protection. Thus we can
schematically define the properties that must be acquired by the
cancereous cells in order to truly deviate the machinery. In an
influential article, these properties were summarized in six hallmarks
(Figure \ref{fig:hallmarks}) which are: resisting cell death, enabling
replicative immortality, sustaning proliferative signaling, evading
growth suppressors, activating invasion and inducing angiogenesis
\citep{hanahan2000hallmarks}. Two new ones were subsequently added in
the light of advances in knowledge \citep{hanahan2011hallmarks}:
deregulating cancer energetics and avoiding immne destruction. The
acquisition of these capacities generally requires many genetic
mutations and is therefore favoured by an underlying genome instability
or tumor-promoting inflammation.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{fig/hallmarks} 

}

\caption[Hallmarks of cancer]{\textbf{Hallmarks of cancer.} The different
biological capabilities acquired by cancer cells. Adapted from
\citet{hanahan2011hallmarks}.}\label{fig:hallmarks}
\end{figure}





Each of these characteristics, or hallmarks, constitutes a research
program in its own right. And for each one there are genetic
alterations. These are tissue-specific or not, specific to a hallmark or
common to several of them \citep{hanahan2000hallmarks}. In any case,
\textbf{cancer can only result from the combination of different
alterations that invalidate several protective mechanisms} at the same
time. This is often part of a multi-step process of hallmark acquisition
that has been experimentally documented in some specific cases
\citep{hahn1999creation} or more recently inferred from genome-wide data
for human patients \citep{tomasetti2015only}. In summary, it appears
that in order to study the functioning of cancer cells it is necessary
to look at several mechanisms and to be able to consider them not
separately but together, in as many different patients as possible. This
ambitious programme has been made possible by a technological
revolution.

\section{The new era of genomics}\label{the-new-era-of-genomics}

\subsection{From sequencing to multi-omics
data}\label{from-sequencing-to-multi-omics-data}

In 2001, the first sequencing of the human genome symbolized the
beginning of a new era, that of what will become \textbf{high-throughput
genomics} \citep{lander2001initial, venter2001sequence}. From the end of
the 20th century, biological data started to accumulate at an
ever-increasing rate \citep{reuter2015high}, feeding and accelerating
cancer research in particular
\citep{stratton2009cancer, meyerson2010advances}. The ability to
sequence the human genome as a whole, for an ever-increasing number of
individuals, has enabled \textbf{less biased and more systematic studies
of the causes of cancer} \citep{lander2011initial}. The number of genes
associated with cancer increased drastically and some very important
genes such as BRAF or PIK3CA have been identified
\citep{davies2002mutations, samuels2004high}. Progress also extended to
the gene expression data. Gene-expression arrays have made an important
contribution by providing access to transcriptomic data (RNA), i.e.,
what has been transcribed from DNA and is therefore one step further in
terms of biological information. This information has made it possible
to further explore the differences between normal and tumor cells
\citep{perou1999distinctive}, or even to refine the classification of
cancers, which until now has been done mainly according to the tumor
site. Breast cancers are thus divided into subtypes with different
combinations of molecular markers that facilitate the understanding of
clinical behavior \citep{perou2000molecular}. One step further, we also
note the appearance of prognostic gene signatures such as gene
expression patterns correlated with the survival of patients
\citep{van2002gene}. This revolution was then extended to other types of
data such as proteins (proteomics), reversible modifications of DNA or
DNA-associated proteins (epigenomics), metabolites (metabolomics) and
others, each representing a perspective that can complement the others
to better understand biological mechanisms, particularly in the case of
diseases \citep{hasin2017multi}. We have thus entered the era of
multi-omics data \citep{vucic2012translating}.

\subsection{State-of-the art of cancer
data}\label{state-of-the-art-of-cancer-data}

With respect to cancer in particular, this wealth of data is
particularly represented by a family of \textbf{studies conducted by The
Cancer Genome Atlas (TCGA) consortium}, started in 2008
\citep{cancer2008comprehensive}. Cohorts of several hundred patients are
thus sequenced over the years for different types of cancer
\citep{cancer2012comprehensive}, resulting today in a total of 11,000
tumors from 33 of the most prevalent forms of cancer
\citep{ding2018perspective}. Figure \ref{fig:tcga} provides a partial
but striking overview of the depth of data available under this program.
We can see the frequencies of alterations of certain groups of genes for
a list of cancer types, making it possible to visualize the disparities
already anticipated in section \ref{epidemio} based on patient survival.
There are indeed important differences between the organs but also
between the different subtypes associated with the same organ. And this
representation only corresponds to one layer of data, that of genetic
alterations. It could be used for transcriptomic, epigenomic or
proteomic data, thus giving rise to an incredibly complex photography.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/tcga} 

}

\caption[Genetic alterations frequencies for cancer types from TCGA data]{\textbf{Genetic alterations frequencies for cancer
types from TCGA data.} Frequencies of alteration per pahway and tumor
types as summaried in Pan-cancer analyses from TCGA data. Reprinted from
\citet{sanchez2018oncogenic}.}\label{fig:tcga}
\end{figure}






However, the diversity of data available for cancer research extends far
beyond this, both in terms of technology and type of data. This may be
data from model organisms such as mice or even tumors of human origin
made more suitable for experimentation. In the latter category, it is
crucial to mention the \textbf{huge amount of data available on cell
lines}, extracted from human tumors and transformed to be studied in
culture. It is then possible to go beyond descriptive data and vary the
experimental conditions in order to study the responses of these cells
to perturbations and to enrich our knowledge. This provides an
opportunity to know the response to more than 100 drugs of about 700
cell lines \citep{yang2012genomics}. The richness of these data, coupled
with the omic profiling of each cell line, enables to study the
determinants of response to treatment with unprecedented scope
\citep{iorio2016landscape}. More recently, but following a similar
logic, other types of inhibition screenings have been proposed based on
a more specific technique called CRISPR-Cas9
\citep{behan2019prioritization}. The simplicity of the cell lines in
relation to the original tumors makes all these studies possible but
sometimes hinders the clinical application of the knowledge acquired.
For this reason, other types of biological models have been developed,
including patient-derived xenografts (PDX) which is an implant of human
tumors in mice to ensure the existence of a certain tumor
microenvironment \citep{hidalgo2014patient}, while maintaining drug
screening possibilities \citep{gao2015high}. These two types of data,
cell lines and PDX, have been used in this thesis, in addition to TCGA
patient data, thus justifying the limitation of this presentation, which
could otherwise be extended to other types of biological models.
Similarly, other technologies are becoming increasingly important in the
generation of cancer data, such as single-cell sequencing
\citep{navin2015first}, but will not be used in this work.

\section{Data and beyond: from genetic to network
disease}\label{data-and-beyond-from-genetic-to-network-disease}

All that remains to be done now is to make sense of all these data, to
organize them, because \textbf{cancer understanding does not flow
directly from the abundance of data}, and the ability to produce it may
have been outpaced by the ability to analyze it
\citep{stadler2014cancer}. A striking example is that of the prognostic
signatures mentioned above. The many signatures or lists of genes
proposed, even for the same cancer type, share relatively few genes, are
difficult to interpret and their efficiency is sometimes poorly
reproducible \citep{domany2014using}. Even more surprisingly, most
signatures composed of randomly selected genes were also found to be
associated with patient survival \citep{venet2011most}. One of the main
avenues for improving the interpretability of the data is the
\textbf{integration of the prior knowledge} we have of the phenomena,
especially in the case of cancer \citep{domany2014using}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/circuit} 

}

\caption[Simplistic representation of cellular circuit and pathways]{\textbf{Simplistic representation of cellular
circuitry.} Normal cellular circuit sand sub-circuits (identified by
colours) can be reprogrammed to regulate hallmark capabilities within
cancer cells. Reprinted from \citet{hanahan2011hallmarks}.}\label{fig:circuit}
\end{figure}






This \emph{a priori} knowledge is in fact already present in Figure
\ref{fig:tcga} since genetic alterations have been grouped in several
categories called pathways. A pathway is a group of biological entities
and associated chemical reactions, working together to control a
specific cell function like apoptosis or cell division. The interest of
these groupings may be understood based on the description of hallmarks.
Indeed, if the ``aim'' of a cancer cell is to inactivate each of the
protective functions, then it is more relevant to think not by gene but
by function. Inactivating only one of the genes associated with the
function may be sufficient and it is no longer necessary to inactivate
the others. Numerous alterations in a large number of genes in various
patients result often in the same key impaired pathways, like
alterations of cell cycle or angiogenesis for instance
\citep{jones2008core}. It is therefore possible to improve the stability
and interpretability of analyses by moving \textbf{from the gene scale
to the pathway scale} \citep{drier2013pathway}. More generally, the
integration of biological knowledge often leads to improved performance
in various cancer-related prediction tasks, either through the selection
of variables or by taking into account the structure of the variables
\citep{bilal2013improving, ferranti2017value}. Increasingly, the
biological variables are not interpreted separately but in relation to
each other \citep{barabasi2004network}. This is reflected in the
emergence of more and more resources to summarize and represent
signaling pathways and associated networks such as SIGNOR
\citep{perfetto2016signor} or the Atlas of Cancer Signaling Network
\citep{kuperstein2015atlas}. Like other diseases, cancer then goes
\textbf{from a genetic disease to a network disease}
\citep{del2010diseases} and one can study how all kinds of genetic
alterations affect the wiring of these networks
\citep{pawson2007oncogenic}, and modify the cellular functions leading
to the previously described cancer hallmarks as depicted schematically
in Figure \ref{fig:circuit}. In short, the richness of the data did not
make it less necessary to use prior knowledge in order to make the
analyses more interpretable and more robust.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/pathways} 

}

\caption[Genetic alterations frequencies from TCGA data mapped on a schematic signaling network]{\textbf{Genetic alterations frequencies from TCGA
data mapped on a schematic signaling network.} Frequencies of alteration
per pathway and tumor types as summarized in Pan-cancer analyses from
TCGA data. Reprinted from \citet{sanchez2018oncogenic}.}\label{fig:pathways}
\end{figure}






The final step, to obtain one of the most complete and integrated
visions of cancer biology, is then to integrate omics knowledge with
knowledge about the structure of pathways to try to understand in detail
how their combinations can lead to so many cancers that are both similar
and different. An example of such a representation is given by mapping
the TCGA data about genetic alterations, presented in Figure
\ref{fig:tcga}, on a representation of the different pathways showing
not only their internal organization but also their cross-talk
\citep{sanchez2018oncogenic}. This representation is proposed in Figure
\ref{fig:pathways} and is one of the most recent and comprehensive view
of the kind of tools and data available to the modeler who wants to
dissect more deeply the mechanisms involved in cancer.

\BeginKnitrBlock{conclubox}
\subsubsection*{Summary}\label{summary-1}
\addcontentsline{toc}{subsubsection}{Summary}

Cancer is more than ever seen as a genetic disease. Its appearance in a
patient results from the accumulation of various genetic alterations
that invalidate the protective mechanisms naturally intended to prevent
uncontrolled proliferation. The simultaneous consideration of the
numerous biological entities involved and the regulatory networks that
link them calls for global systems biology methods. Technological
developments also provide access to different types of omics data
(genes, RNA, proteins, etc.) that provide complementary information, the
joint analysis of which allows us to better understand the complexity of
the mechanisms involved. It should be noted that many physical models of
cancer (cell lines) extend the field of experimentation and generate
data.
\EndKnitrBlock{conclubox}

\chapter{Mechanistic modeling of cancer: from complex disease to systems
biology}\label{mechanistic-cancer}

\epigraph{"How remarkable is life? The answer is: very. Those of us who deal in networks of chemical reactions know of nothing like it... How could a chemical sludge become a rose, even with billions of years to try."}{George Whitesides (The improbability of life, 2012)}

\initial{T}he previous chapter identified the need to organize cancer
knowledge and data. The integration of biological knowledge,
particularly in the form of networks, is a first step in this direction.
The deepening of knowledge, however, requires the ability to manipulate
objects even more, to experiment, to dissect their behaviour in an
infinite number of situations, such as the astronomer with his orrery or
physicians with their old anatomical models (Figure \ref{fig:anatomy}).
Is it then possible to create mechanistic models of cancer in the same
way?

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/anatomy} 

}

\caption[Dissecting a biological phenomenon using a non-computational model]{\textbf{Dissecting a biological phenomenon using a
non-computational model.} Rembrandt, \emph{The Anatomy Lesson of Dr
Nicolaes Tulp}, 1634, oil on canvas, Mauritshuis museum, The Hague}\label{fig:anatomy}
\end{figure}





\section{Introducing the diversity of mechanistic models of
cancer}\label{mechanistic-family}

Modeling cancer is not a new idea. And the diversity of biological
phenomena involved in cancer has given rise to an equally important
diversity of models and formalisms, which we seek here to give a brief
overview in order to better identify the specific models that we will
focus on later. One way to order this diversity is to consider the
scales of these models (Figure \ref{fig:multiscale}). Indeed,
\textbf{cancer can be read at different levels, from the molecular level
of DNA and proteins, to the cellular level, to the level of tissues and
organisms} \citep{anderson2008integrative}. Models have been proposed at
all these scales, using different formalisms
\citep{bellomo2008foundations} and answering different questions.

Consistent with the evolution of knowledge and data, the early models
were at the \textbf{macroscopic level}. While methods and terminologies
may have changed, there are nevertheless traces of these models as early
as the 1950s. We then speak rather of mathematical modeling with a
meaning that is intermediate between what we have defined as mechanistic
models and statistical models \citep{byrne2010dissecting}. First, the
initiation of tumorigenesis was theorized with biologically-supported
mathematical expressions in order to make sense of cancer incidence
statistics \citep{armitage1954age, knudson1971mutation}. These models,
however, remained relatively descriptive in that they did not shed any
particular light on the biological mechanisms involved and focused on
gross characteristics of tumors. The integration of more advanced
knowledge as well as the progressive refinement of mathematical
formalisms has nevertheless allowed these models to proliferate while
gaining in interpretability, with for instance mechanistic models of
metastatic relapse \citep{nicolo2020machine}. Always on a macroscopic
scale, the study of tumor growth has also been the playground of many
mathematicians \citep{araujo2004history, byrne2010dissecting}, even
predicting invasion or response to surgical treatments using spatial
modeling \citep{swanson2003virtual}. This line of research is still
quite active today and provides a mathematical basis for comparison with
tumor experimental growth \citep{benzekry2014classical}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/multiscale} 

}

\caption[The different scales of cancer modeling]{\textbf{The different scales of cancer
modeling.} Cancer can be approached at different scales, from molecules
to organs, using different data (dark blue), but often with the direct
or indirect objective of contributing to the study of clinically
interpretable phenomena (yellow boxes), in particular by studying the
influence of anticancer agents (pale blue). Reprinted from
\citet{barbolosi2016computational}.}\label{fig:multiscale}
\end{figure}









Taking it down a step further, it is also possible to model cancer at
the \textbf{cellular level}, for example by looking at the clonal
evolution of cancer \citep{altrock2015mathematics}. The aim is then to
understand the impact of the processes of mutation, selection, expansion
and cohabitation of different populations of cells, at specifc rates.
The accumulation of a mutation in a population of cells can thus be
studied \citep{bozic2010accumulation}. Modeling at the cellular level is
well suited to the study of interactions between cells, between cancer
cells and their environment or with the immune system. Similar to other
kinds of studies of population dynamics, formalisms based on
differential equations are quite common \citep{bellomo2008foundations};
but there are many other methods such partial differential equations or
agent-based modeling \citep{letort2019physiboss}.

Finally, at an even smaller scale, it is possible to model the
\textbf{molecular networks} at work in cells \citep{le2015quantitative}.
The aim is then to simulate mathematically how the different genes and
molecules regulate each other, transmit information and, in the case of
cancer, end up being deregulated \citep{calzone2010mathematical}. These
models will be the subject of the thesis and will therefore be defined
more precisely and used to detail the concepts and tools of systems
biology in the following sections. It can already be noted that while
these models can integrate the most fundamental biological mechanisms of
living organisms, one of the most burning questions is whether it is
possible to link them to the larger scales that are clinically more
interesting (tissues, organs etc.). Can these models tell us something
about the molecular nature of cancer? About patient survival? Their
response to treatment? These questions apply to all of the above models,
whatever their scales (Figure \ref{fig:multiscale}), but are more
difficult to answer for models defined at molecular scale that are
further from the clinical data of interest. The aim of this thesis is to
provide potential answers to these questions. One of the ways of
approaching these issues has been to propose multi-scale models, which
are nevertheless very complex
\citep{anderson2008integrative, powathil2015systems}. We will focus here
on the use of models defined almost exclusively at the molecular scale,
which is assumed to be prominent, to study what can be inferred on the
larger scales.

Before restricting the landscape to the molecular level, it is important
to point out that the \textbf{diversity of mechanistic models also
extends to the numerous mathematical formalisms encountered}.
\citet{altrock2015mathematics} delivers a relatively exhaustive list of
these, focusing on modeling at the scale of cell populations. For
example, it includes ordinary or partial differential equations,
particularly for modeling cell populations, tumor volumes, diffusion or
concentration equations for various entities (\emph{e.g.}, oxygen or
growth factors). Other formalisms, called agent-based, consider the
interactions between discrete entities, often cells, each of which may
experience events of interest such as mutations or cell death
\citep{wang2014clonal}. Different more or less discrete formalizations
have also been applied to cancer modeling, such as Boolean logic or
fuzzy logic \citep{le2015quantitative}. Many hybrid models also combine
different approaches, such as partial differential equations for a
spatial diffusion reaction coupled with cell population modeling using
discrete agents on a lattice \citep{altrock2015mathematics}.

However, the mechanistic nature of the models does not necessarily force
them to be deterministic, \emph{i.e.}, to deliver always the same
results from the same initial conditions. Indeed, \textbf{many
mechanistic models are based on stochastic or probabilistic processes},
which describe the evolution over time of random variables by defining
event or transition probabilities. Several examples can be found in the
study of the clonal evolution of cancers through branching processes
that model different cell events such as mutations, division and death
that result in various evolution of the cell population size
\citep{durrett2015branching, haeno2012computational}. These processes
fall into the category of Markov processes, which can be found applied
to many other examples such as the modeling of cell positions and their
evolutions on a two-dimensional lattice \citep{anderson2006tumor}. Note
that Markov processes will be used and described in more detail in
section \ref{maboss-section}. All in all, the strong presence of
stochastic approaches thus illustrates the appropriateness of their
formalism for cancer modeling where many events seem intrinsically
random (\emph{e.g.}, appearance of a mutation) or sometimes appear as
such in the current state of knowledge (\emph{e.g.}, change of cellular
status or phenotype). Understanding the very nature of these stochastic
events and their influence on global behavior is thus a major objective
explored by various modeling approaches
\citep{gupta2011stochastic, baar2016stochastic}.

\section{Cell circuitry and the need for cancer systems
biology}\label{cell-circuitry-and-the-need-for-cancer-systems-biology}

Most biological systems, and certainly cells, fall into the category of
\textbf{complex systems}. These are systems made up of many interacting
elements. While these systems can be found in many different scientific
fields, the cell as a complex system is characterized by the diversity
and multifunctionality of its constituent elements (genes, proteins,
small molecules, enzymes), which nevertheless contribute to organized
and \emph{a priori} non-chaotic behaviour
\citep{kitano2002computational}. Thus, the role of a protein such as the
p53 tumor suppressor can only be understood by taking into account the
interplay between its relationships with transcription factors and
biochemical modifications of the molecule itself
\citep{kitano2002computational}. In a cell, as in any complex system,
the multiplication of components and interactions can make the response
or behaviour of the system unexpected or unpredictable. Non-linear
responses, such as abrupt changes in the state of a system, called
critical transitions, can be observed in response to a moderate change
in the signal \citep{trefois2015critical}. Generally speaking, it is
possible to observe \textbf{emergent behaviours}, i.e., behaviours of
the system as a whole that were not trivially deducible from the
individual behaviours of its components. This has been documented,
through experiments and simulations, in the study of cell signalling
pathways and the resulting biological decisions
\citep{bhalla1999emergent, helikar2008emergent}. These considerations
have thus given rise to \textbf{system-level or holistic approaches that
aim to integrate data and knowledge into more comprehensive
representations, often called systems biology}.

What is true for the cell in general is just as true for cancer in
particular. Understanding the intertwining of signaling pathways is
necessary to study their contributions to different cancer hallmarks, as
shown in Figure \ref{fig:circuit}. The concepts described above can thus
be transposed to \textbf{cancer systems biology}
\citep{hornberg2006cancer, kreeger2010cancer, barillot2012computational}.
Indeed, it is often a question of understanding or predicting the impact
of perturbations on cellular networks. Understanding how a single
genetic mutation disrupts and reprograms networks, or even predicting
the responses triggered by a drug on a presumably promising molecular
target, makes little sense without integrated approaches. In addition,
cancers are characterized by the accumulation of numerous mutations and
alterations over time that must be considered concomitantly. These
points of view of biologists and modelers reinforce the observation
already made in the previous chapter of cancer as a network disease, as
a system disease (Figure \ref{fig:pathways}).

Finally, to conclude this general presentation, it is important to
understand that while small molecular network modeling is not recent,
the rise and multiplication of wide range systems biology approaches is
very much related to the production of biological data
\citep{de2002modeling}. The last few decades have seen the emergence of
high-throughput data that has made it possible to identify and link
hundreds of genes or proteins involved in cancer. Exploring the
interaction and back and forth between these models and the data they
use or predict is therefore of utmost importance. In addition, the now
\textbf{massive amount of data has also imposed mathematical or
computational approaches as a central element in the management of this
profusion} and more and more modeling approaches are focused on data
integration or inference
\citep{frohlich2018efficient, bouhaddou2018mechanistic}. More generally,
Figure \ref{fig:pubmed} shows that while the number of scientific
articles devoted to cancer has increased drastically since the 1950s
(panel A), the proportion of these same articles mentioning
\emph{models}, \emph{networks} or \emph{computational} approaches has
also increased (panel B), illustrating a change in paradigms.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{03-Mechanistic_files/figure-latex/pubmed-1} 

}

\caption[PubMed trends in cancer studies.]{\textbf{PubMed trends in cancer studies.} (A)
PubMed articles with the word \emph{Cancer} in either title or abstract
from 1950 to 2019. (B) Proportion of the \emph{Cancer} articles with
additional keywords expressed as PubMed logical queries.}\label{fig:pubmed}
\end{figure}






\section{Mechanistic models of molecular
signaling}\label{mechanistic-models-of-molecular-signaling}

Once the context has been defined, both biologically and
methodologically, it is possible to begin the exploration of the models
that will constitute the core of this thesis: the \textbf{mechanistic
models of molecular networks} and signaling pathways. Before describing
and illustrating some of the existing mathematical formalisms, it is
possible to describe the common fundamental elements of this family of
approaches.

\subsection{Networks and data}\label{networks-and-data}

The first step is to identify the relevant biological entities from a
question or system of interest (e.g.~tumor suppressor genes, signaling
cascades of proteins) and then to model their interactions, the
regulatory relationships that link them. At this stage the model can
generally be represented by a network but this word can cover different
realities \citep{le2015quantitative}. The simplest network just
represents undirected interactions between entities, which therefore
only establishes relationships and not causal mechanisms. But modeling
requires more precise definitions, in particular concerning the
direction of the interaction (is it A that acts on B or the opposite)
and its nature (type of chemical reaction, activation/inhibition etc.).
This is usually summarized as \textbf{activity flows (or influence
diagrams) with activation and inhibition arrows} as in Figure
\ref{fig:circuit} or Figure \ref{fig:toyraf}A. These arrows emphasize
the transformation of static networks into dynamic objects that can be
manipulated and interpreted mechanistically. This work can be taken
further by writing bipartite graphs, known as process descriptions,
which explicitly show the different states of each variable (first type
of nodes), depending on their phosphorylation state for instance, and
the reactions that link them (second type of nodes) as in Figure
\ref{fig:toyraf}B. A more precise description of these different
representations and their meanings can be found in
\citet{le2015quantitative}. \textbf{Once the network structure of the
model has been defined or inferred by the modeler, it is possible to
write the corresponding mathematical formalism} and potentially to
refine certain parameters. Finally, the model is often confronted with
new data to check its consistency with the biological behaviour studied
or possibly make new predictions.

However, all these steps are not linear and sequential, but rather
iterative and cyclical. This \textbf{modeling cycle}, with back and
forth to the data, is not specific to molecular network models, but it
is possible to specify it in this case (Figure \ref{fig:cycle}). The
names of the key players involved in the question of interest are thus
first extracted from adapted data or from the literature. A first
mathematical translation of the relationships between the entities is
then proposed before verifying the compatibility of this model with the
observations, whether qualitative or quantitative. If the compatibility
is not good, we come back to the definition or the parameterization of
the model. If compatibility is correct, the model can be used to make
new predictions or study phenomena that go beyond the initial data set.
Ideally, these predictions will be tested afterwards. This cyclic
approach with two successive checks is analogous to the use of
validation and test data in the evaluation of most learning algorithms.
This analogy can sometimes be masked by the qualitative nature of the
predictions or by the lack of explicit fitting of the parameters.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{fig/cycle} 

}

\caption[Modeling a biological network: an iterative and cyclical process]{\textbf{Modeling a biological network: an iterative
and cyclical process.} Reprinted from \citep{beal2020modelisation}. A
different and simpler version of this cycle is described in
\citep{le2015quantitative}.}\label{fig:cycle}
\end{figure}






\subsection{Different formalisms for different
applications}\label{different-formalisms-for-different-applications}

Beyond these similarities in the construction and representation of
models, the precise mathematical formalism that underlies them varies
according to the type of question and the data \citep{de2002modeling}.
For the sake of simplicity, and without exhaustiveness, we propose to
divide into quantitative and qualitative formalisms which will be
essentially illustrated respectively by \textbf{ordinary differential
equation (ODE)} models and \textbf{logical (or Boolean) models} for
which a graphical and schematic comparison is proposed in Figure
\ref{fig:toyraf}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/toyraf} 

}

\caption[Schematic example of logical and ODE modeling around MAPK signaling]{\textbf{Schematic example of logical and ODE
modeling around MAPK signaling.} (A) Activity flow diagram of a small
part of MAPK signaling, each node representing a gene or protein, with
an example of logical rule for MEK node for the corresponding logical
model. (B) Process description of the same diagram with BRAF and CRAF
merged in RAF for the sake of simplicity; each square representing a
reaction and the corresponding rate; an example differential equation is
provided for the phosphorylated (active) form of MEK.}\label{fig:toyraf}
\end{figure}










One of the most frequent approaches is the use of \textbf{chemical
kinetics} equations to construct ODE systems which are a fairly natural
translation of the process descritption networks described in the
previous section \citep{polynikis2009comparing}. For instance, each
biological interaction can be treated as a reaction governed by the law
of mass action and, under certain hypotheses, as a differential equation
(Figure \ref{fig:toyraf}B); the set of reactions in the system then
generates a set of differential equations with coupled variables, in an
analogous way to the Lotka Volterra system presented in section
\ref{lotkasection}. Thus the variables generally represent quantities of
molecular species, for example concentrations of RNA or proteins, and
the stoichiometric coefficients and reaction rates are used to define
the system parameters. Approximations are sometimes made to simplify the
equations, for example by assuming that they can be written as
Michaelis-Menten's enzymatic reactions, which have a simple and well
known behaviour. However, the theoretical accuracy of quantitative
models has a cost since \textbf{each differential equation requires
parameters}, such as reaction constants or initial conditions, to which
the system is very sensitive \citep{le2015quantitative}. The biochemical
interpretation of the parameters sometimes allows to find their value in
the literature, or in dedicated databeses \citep{wittig2012sabio}, if
the reactions are well characterized, even if possible variations in a
given biological or physical context are often unknown. Since knowledge
of the values of these parameters is often limited or even non-existent,
it may require a very large volume of data (including time series) to
fit the many missing parameters which can be difficult if the number of
parameters is large \citep{villaverde2014reverse}. However, recent work
has demonstrated the feasibility and scalability of this type of
inference with sufficiently rich data \citep{frohlich2018efficient}.

At the same time, more qualitative approaches to modeling biological
networks have been proposed with discrete variables linked together by
rules expressed as logical statements \citep{abou2016logical}. These
models are both more abstract since variables do not have a direct
biological interpretation (e.g.~concentration of a species) but are more
versatile since they can unify different biological realities under the
same formalism (e.g.~activation of a gene or phosphorlation of a
protein). The discrete nature of the variables can then be seen as an
asymptotic case of the sigmoidal (e.g.~Hill function) relationships
often found in biology \citep{le2015quantitative}. The step function
thus obtained can keep a natural interpretation in the context of
biological phenomena: genes activated or not, protein present or absent
etc. Similarly, interactions between species are not quantified but are
based on qualitative statements (e.g.~A will be active if B and C are
active), drastically reducing the number of parameters (Figure
\ref{fig:toyraf}A). If the theoretical interest of this formalism to
study biological mechanisms was proposed quite early
\citep{kauffman1969homeostasis, thomas1973boolean}, many concrete
applications have also been developed over the years, particularly in
cancer research \citep{saez2011comparing, remy2015modeling}. This
\textbf{logical formalism will constitute the core of the work presented
in Part II}, where it will therefore be discussed in greater detail.

\begin{table}

\caption{\label{tab:odelogic}\textbf{Features of quantitative and qualitative
modeling applied to biological molecular networks} (adapted from
\citet{le2015quantitative})}
\centering
\begin{tabular}[t]{>{\bfseries\raggedright\arraybackslash}p{6em}||>{\raggedright\arraybackslash}p{12em}|>{\raggedright\arraybackslash}p{12em}}
\hline
\rowcolor[HTML]{808080}  \multicolumn{1}{c}{\textcolor{white}{\textbf{ }}} & \multicolumn{1}{c}{\textcolor{white}{\textbf{Quantitative modeling}}} & \multicolumn{1}{c}{\textcolor{white}{\textbf{Qualitative modeling}}}\\
\hline
Example formalism & Ordinary differential equation (ODE) models & Logical models\\
\hline
Type of variables & Direct translation of biological quantities, usually continuous & Abstract representation of activity levels, usually discrete\\
\hline
Objective & Quantitatively accurate and temporal simulation of an experimental phenomenon & Coarse-grained simulation of qualitative phenotypes\\
\hline
Advantages & Direct confrontation with experimental data; precise; linear representation of time & Faster design; easy translation of literature-based assertions; simulation of perturbations\\
\hline
Drawbacks & Difficulty determining or fitting parameters & More difficult to link to data; lower precision\\
\hline
\end{tabular}
\end{table}





These two formalisms, which are among the most frequent for modeling
biological networks, share many similarities, in particular the
propensity to be built according to bottom-up strategies based on
knowledge of the elementary parts of the model, i.e., biological
entities and reactions. However, they differ in their implementation and
objectives, \textbf{one aiming at the most accurate representation
possible, the other seeking to capture the essence of the system's
dynamics in a parsimonious way} (Table \ref{tab:odelogic}). The
opposition is not irrevocable, as illustrated by the numerous hybrid
formalisms that lie within the spectrum delimited by these two extremes
such as fuzzy logic or discrete-time differential equations
\citep{aldridge2009fuzzy, le2015quantitative, calzone2018logical}. To
conclude, a comparison between the two approaches applied to the same
problem is proposed by \citet{calzone2018logical}, studying the
epithelio-mesenchymal transition (EMT, a biological process involved in
cancer), to illustrate in concrete terms their complementarity.

\subsection{Some examples of complex
features}\label{some-examples-of-complex-features}

With the help of these models, both qualitative and quantitative, many
complex behaviours have been identified. Benefiting from the knowledge
accumulated in the study of dynamic systems, a whole zoo of patterns
with complex and non-intuitive behaviours such as non-linearities have
been highlighted \citep{tyson2003sniffers}. The MAPK pathway, coarsely
described in Figure \ref{fig:toyraf}, and often simplified as a rather
unidirectional cascade, shows switch or bistability behaviors generated
by the complexity of its multiple phosphorylation sites
\citep{markevich2004signaling}. These models have also been put at the
service of understanding cancer and the erroneous decision-making by
cells resulting from impaired signaling pathways. Thus,
\citet{tyson2011dynamic} summarize superbly well the complexity that can
be hidden in the dynamics of smallest molecular networks as soon as they
contain more than two entites and crossed regulations or feedback loops.
Logical models have also made it possible to better dissect some complex
phenomena at play in the cell such as emergent behaviours
\citep{helikar2008emergent} or mechanisms behind mutation patterns in
cancer \citep{remy2015modeling}.

\section{From mechanistic models to clinical
impact?}\label{from-mechanistic-models-to-clinical-impact}

Mechanistic models have therefore undeniably led to a better
understanding of the complex molecular machinery of signalling pathways.
But beyond the interest that this understanding represents, do these
models also have a clinical utility? In other words, \textbf{are they of
clinical or only scientific value?}

\subsection{A new class of biomarkers}\label{a-new-class-of-biomarkers}

Throughout this thesis, the clinical value of mechanistic models will
often be analyzed by analogy to that of biomarkers. Biomarkers are
usually defined as measurable indicators of patient status or disease
progression, such as prostate-specific antigen (PSA) for prostate cancer
screening or BRCA1 mutation for breast cancer risk
\citep{henry2012cancer}. Biomarkers also encompass multivariate
signatures that identify more complex patterns with clinical
significance. Taking the logic even further, it was therefore proposed
that mechanistic models, which also reveal complex molecular behaviours,
could be considered as biomarkers, capturing perhaps even dynamic
information \citep{fey2015signaling}.

Like oncology biomarkers, the models will be divided into two categories
according to their clinical objectives: \textbf{prognostic models and
predictive models} \citep{oldenhuis2008prognostic}. Prognostic
biomarkers and models are those that provide information on the
evolution of cancer independently of treatment. They are therefore
generally confronted with survival or relapse data. The protein Ki-67
for example, encoded by the MKI67 gene, is known to be indicative of the
level of proliferation and high levels of expression are thus associated
with a poorer prognosis in many cancers \citep{sawyers2008cancer}.
Predictive biomarkers and models, on the other hand, give an indication
of the effect of a therapeutic strategy. The simplest example, but not
the only one, concerns biomarkers that are themselves the target of
treatment: treatments based on monoclonal antibodies directed against
HER2 receptors in breast cancer are only effective if the HER2 receptor
has been detected in the patient \citep{sawyers2008cancer}. Without
attempting to be exhaustive, some logical and ODE models, with either
prognostic or predictive claims, will be described.

\subsection{Prognostic models}\label{prognostic}

One of the first mechanistic models of cell signalling to have been
explicitly presented as a prognostic biomarker is the one proposed by
\citet{fey2015signaling} and describing c-Jun N-terminal kinase (JNK)
pathway in neuroblastoma cells. A summary of the study is provided in
Figure \ref{fig:fey}. The model is an ODE translation of the process
description network of Figure \ref{fig:fey}A, further determined and
calibrated with molecular biology experimental data obtained using
neuroblastoma cell lines. We thus observe the non-linear switch-like
dynamics of JNK activation as a function of cellular stress (Figure
\ref{fig:fey}B). The precise characteristics of this sigmoidal response
can, however, vary from one individual to another as captured by the
network output descriptors \(A\), \(K_{50}\) and \(H\). Fey et al.
proposed to perform neuroblastoma patient--specific simulations of the
model, using patient gene expressions for ZAK, MKK4, MKK7, JNK and AKT
genes to specify the initial conditions of the ODE system. Since JNK
activation induces cell death through apoptosis, the patient-specific
\(A\), \(K_{50}\) and \(H\) derived from patient-specifc models are then
analyzed as prognostic biomarkers (Figure \ref{fig:fey}C). Readers are
invited to refer to the original article for details on model
calibration or binarization of network descriptors
\citep{fey2015signaling}. The authors also showed that in the absence of
positive feedback from \(JNK^{**}\) to \(^PMKK7\), an important
component of non-linearity, the prognostic value is drastically
decreased. All in all, this pipeline from ODE model to survival curves,
thus provides a \textbf{paradigmatic example of the clinical
interpretation of mechanistic models of molecular networks} that will be
reused in later chapters for illustration purposes. Other ODE models
following a similar rationale have been proposed by the same group for
colorectal cancer \citep{hector2012clinical, salvucci2017stepwise} or
glioblastoma \citep{murphy2013activation, salvucci2019system}. Machine
learning approaches have also been proposed to ease the clinical
implementation of this kind of prognostic models by dealing with the
potential lack of patient data needed to personalize them
\citep{salvucci2019machine}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/fey} 

}

\caption[Mechanistic modeling of JNK pathway and survival of neuroblastoma patients, as described by Fey et al.]{\textbf{Mechanistic modeling of JNK pathway and
survival of neuroblastoma patients, as described by
\citet{fey2015signaling}.} (A) Schematic representation, as a process
description, for the ODE model of JNK pathway. (B) Response curve
(phosphorylated JNK) as a function of the input stimulus (Stress) and
characterization of the corresponding sigmoidal function with maximal
amplitude \(A\), Hill exponent \(H\) and activation threshold
\(K_{50}\). (C) Survival curves for neuroblastoma patients based on
binarized \(A\), \(K_{50}\) and \(H\); binarization thresholds having
been defined based on optimization screening on calibration cohort.}\label{fig:fey}
\end{figure}












On the logical modeling side, there are also studies including
prognostic value validation. Thus, \citet{khan2017unraveling} proposed
two logical models of epithelio-mesenchymal transition (EMT) in bladder
and breast cancers. These models are inferred from prior mechanisms
knowledge and data analysis with particular attention to potential
feedback loops. Using these models, it is possible to study the
behaviour of them for all combinations of model inputs (growth factors
and receptor proteins) and derive subsequent signatures for good or bad
prognosis. These signatures are later validated with cohorts of
patients. In this case, the mechanistic model does not seek to capture a
dynamic behavior but to facilitate and \textbf{make understandable the
exploration of combinations of input signals that grow exponentially
with the number of inputs considered}. Other formalisms, called pathway
activity analysis and following the same activity flows principles
(Figure \ref{fig:toyraf}A), have been analysed in the light of their
prognostic value. Their greater flexibility enables the direct use of
networks of several hundred or thousands of genes, such as those present
in the KEGG database \citep{kanehisa2012kegg}. The benefit of
mechanistic modeling is then to organize high-dimensional data and to
facilitate the \emph{a posteriori} analysis of the results.

\subsection{Predictive models}\label{predictive}

But the explicit representation of biological entities in mechanistic
models makes them particularly \textbf{suitable for the study of
well-defined perturbations such as drug effects}. Indeed, by assuming
that the mechanism of action of a drug is at least partially known, it
is possible to integrate this mechanism into the model if it contains
the target of the drug (Figure \ref{fig:netdrug}). One can therefore
simulate the effect of one drug or even compare several. These
strategies have already been implemented in a qualitative way with
logical models used to explain resistance to certain treatments of
breast cancer \citep{zanudo2017network} or even highlight the synergy of
certain combinations of treatments in gastric cancer
\citep{flobak2015discovery}. The value of these models, however, is more
scientific than clinical in that they focus on a single cell line or a
restricted group of cell lines. The possibility to personalize the
predictions or recommendations for different molecular profiles of cell
lines or patients is therefore not obvious. Still within the context of
logical formalism, \citet{knijnenburg2016logic} proposed a broader
approach: if their model needs to be trained, it can nevertheless
provide an analytical framework for several hundred cell lines, while
remaining within the scope of the training data to ensure the validity
of predictions.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/netdrug} 

}

\caption[Network model of oncogenic signal transduction in ER+ breast cancer, including some drugs and their targets]{\textbf{Network model of oncogenic signal
transduction in ER+ breast cancer, including some drugs and their
targets.} Reprinted from \citet{zanudo2017network}.}\label{fig:netdrug}
\end{figure}





Conceptually comparable strategies can be found on the side of
differential equations where large mechanistic models of cell signalling
are also trained to predict the response to different treatments
\citep{bouhaddou2018mechanistic, frohlich2018efficient}. A calibrated
model can then predict the response to a combination of treatments not
tested in the training data, thereby proving the ability of mechanistic
models to extend their predictive value beyond the data
\citep{frohlich2018efficient}. As with prognostic models, mechanistic
approaches other than logical formalisms and ODEs have been proposed and
validated \citep{jastrzebski2018integrative}. What can be learned from
these predictive models is that they require \textbf{significant
training data to be able to go beyond qualitative predictions and
dissect treatment response mechanisms of many cell lines
simultaneously}. For obvious practical and ethical reasons, the
validation of these models is for the moment limited to preclinical data
since they require data for many uncertain therapeutic interventions.

\subsection{Mechanistic models, interventions and
causality}\label{mechanistic-models-interventions-and-causality}

To conclude this first part in a broader way, it is interesting to note
that the now complete description of mechanistic models of cancer makes
it possible to revisit their characteristics from a different point of
view and to link them to the statistical approaches that will be the
subject of the third part of the dissertation. First of all, it should
be remembered that statistical models only highlight associations
between biological variables or entities and not causal relationships.
On the other hand, by explicitly constraining the structure of relations
between variables, mechanistic models become less flexible flexibility
but already propose a causal interpretation. Therefore, the
\textbf{notion of causality is intrinsically embedded in the definition
of the mechanistic model} (section \ref{stat-mech}).

However, if causality is not a by-product when using a statistical or
machine learning model, it is possible to access it through specific
experimental designs, such as randomized clinical trials, or applying
dedicated statistical methods \citep{hernan2020causal}, as described
later on in chapter \ref{causal-chap}. In both cases, the aim is to
compare the effect of a treatment, or more generally of an intervention,
on two groups that are as similar as possible to each other in order to
isolate the specific, causal effect of the intervention on outcome.
Schematically, the identification of causal links can be likened to the
study of well-defined interventions on patients: it is a question of
being able to act in a relevant and specific way on a variable and to
measure the consequences. The notion of intervention is thus very
present in the literature on causality
\citep{eberhardt2007interventions} and was summarized by
\citet{holland1986statistics} in a concise manner: ``no causation
without manipulation''.

In this respect, without being part of the same statistical framework at
all, mechanistic models offer an interesting parallel. As suggested in
the previous paragraph, they allow us to test the effect of certain
interventions: how does the model behave with or without the addition of
a drug to its structure? This ability to study the effect of targeted
interventions again contributes, in a slightly different way, to the
understanding of the system. The particularity of mechanistic models,
once they have been validated, is that they can study the effect of
interventions for which no data are available. In any case, a common
point between the mechanistic approaches of the second part of the
thesis and the statistical approaches of causal inference of the third
part is \textbf{to question the mechanisms and true causes at work
throughout the cancer modeling process, from the biological question to
clinical validation}. It is indeed at this price that one can reach the
level of understanding and confidence required for a real world
application.

This first bridge between mechanistic models of cell signalling and
clinical applications concludes this introductory part. The next part
will be devoted to the definition of new methods to establish this
connection based on logical formalism, before the third part proposes a
more statistical evaluation of the prognostic and predictive values of
the models presented in the previous parts.

\BeginKnitrBlock{conclubox}
\subsubsection*{Summary}\label{summary-2}
\addcontentsline{toc}{subsubsection}{Summary}

The biological knowledge gathered on cancer enables to propose
mechanistic models at different scales. This thesis focuses on the
molecular level in a bottom-up approach based on the biochemical
behaviors described in the literature. These models have in common the
mathematical translation of the activation and inhibition networks that
link all biological entities within a cancer cell. Different
mathematical formalisms exist, including differential equations
(continuous and quantitative models) and logical methods (discrete and
qualitative models) which are among the most frequent and will be used
as examples. For these formalisms as for the others, stochastic
approaches are often proposed to translate tumor mechanisms. Most of
these mechanistic models have a purely theoretical or investigative
purpose but some are used to predict the evolution or response to cancer
treatment, they are then respectively prognostic or predictive.
\EndKnitrBlock{conclubox}

\part{Personalized logical models of
cancer}\label{part-personalized-logical-models-of-cancer}

\chapter{Logical modeling principles and data
integration}\label{logical-modeling-principles-and-data-integration}

\epigraph{"Je suis l’halluciné de la forêt des Nombres.

Ils me fixent, avec leurs yeux de leurs problèmes ;

Ils sont, pour éternellement rester : les mêmes.

Primordiaux et définis,

Ils tiennent le monde entre leurs infinis ;

Ils expliquent le fond et l’essence des choses,

Puisqu’à travers les temps planent leurs causes."}{Émile Verhaeren (Les nombres, in Les Flambeaux noirs, 1891)}

\initial{A}nother way of ordering the diversity of mechanistic models
presented above is to consider their relationship to biological data.
Those that make little use of these data are essentially theoretical
scope models that describe the general functioning of signaling pathways
and associated systems \citep{calzone2010mathematical}. Other models
propose more quantitative models but require much more data, either from
databases or experimental data generated for this purpose in order to
fit the parameters. In the latter case, the necessary data is usually
perturbation data: how does my system react to this or that inhibition
or activation? For a single cell line this already corresponds to a
large amount of data \citep{razzaq2018computational}. And if we want to
extend these approaches to many cell lines, the amount of data becomes
massive \citep{frohlich2018efficient}. For patient-specific models,
access to this perturbation data is even more difficult.

Between theoretical models that are not very demanding in terms of data
but not very applicable clinically and models with a clinical focus but
very demanding in terms of data, an intermediate alternative is missing.
\textbf{Can patient-specific mechanistic models be developed that would
provide qualitative clinical interpretation with a small amount of data,
accessible even in patients?} In this part, composed of three chapters,
a middle way will be described to answer positively to this question.
This methodology will be based on a historically qualitative
mathematical formalism already presented in the previous chapter under
the name of logical modeling. Logical modeling in general will be
detailed in this chapter before describing an original personalized
approach in the next two chapters.

\BeginKnitrBlock{summarybox}
\subsubsection*{Scientific content}\label{scientific-content-1}
\addcontentsline{toc}{subsubsection}{Scientific content}

This chapter presents the theoretical bases of logical modeling and the
tools used thereafter. The content is taken entirely from the literature
and the personal presentation that is made is the one that was used to
introduce article \citet{beal2019personalization} and to organize review
\citet{beal2020modelisation}.
\EndKnitrBlock{summarybox}

\section{Logical modeling paradigms for qualitative
description}\label{logical-modeling-paradigms-for-qualitative-description}

Mathematical models serve as tools to answer a biological question in a
formal way, to detect blind spots and thus better understand a system,
to organize, into a consensual and compact manner, information dispersed
in different articles. In the light of this definition, logical
formalism may seem one of the closest to natural language in that it
\textbf{can translate quite directly the statements present in the
literature} such as ``protein A activates protein B'' or ``the
expression of gene C requires the joint presence of factors D and E''.
Indeed, shortly after the first descriptions of control circuits by
\citet{jacob1961genetic}, the interest of logical models to describe
biological systems was put forward by \citet{kauffman1969homeostasis}
and \citet{thomas1973boolean}. Since then, studies have multiplied
\citep{thomas1990biological}, varying the fields of biological
applications and also the mathematical and computational implementations
\citep{naldi2018colomoto}. The two subsections below summarize the
characteristics common to most of the logical formalisms, before
detailing the implementation chosen in this thesis in section
\ref{maboss-section}. A review of the use of data in logic models will
finally be proposed in section \ref{logical-data-section}.

\subsection{Regulatory graph and logical
rules}\label{regulatory-graph-and-logical-rules}

A logical model is based on a network called \textbf{regulatory graph}
(Figure \ref{fig:logical}), where each \textbf{node} represents a
component (e.g.~genes, proteins, complexes, phenotypes or processes),
and is associated with discrete levels of activity (\(0\), \(1\), or
more when justified). The use of a discrete formalism in molecular
network modeling relies on the highly non-linear nature of regulation,
and thus on the existence of a regulatory threshold. Assuming that each
variable represents a level of expression, it will take the value \(0\)
if the level of expression of the entity is below the regulation
threshold, i.e., insufficient to carry out the regulation; and the value
\(1\) if it is above the threshold and regulation is possible. In other
words, the control threshold discretizes the state space, here the
expression levels. It is therefore possible to distinguish several
thresholds for the same variable, corresponding to distinct controls
that do not take place at the same expression levels. The variable is
then multivalued. This extension greatly enriches the formalism, because
it allows to distinguish situations that are qualitatively different and
that would be confused with Boolean variables. In the continuation of
this thesis, we will consider by default that the activity levels are
binary, \(0\) corresponding to an inactive entity and \(1\) to an active
entity. The \textbf{edges} of this regulatory graph correspond to
influences, either positive or negative, which illustrate the possible
interactions between two entities (Figure \ref{fig:logical}). Positive
edges can represent the formation of active complexes, mediation of
synthesis, catalysis, etc. and they will be later depicted as green
arrows (\(\leftarrow\)). Negative edges on the other hand can represent
inhibition of synthesis, degradation, inhibiting (de)phosphorylation,
etc. and they will be depicted as red turnstiles (\(\vdash\)).

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{fig/logical} 

}

\caption[A simple example of a logical model]{\textbf{A simple example of a logical model.}
Regulatory graph on the left with positive (green) and negative
regulations (red); a set of possible corresponding logical rules on the
right.}\label{fig:logical}
\end{figure}






Then, each node of the regulatory graph has a corresponding Boolean
variable associated to it. The variables can take two values: \(0\) for
absent or inactive (OFF), and \(1\) for present or active (ON). These
variables change their value according to a logical rule assigned to
them. The state of a variable will thus depend on its \textbf{logical
rule}, which is based on logical statements, i.e., on a function of the
node regulators linked with logical connectors AND (\(\&\)), OR (\(|\))
and NOT (\(!\)). These operators can account for what is known about the
biology behind these edges. If two input nodes are needed for the
activation of the target node, they will be linked by an AND gate; to
list different means of activation of a node, an OR gate will be used;
and negative influences will rely on NOT gates. The rules corresponding
to the toy model in Figure \ref{fig:logical} could be interpreted
literally like this: A is activated to 1 if B is active; B is updated to
1 in the absence of A and the presence/activity of C; C is an input of
the model and therefore not regulated. It can be noted that the logical
rules cannot be deduced only from the regulatory graph, which is less
precise and ambiguous. One could thus imagine that B is activated if C
is, OR if A is not, thus changing the behavior of the model.

\subsection{State transition graph and
updates}\label{state-transition-graph-and-updates}

In a Boolean framework, the variables associated to each node can take
two values, either \(0\) or \(1\). We define a model state as a vector
of all node states. All the possible transitions from any model state to
another are dependent on the set of logical rules that define the model.
These transitions can be viewed into a graph called a \textbf{state
transition graph} (STG), where nodes are model states and edges are the
transitions from one model state to another. STG nodes will be later
depicted with rounded squares instead of circles in order to emphasize
the difference with regulatory graphs. That way, trajectories from an
initial condition to all the final states can be determined. In a model
with \(n\) nodes, the STG can contain up to \(2^n\) model state nodes;
thus, if \(n\) is too big, the construction and the visualization of the
graph become difficult.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/synchronous} 

}

\caption[State transition graph and synchronous updates]{\textbf{State transition graph and synchronous
updates.} Stable state (A) and limit cycle (B) attractors obtained for
the example logical model with synchronous updates (all possible updates
simultaneously). Figures above/below STG edges correspond to the number
of nodes updated in each transition.}\label{fig:synchronous}
\end{figure}







Based the simple logical model of Figure \ref{fig:logical} it is
nevertheless possible to represent the STG comprehensively. The idea for
this is to start from a state of the system and track the successive
states defined by the logical rules and the corresponding updates. The
first strategy to construct this STG is to change simultaneously at each
time step all the variables that can be changed (Figure
\ref{fig:synchronous}). This method is referred to as a
\textbf{synchronous updating strategy}. In the second method, referred
to as a \textbf{asynchronous updating strategy}, variables are changed
one at a time (Figure \ref{fig:asynchronous}) and therefore each state
has as many successors as there are components whose state must be
changed according to logical rules (Figure \ref{fig:asynchronous}A).
Performing only one transition at a time implies having to choose this
transition between the different possible options, which can be done
according to pre-established rules or stochastically as explained in
section \ref{maboss-section}. The latter asynchronous method will be
used exclusively in the work presented thereafter.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/asynchronous} 

}

\caption[State transition graph and asynchronous updates]{\textbf{State transition graph and
asynchronous updates.} Stable state (A) and limit cycle (B) attractors
obtained for the example logical model with asynchronous updates (one
update at a time). Figures above/below STG edges correspond to the
number of nodes updated in each transition.}\label{fig:asynchronous}
\end{figure}







We then define attractors of the model as long-term asymptotic behaviors
of the system. Two types of attractors are identified: stable states,
when the system has reached a model state whose successor in the
transition graph is the model state itself; and cyclic attractors, when
trajectories in the transition graph lead to a group of model states
that are cycling. For both synchronous and asynchronous updating
strategies, the toy model shows the existence of \textbf{two types of
attractors: a stable steady state and a limit cycle}, depending on the
initial value of \(C\). There are two disconnected components of the STG
for this example that correspond to the two possible values for the
input \(C\). If \(C\) is initially equal to 0 (inactive), then there
exists only one stable state: \(A=B=C=0\). All the trajectories in the
state transition graph lead to a single final model state. If \(C\) is
initially equal to 1, then the attractor is a limit cycle. The path in
the STG cycles for any initial model state of this connected component.
Note that for the asynchronous and synchronous graphs, the precise paths
or limit cycles may differ. To conclude, it is important to emphasize
and illustrate the characteristics of asynchronous updates in this toy
example. In Figure \ref{fig:asynchronous}A, the transition from the
initial state (\(A=C=0;B=1\)) suggests two distinct possibilities, so it
is necessary to \textbf{define additional rules or heuristics to choose
between possible transitions}. We will come back to this by specifying
the logical modeling implementation chosen in this thesis in section
\ref{maboss-section}, in which a stochastic exploration of these
alternatives is proposed.

\subsection{Tools for logical modeling}\label{logical-tools-list}

Numerous tools have been developed to build logical models and study the
dynamics of the systems under investigation, each with its own
specificity. They allow, for example, to represent regulation networks;
to edit, modify or infer logical rules; to identify stable states; to
reduce models; to visualize graphs of synchronous or asynchronous
transitions. Some also allow to integrate temporal data; to discretize
expression data; to simulate the model stochastically or to integrate
delays; to identify existing models, etc. Among them, we can cite GINsim
\citep{naldi2018logical}, BoolNet \citep{mussel2010boolnet}, pyBoolNet
\citep{klarner2016pyboolnet}, BooleanNet \citep{albert2008boolean},
CellCollective \citep{helikar2012cell}, bioLQM \citep{naldi2018biolqm},
MaBoSS \citep{stoll2012continuous, stoll2017maboss}, PINT \citep{Pint},
CaspoTS \citep{ostrowski2016boolean}, or CellNOptR
\citep{terfve2012cellnoptr}. The interaction between all these tools,
their interoperability and complementarity are highlighted in the form
of a notebook Jupyter \citep{naldi2018colomoto}, and some of them are
described in more details in section \ref{logical-data-section}.

\section{The MaBoSS framework for logical
modeling}\label{maboss-section}

In the present study, all simulations have been performed with MaBoSS, a
\textbf{Ma}rkovian \textbf{Bo}olean \textbf{S}tochastic
\textbf{S}imulator whose design is summarized in Figure \ref{fig:maboss}
and precisely described by \citet{stoll2012continuous} and
\citet{stoll2017maboss}. This framework is based on an asynchronous
update scheme combined with a continuous time feature obtained with
Gillespie algorithm \citep{gillespie1976general}, allowing simulations
to be continuous in time despite the discrete nature of logical
modeling.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{fig/maboss} 

}

\caption[Main principles of MaBoSS simulation framework and Gillespie algorithm]{\textbf{Main principles of MaBoSS simulation
framework and Gillespie algorithm.} (A) A logical model with regulatory
graph, logical rules and transition rates. (B) A subset of the
corresponding state transition graph with two possible transitions in
asynchronous update for a given initial state; the probability
probability associated with each transition comes from Gillespie
algorithm. (C) Schematic diagram of the Gillespie algorithm applied to
the asynchronous transition depicted in (B); random selection of a
specific transition and time by the algorithm from two uniform random
variables. (D) Schematic representation of a logical model simulation
with MaBoSS: average trajectory obtained from the mean of many
individual stochastic trajectories, each resulting from the succession
of transitions as represented in (C).}\label{fig:maboss}
\end{figure}















\subsection{Continuous-time Markov
processes}\label{continuous-time-markov-processes}

The implementation of asynchronous updates proposed by the MaBoSS
software is based on \textbf{continuous time Markov processes applied on
a Boolean state space} and is therefore part of the large family of
stochastic models of cancer outlined in section
\ref{mechanistic-family}. The precise relations of this software with
the generic results on Markovian processes, and the associated
demonstrations, are available in the original publication by
\citet{stoll2012continuous} and detailed in particular in the related
supplementary document\footnote{Additional information on Markov
  processes and MaBoSS in the following
  \href{https://static-content.springer.com/esm/art\%3A10.1186\%2F1752-0509-6-116/MediaObjects/12918_2012_969_MOESM1_ESM.pdf}{document}}.
The essence of the formalism, as set out in this article, is outlined
below.

Before returning to the simplified example of Figure \ref{fig:maboss},
it is possible to describe the general scheme of Markov processes and
Gillespie algorithm for simulating logical models. We restrict ourselves
here to a Boolean model (binary variables), composed of \(n\) variables.
The network state of the system is a vector \(S\) of binary values such
as \(S_i \in \{0, 1\}\), with \(i=1,...,n\) and \(S_i\) representing the
state of the node \(i\). The network state space of all possible network
states is called \(\Omega\). The evolution of the state of the system
can be represented by a stochastic process \(s:t \rightarrow s(t)\)
defined on \(t \in I \subset \mathbb{R}\) applied on the network state
space. For each time \(t \in I \subset \mathbb{R}\), \(s(t)\) represents
a random variable applied on the network state space. Thus,
\(P[s(t)=S]\in[0, 1], \forall S \in \Omega\) and
\(\sum_{S \in \Omega} P[s(t)=S] = 1\). This \textbf{stochastic process
verifies the Markov property which stipulates the absence of memory},
\emph{i.e.}, the conditional probability distribution of future states
of the process depends only on the present state, not on the sequence of
past states. The resulting stochastic process, called Markov process is
defined by an initial condition \(P[s(0)=S], \forall S \in \Omega\) and
the conditional probabilities
\(P[s(t)=S|s(t')=S'], \forall S,S' \in \Omega, \forall t,t' \in I, t'<t\).
In this work we will focus on continuous time Markov processes where it
has been shown that all conditional probabilities are functions of
transition rates \(\rho_{(S' \rightarrow S)} (t) \in [0, \infty[\)
\citep{van2004stochastic}. Only the case of \textbf{time independent
transition rates} (and Markov processes) will be explored below.

It is then possible to re-state the description of the logical models in
this formalism. One of the first representations of the model is the
regulatory network with a set of directed arrows linking the \(n\) nodes
(Figure \ref{fig:maboss}A, left column). For each node \(i\), a logical
rule \(L_i(S)\) is defined based only on the nodes \(j\) for which there
exists an arrow from node \(j\) to \(i\), as represented in Figure
\ref{fig:maboss}A middle column, \emph{e.g.},
\(L_B = (\text{NOT}~A)~\text{AND}~(C)\) with \(L_B\) the logical rule of
node \(B\). The notion of \textbf{asynchronous transition} can then be
defined as a pair of network states \((S, S') \in \Omega\), written
\((S \rightarrow S')\) such that:

\[S'_j=L_B(S) \text{ for a given } j\]
\[S'_i=S_i \text{ for } i \neq j\]

This means that during an asynchronous transition, only one node changes
state, among those whose logical rule allows it. In Figure
\ref{fig:maboss}B, two possible asynchronous transitions from the same
initial state are represented. Then, transition rates
\(\rho_{(S \rightarrow S')}\) are non-zero only if \(S\) and \(S'\)
differ by only one node. In that case, each Boolean logic \(L_i(S)\) is
replaced by two functions
\(k_{0 \rightarrow 1 / 1 \rightarrow 0}^{i}(S) \in [0, \infty[\). The
transition rates are defined as follows: if \(i\) is the node that
differs from \(S\) and \(S'\), then:

\[\rho_{(S \rightarrow S')}=k_{0 \rightarrow 1}^{i}(S) \text{ if } S_i=0\]
\[\rho_{(S \rightarrow S')}=k_{1 \rightarrow 0}^{i}(S) \text{ if } S_i=1\]

Therefore, the \textbf{continuous Markov process is completely defined
by all these \(k^i\) and an initial condition}. The state transition
graph can also be re-defined as a graph in \(\Omega\), with an edge
between \(S\) and \(S'\) if and only if \(\rho_{S \rightarrow S'} > 0\).

\subsection{Gillespie algorithm}\label{gillespie-algorithm}

Although the Markov process is completely defined, the cost of its
theoretical resolution increases exponentially with the number of nodes
since the transition matrix of the system is of size \(2^n \times 2^n\).
Thus, although the exact resolution of these Markov processes from their
master equation has been well described \citep{van2004stochastic}, their
computational cost quickly becomes too high. As an example, a recent
method for the exact resolution of these stochastic processes applied to
Boolean models is limited to models of about 20 nodes
\citep{koltai2020exact}. One solution consists in \textbf{sampling the
probability space by simulating time trajectories in the state
transition graph}, which is what MaBoSS performs through Gillespie
algorithm, also called kinetic Monte-Carlo. The principle of the
algorithm is to compute a finite set of individual stochastic
trajectories of the Markov process and to use them to infer
probabilities for the given Markov process (Figure \ref{fig:maboss}D).
For the sake of simplicity the remainder of this section will describe
the simulation of a single individual stochastic trajectory, first with
a focus on the formal description of the algorithm and then with a more
qualitative explanation.

A stochastic trajectory \(\hat{S}(t)\) is a function from a predefined
time interval \([ 0, t_{max}]\) to \(\Omega\). The iterative computation
of the trajectory is obtained as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  From a state \(S\) at \(t_0\), sum the rates of all possible
  transitions for leaving this state:
  \(\rho_{total}=\sum_{S'}{\rho_{(S \rightarrow S')}}\)
\item
  Draw two uniform random numbers \(u,u' \in [0,1]\)
\item
  Compute the transition time \(\delta t=-log(u)/\rho_{total}\)
\item
  Order the potential target states \(S'^{(j)}\) and their corresponding
  transition rates \(\rho^{(j)}=\rho_{(S \rightarrow S'^{j})}\)
\item
  Choose the new state \(S'=S'^{(k)}\) such that
  \(\sum_{j=0}^{k-1} \rho^{(j)} < u'\rho_{total} < \sum_{j=0}^{k} \rho^{(j)}\),
  with \(\rho^{(0)}=0\)
\item
  Define the trajectory \(\hat{S}(t)\) such as \(\hat{S}(t)=S\) for
  \(t \in [t_0, t_0 + \delta t[\) and \(\hat{S}(t_0 + \delta t) = S'\)
\item
  Repeat iteratively from \(S'\) until \(t_{max}\) is reached
\end{enumerate}

Thus, Gillespie algorithm provides a \textbf{stochastic way to choose a
specific transition among several possible ones} and to infer a
corresponding time for this transition. To achieve this, transition
rates seen as qualitative activation or inactivation rates, must be
specified for each node (Figure \ref{fig:maboss}A). They can be set
either all to the same value by default, in the absence of any
indication, or in various levels reflecting different orders of
magnitude: post-translational modifications are quicker than
transcriptions for instance. These transition rates are translated as
\textbf{transition probabilities} in order to determine the actual
transition (Figure \ref{fig:maboss}B). Indeed, the probability for each
possible transition to be chosen for the next update is the ratio of its
transition rate to the sum of rates of all possible transitions. Higher
rates correspond to transitions that will take place with greater
probability, or in other words more quickly. At each update, the
Gillespie algorithm performs the procedure described above and depicted
schematically in Figure \ref{fig:maboss}C. Two uniform random variables
\(u\) and \(u'\) are drawn and used respectively to select the
transition among the different possibilities (with \(u\)) and to infer
the corresponding time (with \(u'\)). Based on the described formula,
time \(\delta t\) follows an exponential law whose average is equal to
the inverse of the sum of all possible transition rates (Figure
\ref{fig:maboss}C). In the present work, except otherwise stated, all
transition states will be initially assigned to 1.

\subsection{A stochastic exploration of model
behaviours}\label{a-stochastic-exploration-of-model-behaviours}

Since MaBoSS computes stochastic trajectories, it is relevant to compute
several trajectories in order to get an insight of the average behavior
by generating a \textbf{population of stochastic trajectories} over the
asynchronous state transition graph (Figure \ref{fig:maboss}D). The
aggregation of stochastic trajectories can also be interpreted as a
\textbf{description of a heterogeneous population}. Thus, in all the
examples in next chapters, all simulations have consisted of thousands
of computed trajectories. The larger the model, the larger the space of
possibilities and the more trajectories are required to explore it.
Since several trajectories are simulated, initial values of each node
can be defined with a continuous value between 0 and 1 representing the
probability for the node to be defined to 1 for each new trajectory. For
instance, a node with a \(0.6\) initial condition will be set to \(1\)
in \(60\%\) of simulated trajectories and to \(0\) in \(40\%\) of the
cases.

In the present work, we will focus on the \emph{asymptotic} state of
these simulations instead of transient dynamics and we will call
\textbf{node scores} the asymptotic agregated score obtained by
averaging all trajectories at a given final time point. Indeed,
asymptotic states are more closely related to logical model attractors
than transient dynamics and are therefore less dependent on updating
stochasticity and more biologically meaningful \citep{huang2009cancer}.
Note that the simulation time should be chosen carefully to ensure that
the asymptotic state is achieved, and the term \emph{final state} may be
considered as safer. All in all this modeling framework is at the
intersection of logical modeling and continuous dynamic modeling. If the
definition of time remains rather abstract and difficult to interpret
experimentally, the stochastic exploration of trajectories makes it
possible to refine the purely binary interpretation of the variables.
Finally, it is important to point out that while the following chapters
will focus on the MaBoSS tool, it is not exclusive but on the contrary
very complementary to the other logical model analysis tools mentioned
in section \ref{logical-tools-list}, or even to the methods specific to
Markov processes in particular. The choice of the tools will be made
according to the biological questions and the associated analyses:
estimation of attractors or basins of attraction, exact resolution,
study of transient dynamics, etc.

\subsection{From theoretical models to data
models?}\label{from-theoretical-models-to-data-models}

To sum up, logical formalism makes it possible to design qualitative
models that reflect \emph{a priori} knowledge of the phenomena being
studied. Thus, they allow answering questions for which there is little
quantitative information on the precise mechanisms involved in a
disease, for instance a lack of data related to the expression of genes,
the quantity of key proteins or the speed of certain processes. Logical
models can confirm that a network is a good illustration of the
underlying biological question. However, the construction of the model
from the literature often results in a generic consensus network that
cannot explain the differences observed between certain patients with
different molecular profiles. In order to propose a patient-specific
mechanistic approach, it seems crucial to \textbf{use the biological
data} available. How is this possible in a formalism that is by
definition quite abstract?

\section{Data integration and semi-quantitative logical
modeling}\label{logical-data-section}

The higher level of abstraction of the logical formalism sometimes makes
the necessary back and forth between theoretical modeling and
experimental or clinical data less easy. However, many theoretical
approaches have been developed over the years to enable this dialogue at
all stages, from the construction to the validation of a logical model,
as summarized in Figure \ref{fig:logical-data}. This section summarizes
some of these approaches to show \textbf{how the use of biological data
enriches logical models and brings them closer to clinical applications}
in precision medicine. The purpose of this presentation is also to
better contextualize the original approach presented in the following
chapter. It should be noted that the methods presented below are all
applicable to logical models, and illustrated with such examples where
possible. However, some methods are not specific to this formalism and
can be applied to other modeling frameworks.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/logical-data} 

}

\caption[Data integration in logical modeling]{\textbf{Data integration in logical
modeling.} The main types of data used are shown on the left; the
essential steps of the logical modeling are shown linearly on the right.}\label{fig:logical-data}
\end{figure}





\subsection{Build the regulatory
graph}\label{build-the-regulatory-graph}

Faced with a biological question (Figure \ref{fig:logical-data}, first
step), it is crucial to identify the main actors in the process in order
to \textbf{define the outline of the model} (Figure
\ref{fig:logical-data}, second step). A first approach relies on the
existing scientific literature on the topic: which biological species
and which interactions have been identified as relevant to my problem?
In a more automatic way, it is possible to extract information from
different databases in order to establish an initial list of biological
entities and interactions associated with a biological phenomenon or
even a gene of interest \citep{kanehisa2012kegg, perfetto2016signor}. As
an example, starting from the study of E2F1 gene as the hub of many
regulatory mechanisms, \citet{khan2017unraveling} have reconstructed a
dense network of interactions in the vicinity of E2F1, which will be
used for the construction of their subsequent model. The main difficulty
here is to choose and select the relevant biological information adapted
to the context of the model to be created, depending for example on the
type of cancer studied or the desired level of precision.

But if the literature can be considered as processed data, it is also
possible to use directly experimental data related to the problem under
study. Key actors of biological processes identified by statistical
analysis, such as differentially expressed genes or the most frequently
mutated genes in a patient cohort, are selected and used as a starting
point for the construction of the model \citep{remy2015modeling}. More
comprehensive approaches can use differential analysis tools on
signaling pathways, rather than individual genes, to choose the relevant
processes to include by contrasting different groups of patients based
on their grades, metstatic status, resistance to treatments etc.
\citep{martignetti2016roma, montagud2017conceptual}. Similarly, the
study of regulatory networks involving transcription factors may justify
the use of ChIP-seq data to identify possible new transcriptional
regulations not previously listed \citep{collombet2017logical}.

Once the main actors have been identified, it is necessary to
\textbf{infer the links} between them (Figure \ref{fig:logical-data},
third and fourth step). However, starting from a list of genes and
proteins of interest, how can we ensure that the regulatory
relationships are complete and relevant? While a careful reading of the
literature can provide locally interesting information, the use of omics
data is also a resource that can be broken down into different levels of
precision. The major interest of these methods, assuming that the data
are adequate and sufficiently massive, is to be able to extract
information as large as the dataset, potentially on hundreds of
entities, and above all specific to the object of study: a cancer
subtype or a particular cell line can thus generate their own
interaction network \citep{lefebvre2010human}. Inference methods extract
biological knowledge hidden in large databases, summarize it and
represent it via networks. Many methods construct coexpression networks,
which are non-oriented graphs, with different metrics and methods
\citep{margolin2006aracne, vert2007new}. Other approaches seek to infer
causal relations between components, allowing the reconstruction of
directed graphs where the links between entities are oriented, and
sometimes even signed as activating (positive) or inhibiting (negative)
regulations. These methods often make use of time series
\citep{hill2016inferring} or perturbation data
\citep{meinshausen2016methods}, but also more recently from
observational data \citep{verny2017learning}. The information extracted
from the data is then directly readable in the form of activity flows as
described in the SBGN standards \citep{novere2009systems}, thus
providing a representation adapted to the construction of qualitative
models and \emph{a fortiori} of logical models
\citep{le2015quantitative}. Closer to the objective of defining logical
models, certain methods allow the study and inference of co-regulation
expressed with logical operators \citep{elati2007licorn}, thus
facilitating the passage from the definition of an interaction network
to the construction of a true logical model.

\subsection{Define the logical rules}\label{define-the-logical-rules}

Precision must then be taken further by defining the logical rules that
complete the network (Figure \ref{fig:logical-data}, fifth step). The
first source of aggregated data to define logical rules is the
scientific literature. The modeler looks for the state of knowledge on a
given regulatory mechanism and translates it into a \textbf{local
logical rule}, according to the desired level of precision. For example,
it has been observed that the protein kinase AKT can stabilize the
oncogene MDM2 by phosphorylation, which leads to the degradation of p53
by forming a complex with it: this example can be translated by a simple
inhibition relationship of AKT on p53 if this level of precision is
considered sufficient or else intermediate species such as MDM2 can be
used \citep{cohen2015mathematical}. Then, the effect of inhibition must
be defined: can MDM2 alone inhibit p53 or does the presence of other
activators outweigh this effect? This kind of considerations allows to
define the logical combinations between the different inputs of a
network node. In some cases, experimental data can be used to answer
such questions: is a single activator sufficient or is the presence of
all activators necessary? Which of the activator or inhibitor prevails
in the case of simultaneous presence? While this information is often
found in the literature, one should generate one's own experimental data
to ensure an answer tailored to the study context, using a variety of
experimental molecular biology techniques. For example, in order to
elucidate the relationship between Foxo1 and Cebpa in a model of
differentiation of myeloid and lymphoid cells,
\citet{collombet2017logical} first established the physical relationship
between these species by ChIP-seq before determining the nature of this
relationship using an ectopic expression experiment of Foxo1 in
macrophage cells.

Other, more global approaches have been developed in recent years,
driven by the influx of data from high-throughput sequencing techniques.
Based on this rich and complex data, it has become possible to
\textbf{infer entire logical models, with precisely defined rules and
interactions} \citep{ostrowski2016boolean}. The algorithms CellNOpt
\citep{terfve2012cellnoptr} and caspo \citep{videla2017caspo} provide
two examples of these approaches, and more recently the SCNS tool
described a graphical interface to infer logical models from single cell
data \citep{woodhouse2018scns}. This model-inference goes beyond simpler
structure-inference by defining the logical rules, but it is generally
based on a predefined topological structure to which time series or
perturbation data are added. These data provide access to the response
dynamics of a system. By questioning the way the system reacts, these
data are therefore richer than a snapshot and thus facilitate the
transition from correlation to causality, and thus the inference of
logical rules. In practice, the use of proteomic or phospho-proteomic
data is often recommended because these data account for the activity of
the protein and are in fact the closest to the cellular response
\citep{ostrowski2016boolean, terfve2012cellnoptr, terfve2015largescale}.
In spite of the richness of this type of data, model inference is
sometimes still an under-determined problem that can lead to a large
number of models with different logical rules equally compatible with
the data. In such situations, it is then a matter of choosing the model
on the basis of biological relevance criteria or of accepting to use
families of models, or ensemble models, instead of limiting oneself to a
single model \citep{videla2017caspo}. In all cases, constructing logical
rules directly from data specific to the problem can make it possible to
obtain logical rules that are also specific to the context or the system
under study \citep{saezrodriguez2011comparing}. For example, the
inference of logical models specific to one or some cancer cell lines is
a powerful tool to study their particularities
\citep{razzaq2018computational}.

\subsection{Validate the model}\label{validate-the-model}

Finally, the data can be used to validate the \textbf{biological or
clinical relevance of the models} (Figure \ref{fig:logical-data}, sixth
step). Compared to a system of differential equations, logical modeling
has the particularity of being more abstract and therefore less directly
reliable to an experimental reality for its validation. A system of
differential equations can be compared to the chemical kinetics of the
biological system under study. Compared to continuous formalisms, the
dynamics of logic model simulation is more difficult to take into
account but it is possible to verify it qualitatively, for example by
validating the cyclic nature of activation trajectories for a model
simulating the cell cycle \citep{faure2006dynamical} or cellular
decisions as a function of the activation signal
\citep{calzone2010mathematical}. A second, more frequent approach
consists in looking at the model's steady states and associating them
with physiological conditions
\citep{weinstein2017network, cohen2015mathematical}. A third strategy
focuses on the asymptotic state reached during the stochastic simulation
of the model(s), a state representing a mixture of the different steady
states according to the probability that the model has of reaching them.
It is also possible, in some model checking frameworks, to study the
ability of models to reach certains states, interpreted as cellular
types, from given initial conditions \citep{abou2015model}.

In many models, to facilitate the analysis, \textbf{nodes representing
phenotypes have been added as ``read-out'' of the activity of certain
entities}. Thus, if a model includes a node named \emph{Proliferation},
it will then be simpler to draw interpretations from the simulations
performed with the model that will be linked to experimental
observations of tumor growth or cell proliferation
\citep{grieco2013integrative, steinway2015combinatorial}. To validate
these models, the activity of phenotypes when forcing some node activity
to \(0\) or \(1\) is compared with the results of gene mutations
reported in experiments carried out on mice or cell lines
\citep{faure2006dynamical, cohen2015mathematical}. Another similar
method for validating the relevance of a logical model is based on the
analysis of the effects of different therapeutic molecules. The
mechanistic nature of logical modeling makes it possible to simulate the
effect of these molecules, at least for targeted treatments with known
mechanisms of action. It is then possible to simulate the effect of an
inhibitory molecule by forcing the activity of its target to 0 and to
compare with data
\citep{zanudo2017network, iorio2016landscape, knijnenburg2016logic}.

Beyond validation, some studies have predicted \textbf{new therapeutic
targets} based on logical models, for instance by pointing out
weaknesses in the topology of a regulatory system
\citep{sahin2009modeling}. Taking advantage of the versatility of the
formalism to study combinations of therapeutic molecules, logical
modeling has also proved fruitful in predicting the best therapeutic
combinations and their synergies, in the context of gastric cancers for
example \citep{flobak2015discovery}. Experimental confirmation of the
predictions resulting from the modeling is then the ultimate stage in
the validation of a logical model, completing the fruitful round trip
between models and data.

\BeginKnitrBlock{conclubox}
\subsubsection*{Summary}\label{summary-3}
\addcontentsline{toc}{subsubsection}{Summary}

Logical models represent a qualitative formalism where biological
entities are represented by discrete entities linked together by logical
rules. The evolution of the biological system thus modeled can be
described as a Markov process, as in the MaBoSS software that
stochastically explores the space of possibilities of the model thus
defined through a Gillespie algorithm. This approach allows both to
avoid the exponential computational cost of exact resolutions and to
translate the stochastic diversity of heterogeneous cell populations.
Finally, although discrete and theoretical in nature, logical models
allow the use of quantitative experimental data, in order to support or
validate their biological relevance.
\EndKnitrBlock{conclubox}

\chapter{Personalization of logical models: method and prognostic
validation}\label{personalization-chap}

\epigraph{"All happy families are alike; each unhappy family is unhappy in its own way."}{Leo Tolstoy (Anna Karenina, 1877)}

\initial{N}ow that logical modeling has been introduced, it is possible
to come back to the question that structures this part and to refine it.
\textbf{Is it possible to use routine omics data to obtain logical
models that provide qualitative clinical interpretation?} We thus
propose a sequential approach, separating the model construction process
from the integration of biological data. A generic logical model is
first built, based on the literature knowledge, and the data are then
used to specify the model. Indeed, the model as defined from the
literature is often generic in the sense that it summarizes the state of
knowledge on a probably heterogeneous pathology or population. Assuming
that this general regulatory scheme provides a relevant framework for
the system, it may then be relevant to use more precise omics data to
impose biologically sourced constraints on the model: inactivation of a
gene in a patient, activation of a protein or a signalling pathway by
overexpression or phosphorylation, etc. This approach, called
\textbf{PROFILE} (PeRsonalization OF logIcaL ModEls), allows the
integration of both discrete (mutations) and continuous data (RNA
expression levels, proteins) based on the MaBoSS software, and leads to
specific models of a cell line or a patient.

\BeginKnitrBlock{summarybox}
\subsubsection*{Scientific content}\label{scientific-content-2}
\addcontentsline{toc}{subsubsection}{Scientific content}

This chapter presents the method developed during the thesis to
personalize logical models, \emph{i.e.}, generate patient-specific
models from a single generic one. The principles of the method and some
analyses on patient data have been comprehensively described in
\citet{beal2019personalization} and briefly summarized in
\citet{beal2020personalized}. Analyses on cell lines are unpublished.
\EndKnitrBlock{summarybox}

\section{From one generic model to data-specific models with PROFILE
method}\label{from-one-generic-model-to-data-specific-models-with-profile-method}

The PROFILE method is summarized in Figure \ref{fig:PROFILE-abstract}
and the different steps are successively described in the following
subsections.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{fig/PROFILE-abstract} 

}

\caption[Graphical abstract of PROFILE method to personalize logical models with omics data]{\textbf{Graphical abstract of PROFILE
method to personalize logical models with omics data.} On the one hand
(upper left), a generic logical model, in a MaBoSS format is derived
from literature knowledge to serve as the starting-point. On the other
hand (upper right), omics data are gathered (e.g., genome and
transcriptome) as data frames, and processed through functional
inference methods (for already discrete genome data) or
binarization/normalization (for continuous expression data). The
resulting patient profiles are used to perform model personalization,
i.e., adapt the generic model with patient data. The merging of the
generic model with the patient profiles creates a personalized MaBoSS
model per patient. Then, biological or clinical relevance of these
patient-specific models can be assessed.}\label{fig:PROFILE-abstract}
\end{figure}















\subsection{Gathering knowledge and
data}\label{gathering-knowledge-and-data}

The first steps are therefore to build a logical model adapted to the
biological question (Figure \ref{fig:PROFILE-abstract}, upper left) and
to collect omics data that will be used to personalize the model (Figure
\ref{fig:PROFILE-abstract}, upper right). The construction of the model
can be based on literature or data (see previous chapter). In the latter
case, the data used to build the model will preferably be distinct from
those used to personalize the model.

\subsubsection{A generic logical model of cancer
pathways}\label{a-generic-logical-model-of-cancer-pathways}

In this chapter, which is essentially methodological in nature, we will
use a \textbf{published logical model of cancer pathways} to illustrate
our PROFILE methodology. It is based on a regulatory network summarizing
several key players and pathways involved in cancer mechanisms: RTKs,
PI3K/AKT, WNT/\(\beta\)-catenin, TGF-\(\beta\)/Smads, Rb, HIF-1, p53 and
ATM/ATR \citep{fumia2013boolean}. The later analyses will be mainly
focused on two read-out nodes, \emph{Proliferation} and
\emph{Apoptosis}. Based on the model's logical rules
\emph{Proliferation} node is activated by any of the cyclins (CyclinA,
CyclinB, CyclinD, and CyclinE) and is, thus, an indicator of cyclin
activity as an abstraction and simplification of the cell cycle
behavior. \emph{Apoptosis} node is regulated by Caspase8 and Caspase9.
This generic model contains 98 nodes and 254 edges. Further details and
visual representation are provided in section \ref{appendix-fumia} and
Figure \ref{fig:Fumia}. Model files are available in MaBoSS format in a
dedicated
\href{https://github.com/sysbio-curie/PROFILE/tree/master/Models/Fumia2013}{GitHub
repository}.

\subsubsection{Cancer data to feed the
models}\label{cancer-data-to-feed-the-models}

In order to showcase the method, \textbf{breast-cancer patient data} are
gathered from METABRIC studies
\citep{curtis2012genomic, pereira2016somatic}. 1904 patients have data
for both mutations, copy number alterations, RNA expression and clinical
status (e.g.~survival). This number rises to 2504 patients if we only
look at the mutations. Additional analyses were also performed based on
the smaller and clinically less complete TCGA breast cancer data
\citep{cancer2012comprehensive}. These are detailed in
\citet{beal2019personalization} but not included in this thesis. A more
comprehensive description of these two databases can be found in section
\ref{appendix-datasets-patients}.

In addition to these examples proposed in the original article, an
application to \textbf{cell line data} is proposed in section
\ref{PROFILE-CL} to link to the next chapters. A cohort of 663 cell
lines from different types of cancer will be used. The data are from
Cell Models Passports \citep{van2019cell} and are described in more
detail in the appendix \ref{appendix-cl}. In all cases, samples and cell
lines will sometimes be referred to as patients for the sake of
simplicity.

\subsection{Adapting patient profiles to a logical
model}\label{adapting-patient-profiles-to-a-logical-model}

Before describing precisely the methodologies for using the data to
generate patient-specific models, it is important to understand that
these data will need to be transformed. This is the transformation of
raw omics data into processed profiles that can be used directly in
logical modeling.

\subsubsection{Functional inference of discrete
data}\label{functional-inference-of-discrete-data}

Since the logical formalism is itself discrete, the integration of
discrete data is more straightforward, at least at the first glance. The
most natural idea, used in many previous works, is to \textbf{interpret
the functional effect of these alterations} and to encode it directly in
the model. For instance, a deleterious mutation is integrated into the
model by setting the corresponding node to \(0\) and ignoring the
logical rule associated to it. For activating mutation, the node is set
to \(1\). The main obstacle is therefore to estimate the functional
impact of the alterations in order to translate them as well as possible
in the model.

For mutations, based on the variant classification provided by the data,
inactivating mutations (nonsense, frame-shift insertions or deletions
and mutation in splice or translation start sites) are assumed to
correspond to loss of function mutations and therefore the corresponding
nodes of the model are forced to \(0\). Then, missense mutations are
matched with OncoKB database \citep{chakravarty2017oncokb}: for each
mutation present in the database, an effect is assessed (gain or loss of
function assigned to \(1\) and \(0\), respectively) with a corresponding
confidence based on expert and literature knowledge. Then, mutations
targeting oncogenes (resp. tumor-suppressor genes), as defined in the
2020+ driver gene prediction method \citep{tokheim2016evaluating}, are
assumed to be gain of function mutations (resp. loss of function) and
therefore assigned to \(1\) (resp. \(0\)). To rule potential passenger
mutations out, each automatic assignment of a oncogene/tumor-suppressor
gene muations requires that the effect of the mutation has been
identified as significant by predictive software based on protein
structure such as SIFT \citep{kumar2009predicting} or PolyPhen
\citep{adzhubei2010method}.

For integration of copy number alterations, we use the discrete
estimation of gain and loss of copies from GISTIC algorithm processing
\citep{mermel2011gistic}. The loss of both alleles of a gene (labelled
-2) can thus be interpreted as a 0. Conversely, a significant gain of
copies (labelled +2) denotes a gene that tends to be more highly
expressed although the interpretation is more uncertain.

\subsubsection{Normalization of continuous
data}\label{normalization-of-continuous-data}

The integration of continuous data, such as RNA expression levels, in
logical modeling is more difficult. The stochastic framework of MaBoSS
provides however some possibilities. The main continuous mechanistic
parameters of MaBoSS are the initial conditions of each node (its
initial probability of being activated among the set of simulated
stochastic trajectories) and the transition rates associated with the
nodes (its probability to have its transition performed in an
asynchronous update). In order to facilitate the use of continuous data
through one of these two possibilities, we propose to transform them so
that the \textbf{values are continuous between 0 and 1}, what we will
refer to hereafter as normalized data. \textbf{It is assumed that these
continuous data can be good proxies of biological activity}, 0
corresponding to a very low level of activity of the biological entity
and 1 to a very high level. This assumption will have to be explained
and justified each time: high level of expression of an RNA or
significant phosphorylation of a protein interpreted as continuous
markers of an important biological activity for example.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{05-Personalized_files/figure-latex/ERG-1} 

}

\caption[Bimodal distribution of ERG gene in TCGA prostate cancer cohort]{\textbf{Bimodal distribution of ERG gene in TCGA
prostate cancer cohort.} This bimodality is largely explained by the
fusion status of ERG gene. Patients for whom the gene has fused with
TMPRSS2 have a much higher level of RNA expression for ERG.}\label{fig:ERG}
\end{figure}






One of the assumptions of our analysis is that the interpretation of
continuous data can only be relative and not absolute. It is indeed
difficult to define an absolute threshold of RNA level at which a gene
will be considered as activated. This may depend on contexts,
technologies or even the way in which the data have been processed. On
the other hand, it is possible to estimate that a gene is over-expressed
for a patient compared to a cohort of interest. In contrast, the effect
of a mutation can be estimated more independently. Thus, the
\textbf{continuous data will be normalized for the whole cohort
studied}, for each gene individually. In order to retain biological
information as much as possible, distribution patterns are identified
and normalized in different ways (Figure \ref{fig:logical-processing}).
We will illustrate the process by taking the example of the expression
data expressed with continuous RNA levels. Beforehand, genes with no
variation in expression level or too many missing values are discarded
from the analysis. Then, we seek to identify first the genes that have a
\textbf{bimodal} distribution. Indeed, these naturally fit into a binary
formalism and this bimodality often has an underlying biological
explanation. As an example, in the TCGA prostate cancer cohort (used in
section \ref{prostate-model}), a gene called ERG has a bimodal
distribution when looking at RNA levels in all patients. This
distribution is almost entirely explained by an underlying genetic
alteration that is the fusion of the ERG gene with the TMPRSS2 gene
promoter (Figure \ref{fig:ERG}), which is very common in this cancer
\citep{tomlins2005recurrent}. In the data we identify bimodal patterns
based on three distinct criteria: \textbf{Hartigan's dip test of
unimodality, Bimodality Index (BI) and kurtosis}. The dip test measures
multi-modality in a sample using the maximum difference between
empirical distribution and the best unimodal distribution, i.e., the one
that minimizes this maximum difference \citep{hartigan1985dip}. Values
below \(0.05\) indicate a significant multi-modality. In PROFILE, this
dip statistic is computed using the R package \emph{diptest}. The
Bimodality Index (BI) evaluates the ability to fit two distinct Gaussian
components with equal variance \citep{wang2009bimodality}. Once the best
2-Gaussian fit is determined, along with the respective means \(\mu_0\)
and \(\mu_1\) and common variance \(\sigma\), the standardized distance
\(\delta\) between the two populations is given by

\[\delta = \dfrac{|\mu_0-\mu_1|}{\sigma}\]

with \(\mu_i\) the mean of Gaussian component \(i\), and the BI is
defined by

\[BI=[p(1-p)]^{1/2}\delta\]

where \(p\) is the proportion of observations in the first component. In
PROFILE, BI is computed using the R package \emph{mclust}. Finally, the
kurtosis method corresponds to a descriptor of the shape of the
distribution, of its tailedness, or non-Gaussianity. A negative kurtosis
distribution, especially, defines platykurtic (flattened) distributions,
and potentially bimodal distributions. It has been proposed as a tool to
identify small outliers subgroups or major subdivisions
\citep{teschendorff2006pack}. In our case, we focus on negative kurtosis
distributions to rule out non-relevant bimodal distributions composed of
a major mode and a very small outliers' group or a single outlier.
Although Dip test, BI and negative kurtosis criteria emerge as similar
tools in the sense that they select genes whose values can be clustered
in two distinct groups of comparable size, we choose to combine them in
order to correct their respective limits and increase the robustness of
our method. For that, we consider that \textbf{all three conditions (Dip
test, Bimodality Index and kurtosis) must be fulfilled in order for a
gene to be considered as bimodal}. The thresholds of each test are
inspired by those advocated in the papers presenting the tools
individually. Dip test is a statistical test to which the classical
\(0.05\) threshold has been chosen. In the article describing BI,
authors explored a cut-off range between 1.1 and 1.5 and we chose
\(1.5\) for the present work. Regarding kurtosis, the usual cut-off is
\(0\), but since this criterion does not directly target bimodality,
this criterion has been relaxed to \(K < 1\). Several examples of the
relative differences and complementarities between these criteria can be
seen in Figure \ref{fig:bimodality}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{fig/bimodality} 

}

\caption[Bimodality criteria and their combinations]{\textbf{Bimodality criteria and their
combinations.} Examples of gene expression distributions for the
different combinations of bimodality criteria: Dip test, Bimodality
Index (BI) and kurtosis (K). Plots are organized in a Venn diagram.}\label{fig:bimodality}
\end{figure}






Non-bimodal genes are further classified as unimodal or zero-inflated
distributions, looking at the position of the distribution density peak
(Figure \ref{fig:logical-processing}A). Then, based on this three
category classification of genes, a \textbf{pattern-preserving
normalization} can be performed, as summarized in Figure
\ref{fig:logical-processing}B. For a bimodal gene \(i\), a 2-component
Gaussian mixture model is fitted using \emph{mclust} R package resulting
in a \emph{lower} component \(C_{i,0}\) (with mean \(\mu\)) and an
\emph{upper} component \(C_{i,1}\). Denoting \(X_{i,j}\) the expression
value for gene \(i\) and sample \(j\), \(X_{i,j}\) has a probability to
belong to \(C_{i,0}\) or \(C_{i,1}\) such as
\(P[X_{i,j} \in C_{i,0}]+P[X_{i,j} \in C_{i,1}]=1\). These probabilities
result from posterior inference using Bayes' rule. For bimodal genes,
the normalization processing is therefore defined as:

\[X_{i,j}^{norm}=P[X_{i,j} \in C_{i,1}]\]

For unimodal distributions, we transform data through a sigmoid function
in order to maintain the most common pattern which is unimodal and
nearly-symmetric:

\[X_{i, j}^{norm}=\dfrac{1}{1+e^{-\lambda(X_{i, j}-median(X_{i}))}}\]

Since the slope of the function depends on \(\lambda\), we adapt it to
the dispersion of initial data in order to maintain a significant
dispersion in \([0, 1]\) interval: more dispersed unimodal distributions
are mapped with a gentle slope, peaked distributions with a steep one.
We map the median absolute deviation
\(MAD(X_{i})=median(|X_{i}-median(X_i)|)\) on both sides of the median
respectively to \(0.25\) and \(0.75\) to ensure a minimal dispersion of
the mapping. Thus, the proposed mapping results in:

\[\lambda=\dfrac{log(3)}{MAD(X_i)}\]

Last, zero-inflated distributions are transformed by linear
normalization of the initial distribution:

\[X_{i, j}^{norm}=\dfrac{X_{i, j}-min(X_{i})}{max(X_{i}-min(X_{i}))}\]

The transformation is applied to data between 1st and 99th quantiles to
be more robust to outliers. Values outside this range are respectively
assigned to \(0\) and \(1\). All the categoriation of distributions and
the subsequent normalizations are summarized in Figure
\ref{fig:logical-processing}. With the help of the categories described
here, it is also possible to binarize the continuous data quite simply.
This binarization is required for some methods of network inference or
logical modeling but will not be used in the examples presented below.
Readers may refer to \citet{beal2019personalization} for more details.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{fig/logical-processing} 

}

\caption[Normalization of continuous data for logical modeling]{\textbf{Normalization of continuous
data for logical modeling.} (A) Combinations of tests and criteria to
classify distributions of continuous data (such as gene expression for
one gene and all patients) as bimodal, unimodal or zero-inflated. (B)
Normalization methods for each kind of distribution.}\label{fig:logical-processing}
\end{figure}







\subsection{Personalizing logical models with patient
data}\label{personalizing-logical-models-with-patient-data}

It is now possible to redefine more precisely the ways of integrating
data into a logical model defined with MaBoSS, as sketched at the
beginning of the previous section. \textbf{Personalization is defined
here as the specification of a logical model with data from a given
patient}: each patient has a personalized model tailored to his/her
data, so that all personalized models are different specifications of
the same logical model, using data from different patients (Figure
\ref{fig:PROFILE-abstract}). Based on MaBoSS formalism and the processed
patient data, there are several possibilities to personalize a generic
logical model with patient data. One possibility to have
patient-specific models is to force the value of the variables
corresponding to the altered genes in a given patient, i.e.,
constraining some model nodes to an inactive (\(0\)) or active (\(1\))
state (Figure \ref{fig:logical-personalization}A). In order to constrain
a node to \(0\) (resp. \(1\)), the initial value of the node is set to
\(0\) (resp. \(1\)) and \(k_{0\rightarrow 1}\) (resp.
\(k_{1\rightarrow 0}\)) to \(0\) to force the node to maintain its
initially defined state. For instance, the effect of a TP53 inactivating
mutation can be modeled by setting the node \emph{p53} in the model and
its initial condition to \(0\) and ignoring the logical rule of p53
variable. Because of the type of data used, this personalization method
is referred to as \textbf{discrete personalization}. It has also been
called \emph{strict node variants} in \citet{beal2019personalization}
because this data integration overwrites the logical rules.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/logical-personalization} 

}

\caption[Methods for personalization of logical models]{\textbf{Methods for
personalization of logical models.} (A) Personalization with discrete
data, such as mutations, with some nodes forced to \(0\) based on loss
of function alteration (left) or \(1\) based on gain of
function/constitutive activation (right). (B) Personalization with
continuous data used to define the initial conditions of nodes and to
influence the transitions rates and the subsequent probabilities of
transition in asynchronous update; the graph on the left represents the
normalized values of genes A, B and C for patients 1, 2 and N; the right
side represents the personalization of logical model using values from
patient N (red profile), first defining the initial probabilities of
node activation (middle) and then influencing the probabilities of
transitions from one state to another (right).}\label{fig:logical-personalization}
\end{figure}















Another possible strategy is to modify the initial conditions of the
variables of the altered genes according to the results of the
normalization (i.e., the probability of initial activation for one node
among the thousands of stochastic trajectories). These initial
conditions can capture different environmental and genetic conditions.
Nevertheless, in the course of the simulation, these variables will be
prone to be updated depending on their logical rules. Finally, as MaBoSS
uses Gillespie algorithm to explore the STG, data can be mapped to the
transition rates of this algorithm. In the simplest case, all transition
rates of the model are set to \(1\), meaning that all possible
transitions are equally probable. Alternatively, it is possible to
separate the speed of processes by setting the transition rates to
different values to account for what is known about the reactions: more
probable reactions will have a larger transition rate than less probable
reactions \citep{stoll2012continuous}. For this, different orders of
magnitude for these values can be used. They are set according to the
activation status of the node (derived from normalized values) and an
amplification factor \(F\), designed to generate a higher relative
difference in the transition rates, and are therefore defined for each
node \(i\) and sample \(j\):

\[k^{0\rightarrow1}_{i,j}=F^{2(X^{norm}_{i,j}-0.5)}\]
\[k^{1\rightarrow0}_{i,j}=\dfrac{1}{k^{0\rightarrow1}_{i,j}}\] Thus, if
a gene has a value of \(1\) based on its RNA profile,
\(k_{0\rightarrow1}\) (resp. \(k_{1\rightarrow0}\)) will be \(10^2\)
(resp. \(10^{-2}\)) with an amplification factor of \(100\). This
amplification factor is therefore a hyper-parameter of the method. Very
low values of \(F\) will have no impact while higher values will make
some transitions almost impossible and the method will then approach the
discrete personalization described above. Some quantitative
illustrations of the influence of \(F\) are provided in
\citet{beal2019personalization}. The integration of continuous data
through the initial conditions of the nodes and the transition rates are
combined to form a second personalization method called
\textbf{continuous personalization} and described in Figure
\ref{fig:logical-personalization}B. This method has also been called
\emph{soft node variants} to emphasize its difference with
discrete/strict personalization: it may influence the trajectories in
the solution state space leading to a change in probabilities of the
resulting stable state but it does not overwrite the logical rules. To
illustrate a little more explicitly the impact of continuous
personalization, if a given node has a normalized value of \(0.8\) after
data processing (based on proteins levels for instance), it will be
initialized as \(1\) in 80\% of the stochastic trajectories, its
transition rate \(k_{0\rightarrow1}\) will be increased (favoring its
activation) and its transition rate \(k_{1\rightarrow0}\) will be
decreased (hampering its inactivation). These changes increase the
probability that this node will remain in an activated state close to
the one inferred from the patient's data, while maintaining the validity
of its logical rule. Thus, continuous personalization appears as a
smoother way to shape logical models' simulations based on patient data.

In summary, different types of data can be used, with different
integration methods. Note that it is quite natural to use genetic
alterations (mutations, CNA) to specify definitive changes in models
(such as those of discrete personalization) since this corresponds to
biological reality: a mutation cannot be undone or reversed. Conversely,
continuous alterations in expression or phosphorylation are subject to
modification and regulation, thus justifying their interpretation in a
less strong and definitive way (such as continuous personalization).
Finally, it follows from these definitions that there are different
strategies for personalizing a logical model since discrete and
continuous personalizations can each use different types of data; and
moreover, these two strategies can be combined. \textbf{Except otherwise
stated, mutations (resp. RNA or protein) will always be integrated using
discrete (resp. continuous) personalization and the joint integration of
both types of data will therefore combine both methods.} The relative
merits of the different personalization strategies will be discussed
below.

\section{An integration tool for high-dimensional
data?}\label{an-integration-tool-for-high-dimensional-data}

Once the method has been defined, it is imperative to study its validity
and possible limitations. This comes down to answering the question:
\textbf{do personalized models capture a biological reality}, and in our
case do they discriminate between different types of cancer?

\subsection{Biological relevance in cell lines}\label{PROFILE-CL}

These questions can be addressed using cell line data. Using the logical
model of cancer pathways from \citet{fumia2013boolean}, it is possible
to study the 663 cell lines from different types of tumors by
integrating their processed omics profiles to the generic logical model
to obtain as many personalized models. If we focus on the read-out of
\emph{Proliferation}, one of the easiest to interpret, there are several
ways to study its relevance. For each cell line and each personalization
strategy (and corresponding data type) we can define a personalized
model and derive the asymptotic value the \emph{Proliferation} node,
called \emph{Proliferation} score. This score is therefore \emph{a
priori} different for all cell lines that present a different molecular
profile. For the whole population of cell lines, this score can be
confronted with other markers of proliferation such as the levels of
Ki67 \citep{miller2018ki67}, here replaced as an example by the RNA
levels of the corresponding MKI67 gene. Thus, two independent indicators
of the proliferative nature of cell lines are compared. The first is the
\emph{Proliferation} score, which is the final, supposedly asymptotic,
value of the \emph{Proliferation} node, for models that have been
constrained with the omics profiles of the corresponding cell lines. On
the other hand, an independent experimental indicator of proliferation
is used, both the level of the MKI67 biomarker (not used in the
personalization process).

It can then be observed that the simulated \emph{Proliferation}
indicator, derived from the personalized models, correlates positively
with the biomarker, but only when RNA has been used in the
personalization (Figure \ref{fig:PROFILE-CL}A). The \textbf{sign of the
correlation is qualitatively correct, but the heterogeneity appears to
be very large and most of the variability is not captured by the
models}. This heterogeneity is also visible by focusing on some types of
cancer (Figure \ref{fig:PROFILE-CL}B). Thus this kind of comparison only
validates the models' ability to retrieve a RNA biomarker (not used in
personalization) when they themselves integrate other RNA data. It is
also consistent that scores from models personalized with mutations only
have less uniform distributions due to the discrete nature of the data
and the many identical profiles: many cell lines are not distinguishable
by mutations only.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{05-Personalized_files/figure-latex/PROFILE-CL-1} 

}

\caption[Validation of personalized \emph{Proliferation} scores in cell lines]{\textbf{Validation of personalized
\emph{Proliferation} scores in cell lines.} (A) Comparison with MKI67
proliferation biomarker for all cancer cell lines. (B) Same with breast
(BRCA) and lung (LUAD) cancer only. (C) Comparison with doubling times
in a subset of 60 cell lines.}\label{fig:PROFILE-CL}
\end{figure}







It is possible to go one step further by comparing these personalized
\emph{Proliferation} scores with the doubling time of the cell lines,
i.e., the time it takes for the cell line population to double. A cell
line described as proliferative (high \emph{Proliferation} score) should
thus have a low doubling time. This can be observed qualitatively by
using a subgroup of cell lines for which this information is available
(Figure \ref{fig:PROFILE-CL}C). These correlations are not significant
and once again summarize a large heterogeneity. Predicting doubling
times is, however, a rather difficult task with poor accuracies, even
with the help of more flexible machine learning low
\citep{kurilov2020assessment}.

\subsection{Validation with patient data}\label{validation-METABRIC}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{05-Personalized_files/figure-latex/PROFILE-METABRIC-Grade-1} 

}

\caption[Comparaison of personalized scores with tumor grades for breast cancer patients in METABRIC cohort]{\textbf{Comparaison of personalized
scores with tumor grades for breast cancer patients in METABRIC cohort.}
Comparisons are provided for different personalization strategies (with
mutations and/or RNA) and two different model nodes
(\emph{Proliferation} and \emph{Apoptosis}).}\label{fig:PROFILE-METABRIC-Grade}
\end{figure}







Patient data can as well be used to reproduce analyses of the same type
as those previously performed with the MKI67 biomarker, as was done in
\citet{beal2019personalization}, but we focus here on the more clinical
applications of the personalized mechanistic models. By analogy with the
validations proposed for other mechanistic models
\citep{fey2015signaling}, it is also possible to evaluate the
\textbf{prognostic value of personalized logical models on patient
data}. For example, when studying breast cancer patients in the METABRIC
cohort, \emph{Proliferation} and \emph{Apoptosis} scores differ
according to tumor grade. The more advanced tumors (grade 3) are
associated with higher \emph{Proliferation} scores and lower
\emph{Apoptosis} scores (Figure \ref{fig:PROFILE-METABRIC-Grade}). This
is in line with the natural interpretation that could be given since
proliferation is by definition a sign of cancer progression while
apoptosis, a programmed death of defective cells, is on the contrary a
protective mechanism. While these trends are monotonous and clearly
significant for the third strategy using both mutations and RNA
(\(p<10^{-12}\) with Jonckheere--Terpstra test for ordered differences
among classes, for both nodes), this is not the case when the two types
of data are used separately: mutations (resp. RNA) are not sufficient to
personalize \emph{Proliferation} (resp. \emph{Apoptosis}) scores in a
meaningful way. The personalisation method therefore seems to be able to
combine discrete and continuous data in such a way that some of the
biological information is preserved.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{05-Personalized_files/figure-latex/PROFILE-METABRIC-Cox-1} 

}

\caption[Hazard ratios for \emph{Proliferation} and \emph{Apoptosis} in a survival Cox model in METABRIC cohort]{\textbf{Hazard ratios for
\emph{Proliferation} and \emph{Apoptosis} in a survival Cox model in
METABRIC cohort.} Higher \emph{Proliferation} (resp. \emph{Apoptosis})
scores correspond to higher (resp. lower) probabilities of death.}\label{fig:PROFILE-METABRIC-Cox}
\end{figure}






This comparison to clinical data can be extended to \textbf{patient
survival data} in the same cohort. If we focus on the strategy
integrating both mutations and RNA, we observe that in a Cox model of
survival, \emph{Proliferation} is significantly associated with a higher
risk of event while \emph{Apoptosis} is associated with a lower risk,
which is again consistent (Figure \ref{fig:PROFILE-METABRIC-Cox}). In a
more schematic and visual way, it is possible to transform these
continuous \emph{Proliferation} and \emph{Apoptosis} scores into binary
indicators (using medians) and observe their impact on survival, as it
has been done in previously mentioned studies
\citep{fey2015signaling, salvucci2019system}. The shortcomings of such
approaches will be discussed from a statistical point of view in Part
III. We then observe the same behaviour for the two personalized scores
(Figure \ref{fig:PROFILE-METABRIC-Survival}A and B). Interestingly, if
we combine the indicators to create groups that are expected to be of
very bad prognosis (high \emph{Proliferation}, low \emph{Apoptosis}) or
of very good prognosis (low \emph{Proliferation}, high
\emph{Apoptosis}), we further discriminate patients and confirm the
qualitatively meaningful interpretation of the personalized scores. It
should be noted that the clinical validations presented here remain
voluntarily simple and quite close to those proposed in similar
articles. Discussions and statistical developments will be proposed in
Part III.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{05-Personalized_files/figure-latex/PROFILE-METABRIC-Survival-1} 

}

\caption[Prognostic value of \emph{Proliferation} scores for breast cancer patients in METABRIC cohort]{\textbf{Prognostic value of
\emph{Proliferation} scores for breast cancer patients in METABRIC
cohort.} (A) Survival curve for overall survival stratified with
\emph{Proliferation} scores from personalized models integrating
mutations and RNA; scores have been binarized based on median and
survival censored at 120 months. (B) Same with \emph{Apoptosis} scores.
(C) Survival curve stratified with combinations of \emph{Proliferation}
and \emph{Apoptosis} scores, based on the same thresholds, and the
corresponding number of patients at risk (D).}\label{fig:PROFILE-METABRIC-Survival}
\end{figure}











\subsection{Perspectives}\label{perspectives}

In summary, this kind of application of personalized models allows the
\textbf{integration of quite heterogeneous and moderately dimensional
biological data in a constrained framework} that orders the
relationships between variables and guides interpretations. Comparison
with external biological or clinical data then makes it possible to
verify the absence of major contradictions in the definition of the
model. However, the interest of these mechanistic approaches in this
type of task appears as quite moderate compared to statistical models.
The qualitative aspect is not necessarily compensated here by the
integration of knowledge into the structure of the model, especially in
examples that use an extremely broad logical model, which has not been
specifically designed for the problems to which it is applied. It is
then necessary to study the application of these personalized models to
more suitable problems, in which the explicitly mechanistic nature of
the models can be exploited.

\BeginKnitrBlock{conclubox}
\subsubsection*{Summary}\label{summary-4}
\addcontentsline{toc}{subsubsection}{Summary}

Mechanistic models built from the literature are necessarily generic and
not patient-specific, which hampers their clinical application. The
PROFILE method proposes to personalize a generic knowledge-based
structure with patient omics data. The restriction to only initial and
static patient omics profiles calls for the use of a parsimonious
modeling framework such as logical formalism. The method is then based
on the application of patient-specific constraints to the generic model
without fitting any parameter but interpreting \emph{a priori} the
biological effect of its alterations: nodes maintained forcibly in their
states to translate mutations, or transitions between certain favoured
states to translate RNA/protein over- or under-expression. Model
simulations are thus induced to follow stochastic trajectories closer to
the inferred patient states. The biological relevance of the
personalized models thus obtained is validated a first time by verifying
their capacity to recover certain characteristics of the systems studied
such as differences in the proliferation of cell lines or differences in
the prognosis of certain patients.
\EndKnitrBlock{conclubox}

\chapter{Personalized logical models to study an interpret drug
response}\label{logical-drug-chap}

\epigraph{"Il serait excellent que tout médecin ait la possibilité d'expérimenter un grand nombre de médicaments sur lui-même. Sa compréhension de leurs effets en serait tout autre."}{Mikhail Bulgakov (Morphine, 1927)}

\initial{H}istorically, all mechanistic models of molecular networks,
and logical models in particular, have been widely used to study
response to treatments
\citep{flobak2015discovery, jastrzebski2018integrative}. Indeed,
biological entities, many of which are prospective therapeutic targets,
are explicitly represented in the model, making it possible to simulate
their inhibition. This is what will be presented in this chapter using
the personalized logical models described above. Can they be used to
study the response of biological systems to perturbations, in this case
the response of cell lines to gene or protein inhibitions? Compared to
the numerous statistical models designed to predict the sensitivity of
cell lines to treatements, what information do these personalized
mechanistic models provide?

\BeginKnitrBlock{summarybox}
\subsubsection*{Scientific content}\label{scientific-content-3}
\addcontentsline{toc}{subsubsection}{Scientific content}

This chapter extends the method presented in the previous chapter to
investigate drug response with personalized logical models. The first
application to cell lines of all cancer types was presented orally at
(ISMB2020){[}\url{https://www.youtube.com/watch?v=6EMBycoR0Ow}{]} in
Basel but is not published.

The example about BRAF in melanomas and colorectal cancers is under
review and the corresponding pre-print is available as
\citet{beal2020personalized}. In this joint work, only the construction
of the generic logical model and the model-checking procedure were
mostly carried out by collaborators and especially by an intern under my
joint supervision. These two steps will therefore be described more
succinctly.

Finally, the work on prostate cancer presented in a third section will
be submitted soon. It is also a joint work, in which my participation
focused on the application of the PROFILE method.
\EndKnitrBlock{summarybox}

\section{One step further with drugs}\label{one-step-further-with-drugs}

One of the main clinical consequences of the underlying molecular
complexity of cancers is the divergent response to treatment, even for
\emph{a priori} similar tumors. In the light of high-throughput
sequencing data, the mechanisms governing these responses are somewhat
better understood, for patients and especially for model organisms such
as cell lines \citep{heiser2012subtype, garnett2012systematic}. But
beyond a few simple cases, the diversity of response biomarkers once
again calls for \textbf{holistic approaches} to unravel the underlying
mechanisms.

\subsection{Modeling response to cancer
treatments}\label{modeling-response-to-cancer-treatments}

To study these observed differences in drugs response in various
cancers, some approaches based on mathematical modeling were developed
to explore the complexity of differential drug sensitivities\footnote{Sensitivity
  is understood throughout this chapter in the biological sense,
  \emph{i.e.}, the response of a biological system (here cell lines) to
  an external disturbance (\emph{e.g.}, a drug). This definition is
  extended to personalized logical models whose response to the same
  perturbations is studied.}. A number of \textbf{machine learning-based
methods for predicting sensitivities} have been proposed
\citep{costello2014community}, either without particular constraints or
with varying degrees of prior knowledge; but they do not necessarily
provide a mechanistic understanding of the response. Some other
approaches focused on the description of the processes that might
influence the response by integrating knowledge of the signaling
pathways and their mechanisms and translated it into a mathematical
model
\citep{eduati2017drug, jastrzebski2018integrative, frohlich2018efficient}.
The first step of this approach implies the construction of a network
recapitulating knowledge of the interactions between selected biological
entities (driver genes but also key genes of signaling pathways),
extracted from the literature or from public pathway databases, or
directly inferred from data \citep{verny2017learning}. This static
representation of the mechanisms is then translated into a dynamical
mathematical model with the goal to not only understand the observed
differences \citep{jastrzebski2018integrative} but also to predict means
to revert unexpected behaviours.

One way to address issues related to patient response to treatments is
to \textbf{fit these mechanistic models to the available data}, and to
train them on high-throughput cell-line specific perturbation
data\footnote{In this thesis, this term refers to data from biological
  systems (e.g.~cell lines) that have been disrupted according to
  different technologies or molecules: drugs, CRISPR/Cas9 etc. The
  dynamic response of the studied system to these perturbations is thus
  accessed instead of being restricted to a static knowledge of the
  system.}
\citep{eduati2017drug, jastrzebski2018integrative, klinger2013network}.
These mechanistic models are then easier to interpret with regard to the
main drivers of drug response. They also enable the \emph{in silico}
simulations of new designs such as combinations of drugs not present in
the initial training data \citep{frohlich2018efficient}. However, these
mechanistic models contain many parameters that need to be fitted or
extracted from the literature. Some parsimonious mathematical formalisms
have been developed to make up for the absence of either rich
perturbation data to train the models or fully quantified kinetic or
molecular data to derive the parameters directly from literature. One of
these approaches is the logical modeling, which uses discrete variables
governed by logical rules. Its explicit syntax facilitates the
interpretation of mechanisms and drug response
\citep{zanudo2017network, iorio2016landscape} and despite its
simplicity, semi-quantitative analyses have already been performed on
complex systems including drug response studies
\citep{knijnenburg2016logic, eduati2020patient}.

\subsection{An application of personalized logical
models}\label{an-application-of-personalized-logical-models}

But logical formalism has also shown its relevance regarding drug
response in cases where the model is not automatically trained on data
but simply constructed from literature or pathway databases and where
biological experiments focus on a particular cell line
\citep{flobak2015discovery}. The study is then restricted to one cell
line only from which some data and parameters have been experimentally
inferred. Using the PROFILE method, it is possible to generate
personalized logical models associated with different cell lines and
then use them to study the response to treatment. \textbf{Since the
models are not trained with perturbation data but simply
specified/constrained by interpreting the molecular profiles, it is
possible to personalize the logical models with a rather limited amount
of data}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/PROFILE-drug} 

}

\caption[Schematic extension of PROFILE-personalized logical models to drug investigation]{\textbf{Schematic extension of
PROFILE-personalized logical models to drug investigation.} (A)
Schematic representation of a logical model of cancer molecular
networks, in particular the one described in appendix
\ref{appendix-verlingue} and used in the next subsection. (B) Sequential
pipeline for drug response investigation with PROFILE, starting from a
generic logical model, then transformed into several personalized models
with different molecular profiles (correspondong to several cell lines);
these models are finally simulated with a defined drug inhibition. (C) A
possible analysis of the predictions of personalized models obtained
from the generic model described in (A); a PCA is computed based on the
final phenotype scores from personalized model, it allows to interpret
biologically how the models represent cell lines (\emph{e.g.}, more or
less proliferative) and especially what impact of treatment they predict
(\emph{e.g.}, decrease \emph{Proliferation} or increase
\emph{Apoptosis}).}\label{fig:PROFILE-drug}
\end{figure}


















The principles are summarized in Figure \ref{fig:PROFILE-drug}. A
generic model (Figure \ref{fig:PROFILE-drug}A) is first transformed into
as many personalized models as there are cell lines with an omics
profile. These persnalized models are then simulated by adding the
effect of a given treatment (Figure \ref{fig:PROFILE-drug}B). The
treatments that can be studied are generally targeted inhibitors.
Generally speaking, one must be able to \textbf{translate the mechanism
of action of the treatment into the logical model}. The impact of more
systemic treatments such as chemotherapy or radiotherapy is more
difficult to study with these methods, in any case with most of the
logical models published to date, even if in theory, precise modeling of
the pathways associated with these treatments (such as DNA repair) could
contribute to this.

It is then possible to analyze the personalized scores for each cell
line (asymptotic values of the phenotypic read-outs of the model) with
or without the effect of treatment. If the model includes more than two
phenotypes of interest, such as the one in Figure
\ref{fig:PROFILE-drug}A, one can visualize these behaviors in the PCA
space of the personalized scores, as shown schematically in Figure
\ref{fig:PROFILE-drug}C. In this case the directions of the original
phenotype features (\emph{Proliferation}, \emph{Apoptosis},
\emph{Quiescence}) have been added in the PCA-transformed space in order
to facilitate the interpretation of positions and drug-induced
displacements. In this mock example, based on personalized models,
treatment would promote a shift from proliferative to more apoptotic or
quiescent behaviors, in particular in the red and green cell lines,
which are \emph{a priori} more sensitive to the treatment.

\subsection{A pan-cancer attempt}\label{a-pan-cancer-attempt}

This versatile analysis framework was first applied during this thesis
to a \textbf{large pan-drug and pan-cancer analysis}. On the basis of
generic logical models such as those previously presented (see appendix
\ref{appendix-fumia} and \ref{appendix-verlingue}), and in view of the
abundance of available data (across cancer tissues and drugs such as in
GDSC cell line dataset, see appendix \ref{appendix-fumia}), there were
no theoretical obstacle to such an analysis. Although the simulations
were carried out without any problems, the analysis nevertheless proved
extremely difficult to interpret. We will highlight the various problems
encountered, propose an illustration and some perspectives that led to
the work presented in the following section.

Based on the PROFILE methodology and GDSC data, hundreds of personalized
models can be obtained, each corresponding to a cell line. For each of
these personalized models, several dozen of potential drugs have a
mechanism of action that can be mechanistically translated into the
logical model. We thus obtain \textbf{tens of thousands of
``personalized model/drug'' pairs that correspond to experimentally
evaluated drug sensitivies} (cf appendix \ref{appendix-GDSC} for
details). Firstly, the comparison of simulated and experimental data is
not straightforward. As the models are qualitative, it is necessary to
carry out the validation in this spirit. The idea is not to predict
sensitivity quantitatively, rather to verify their relative relevance.
In the first place, do we recover the cell lines that are most sensitive
to a given drug? With several hundred cell lines, it is difficult to
make this reflection graphically as in Figure \ref{fig:PROFILE-drug}C.
More quantitative approaches, such as correlation, would require the
\textbf{definition of a precise sensitivity proxy in personalized
models}. Should we choose the personalised \emph{Proliferation} score
obtained with drug? Or the drug-induced displacement in the mechanistic
model (the drug arrows in Figure \ref{fig:PROFILE-drug}C)? Or is a
combination of phenotypes used, if so which one? As for experimental
metrics, which ones to choose, and what interpretations do they allow?
Whatever the choice, dose-response AUC or IC50 (see details in the
appendix \ref{appendix-GDSC}), a problem arises: can the sensitivities
of a cell line to different drugs be compared? Such a comparison would
allow the most clinically interesting questions of precision medicine to
be asked: for a given molecular profile, can the model predict the best
treatment to administer? However, AUCs are comparable for different
drugs only if the concentration ranges tested are similar; and IC50s are
extrapolated, sometimes well beyond the concentrations tested.
Qualitative comparisons for a given drug therefore seem the most
meaningful, as long as a relevant proxy in personalized models can be
justified.

Aware of these difficulties, if one decides to do a correlation
analysis, for each drug, of the personalized correlation scores with
experimental sensitivities, one realizes that \textbf{some experimental
responses correlate well with the behaviours of personalized models
while others do not.} But it is difficult to decide between two
different interpretations: does this mean that correctly predicted drugs
are well modeled and others are not? Or does it mean that some
correlations appear to be better by chance because so many drugs have
been modeled? A case study can be illustrated more precisely with the
example shown in Figure \ref{fig:PROFILE-PCA}. In order to simplify the
analysis presented schematically in Figure \ref{fig:PROFILE-drug}C, the
663 cell lines were averaged by cancer type (according to
\href{https://gdc.cancer.gov/resources-tcga-users/tcga-code-tables/tcga-study-abbreviations}{TCGA
denominations}) and the drug-induced shifts are all represented from the
origin in the PCA space. There is evidence that the effect of the drug
on personalized models (using only mutations) tends to make them less
proliferative and more apoptotic/quiescent (Figure
\ref{fig:PROFILE-PCA}A). This shift is strongest for those types of
cancer that are actually most sensitive to this inhibitor experimentally
(\emph{i.e.}, low AUC), such as skin cutaneous melanomas (SKCM) in
particular, and colorectal (COAD/READ) or pancreatic (PAAD) cancers to a
lesser extent. The ability of personalized models to explain this
difference can be understood by a known underlying biological reality:
the prevalence of BRAF or RAS mutations in these cancers. The three
aforementioned cancers are thus very frequently mutated for one of the
two genes (Figure \ref{fig:PROFILE-PCA}B). Then, the model translates
the fact that these two genes are located just upstream of MAP2K1. It is
therefore natural that an inhibition just downstream of these important
mutations is particularly effective (Figure \ref{fig:PROFILE-PCA}C). In
a case such as this, the relevance of the model can be explained and
justified \emph{a posteriori}. This analysis is much more difficult in
the vast majority of cases, whether the correlations are apparently good
or not.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{06-Drugs_files/figure-latex/PROFILE-PCA-1} 

}

\caption[PROFILE-generated models and sensitivites to MAP2K1  inhibitors averaged per cancer type]{\textbf{PROFILE-generated models and
sensitivites to MAP2K1 inhibitors averaged per cancer type.} (A) Effects
of MAP2K1 inhibitors on personalized logical models averaged per cancer
types and represented in a normalized PCA space with super-imposed
original phenotypes. (B) Proportion of BRAF- and RAS-mutated cell lines
in some cancer types. (C) Zoom on the MAPK pathway of the logical model
used.}\label{fig:PROFILE-PCA}
\end{figure}









This example highlights a problem of scope. \textbf{The fact that the
method enables to study hundreds of cell lines and dozens of drugs does
not mean that it is relevant in each case}. The description of pathways
in the model is more or less accurate. For example, a node at the model
boundaries probably has many regulators missing. Is it then relevant to
investigate the response of personalized models to its inhibition? It is
therefore necessary to restrict the drugs studied. Similarly, even if
the logical model summarizes many important pathways, it is probably
unsuitable for certain cell lines or certain types of cancer with
different etiologies. However, it is difficult to restrict the scope of
the analysis in an unbiased way without having designed a model \emph{de
novo} for a specific purpose.

For all these reasons, it was decided to leave aside this naive,
broad-spectrum approach in favour of starting from a more specific
biological question and constructing the appropriate logical model.

\section{Case study on BRAF in melanoma and colorectal
cancers}\label{case-study-on-braf-in-melanoma-and-colorectal-cancers}

In order to address the limitations outlined in this exploratory
analysis, we propose here a pipeline based on logical modeling and able
to go from the formulation of a specific biological question to the
validation of a mathematical model on pre-clinical data, in this case a
set of cell lines, and the subsequent interpretation of potential
resistance mechanisms\footnote{For the sake of completeness, all the
  steps will be described below or in appendix; the \emph{``Logical
  model''} and \emph{``Model checking''} steps that I supervised jointly
  without implementing them directly will be described more succinctly.}
(Figure \ref{fig:BRAF-GA}). As before, \textbf{one of the main points of
differentiation with existing mechanistic approaches, is that this
framework does not rely on any training of parameters but only on the
automatic integration and interpretation of molecular features}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/BRAF-GA} 

}

\caption[BRAF modeling flowchart: from a biological question to validated personalized logical models]{\textbf{BRAF modeling flowchart: from a biological
question to validated personalized logical models.} Logical models are
written with MaBoSS, and the checking model procedure is therefore
provided in the same formalism. Cell line data are taken from Cell Model
Passports \citep{van2019cell}.}\label{fig:BRAF-GA}
\end{figure}







\subsection{Biological and clinical
context}\label{biological-and-clinical-context}

The construction of a mathematical model must be based first and
foremost on a precise and specific biological question, at the origin of
the design of the model. Here, we choose to explore the different
responses to treatments in tumors from diverse cancers that bear the
same mutation. A well-studied example of these variations is the BRAF
mutation and especially its V600E substitution. BRAF is mutated in 40 to
70\% of melanoma tumors and in 10\% of colorectal tumors, each time
composed almost entirely of V600E mutations
\citep{cantwell2011brafv600e}. In spite of these similarities,
\textbf{BRAF inhibition treatments have experienced opposite results
with improved survival in patients with melanoma
\citep{chapman2011improved} and significant resistance in colorectal
cancers \citep{kopetz2010plx4032}}, suggesting drastic mechanistic
differences. Some subsequent studies have proposed context-based
molecular explanations, often highlighting surrounding genes or
signalling pathways, such as a feedback activation of EGFR
\citep{prahallad2012unresponsiveness} or other mechanisms
\citep{poulikakos2011raf, sun2014reversible}. These various findings
support the need for an integrative mechanistic model able to formalize
and explain more systematically the differences in drug sensitivities
depending on the molecular context. The purpose of the study we propose
here is not to provide a comprehensive molecular description of the
response but to verify that the existence and functionality of the
suggested feedback loops around the signalling pathway in which BRAF is
involved \citep{prahallad2012unresponsiveness} may be a first hint
towards these differences. For a more thorough study of these cancers,
we refer to other works
\citep{eduati2017drug, baur2020connecting, cho2016attractor}.

\subsection{A logical model centred on
BRAF}\label{a-logical-model-centred-on-braf}

A logical model summarizing the main molecular interactions at work in
colorectal cancers and melanomas is thus built from the literature and
completed with databases. As previously mentioned, the objective is to
understand whether it is possible to model and explain differences in
responses to BRAF inhibition in melanoma and colorectal cancer patients
using the same regulatory network. \textbf{The fact that the two cancers
share the same network but differ from the alterations and expression of
their genes constitutes our prior hypothesis}. The focus of this model
is put on two important signaling pathways involved in the mechanisms of
resistance to BRAF inhibition which are the ERK1/2 MAPK and PI3K/AKT
pathways \citep{ursem2018emerging, rossi2019drug}. The generic network
presented in Figure \ref{fig:BRAF-model} recapitulates the known
interactions between the biological entities of the network that was
first built from the literature, and then verified and completed with
potential missing connections using SIGNOR database
\citep{perfetto2016signor}. More details and references about the model
can be found in appendix \ref{appendix-pantolini}. All in all, the
logical model formalizes the knowledge compiled from different sources
and highlights the role of SOX10, FOXD3, CRAF, PI3K, PTEN and of EGFR in
resistance to anti-BRAF treatments. In order to facilitate the
biological interpretation of the model's behaviors, \textbf{a phenotypic
read-out is defined: it is the \emph{Proliferation} node} which
summarizes the proliferative capacity of the model resulting from the
activation of the different signaling pathways.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/BRAF-model} 

}

\caption[Logical model of signaling pathways around BRAF in colorectal and melanoma cancers]{\textbf{Logical model of signaling pathways
around BRAF in colorectal andmelanoma cancers.} Grey nodes represent
input nodes, which may correspond to the environmental conditions.
Square nodes represent multi-valued variable (\emph{MEK}, \emph{ERK},
\emph{p70} and \emph{Proliferation}). Dark blue nodes accounts for
families (several genes/entities for one node). Light blue node
represents the phenotypic read-out of the model, \emph{i.e.},
\emph{Proliferation}.}\label{fig:BRAF-model}
\end{figure}










Once the structure of the model was defined, and before moving on to its
personalization, its consistency with the literature was checked using a
\textbf{model-checking procedure}. Indeed, due to the complexity of the
system, properly taking into account the interactions between entities
does not automatically guarantee that the model will reproduce certain
dynamic behaviours. It is therefore a question of verifying whether the
model reproduces certain biological assertions found in the scientific
literature. An example of a biological assertion may be the reactivation
of the MAPK (mitogen-activated protein kinase) pathway through EGFR
signal after BRAF inhibition in colorectal cancer
\citep{prahallad2012unresponsiveness}: it is possible to check whether a
simulation of this situation with the model gives the same result or
not. Because there are many such assertions and because it is useful to
verify them as the model is built, automatic model-checking tools have
been defined, based on the MaBoSS syntax and inspired by the Python
\emph{unittest} library. More details are provided in
\citet{beal2020personalized} and in a corresponding
\href{https://github.com/sysbio-curie/MaBoSS_test}{GitHub repository}.
The list of biological assertions used to validate the model is detailed
in the appendix \ref{appendix-pantolini}.

\subsection{Cell lines data}\label{cell-lines-data}

The omics profiles of colorectal and melanoma cell lines are downloaded
from Cell Model Passports portal \citep{van2019cell}. 64 colorectal
cancer (CRC) cell lines and 65 cutaneaous melanoma (CM) cell lines are
listed in the database, with at least mutation or RNA-seq data (59 CM
and 53 CRC with both mutations and RNA-seq data). These omics profiles
are used to generate cell-line-specific logical models as described in
PROFILE method (Figure \ref{fig:PROFILE-abstract}). The prevalence of
mutations and their combination for the two types of cancer can be seen
in Figure \ref{fig:BRAF-GDSC}A and is consistent with the clinical
situation described above with melanomas more frequently BRAF-mutated
and colorectal cancers more frequently RAS-mutated.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{06-Drugs_files/figure-latex/BRAF-GDSC-1} 

}

\caption[Descriptive analysis of cell lines for melanomas and colorectal cancers]{\textbf{Descriptive analysis of cell lines for
melanomas and colorectal cancers.} (A) Number of cell lines for the four
most frequently mutated genes and their combinations (plot from UpSetR
package \citep{conway2017upsetr}). (B) Differential sensitivities to
BRAF inhibition by the drug PLX-4720 (lower panel) or by CRISPR
inhibition (upper panel), depending on BRAF mutational status and cancer
type. Numbers of cell lines in each category are indicated. Note that
high sensitivities correspond to low AUC and high scaled Bayesian
factors.}\label{fig:BRAF-GDSC}
\end{figure}











In order to validate the relevance of personalized models to explain
differential sensitivities to drugs, some experimental screening
datasets are used. \textbf{Drug screening data} are downloaded from the
Genomics of Drug Sensitivity in Cancer (GDSC) dataset
\citep{yang2012genomics} which includes two BRAF inhibitors: PLX-4720
and Dabrafenib. The cell lines are treated with increasing concentration
of drugs and the viability of the cell line relative to untreated
control is measured. The dose-response relative viability curve is
fitted and then used to compute the \textbf{area under the dose-response
curve (AUC)} \citep{vis2016multilevel}. AUC is a value between \(0\) and
\(1\): values close to \(1\) mean that the relative viability has not
been decreased, and lower values correspond to increased experimental
sensitivity to inhibitions (details in appendix \ref{appendix-GDSC}).
The results obtained with the two drugs are very strongly correlated
(Pearson correlation of \(0.91\)) and the analyses presented here will
therefore focus on only one of them, PLX-4720.

In a complementary way, some results of \textbf{CRISPR/Cas9 screening}
are also downloaded from Cell Model Passports. This technology, which is
described in more detail in the appendix \ref{appendix-CRISPR}, allows
targeted inhibitions of certain genes. Two different datasets from
Sanger Institute \citep{behan2019prioritization} and Broad Institute
\citep{meyers2017computational} are available. We use \textbf{scaled
Bayesian factors} to assess the effect of CRISPR targeting of genes.
These scores are computed based on the fold change distribution of
single guide RNAs \citep{hart2016bagel}. The highest values indicate
that the targeted gene is essential to the cell fitness. The agreement
between the two databases is good \citep{dempster2019agreement} but we
choose to focus on the Broad database, which is more balanced in terms
of the relative proportions of melanomas and colorectal cancers.

Figure \ref{fig:BRAF-GDSC}B illustrates both the relative quantities of
cell lines for which drug or CRISPR screening data are available
(depending on their BRAF status) as well as differences in experimental
sensitivity to BRAF inhibition. The greater sensitivity of BRAF-mutated
melanomas compared to BRAF-mutated colorectal cancers is well observed
for PLX-4720. However, the overlap in the distributions requires a
deeper look into the data and a search for more precise explanations of
the differences in sensitivity, including within each type of cancer.
The finding appears to be similar for CRISPR despite a sample size that
is too small; the higher average sensitivity of melanomas even extends
to non-mutated BRAF.

\subsection{Validation of personalized models using CRISPR/Cas9 and drug
screening}\label{validation-of-personalized-models-using-crisprcas9-and-drug-screening}

The validation of personalized logical models using these screening data
is done with the following rationale. First, the models are personalized
using omics data from the cell lines. Then, two separate simulations are
performed for each personalized model: one without the inhibition, the
other by creating and activating a BRAF inhibitor to mimic the drug or
CRISPR inhibition. A ratio of the \emph{Proliferation} phenotype
obtained with inhibition and without inhibition is obtained and can be
written as follows for each personalized logical model:

\[\dfrac{Proliferation(t_{final})~\text{with BRAF inhibited}}{Proliferation(t_{final})~\text{without any inhibition}}\]

Since the expected effect of the drug is to decrease proliferation, this
ratio is expected to be less than or equal to 1, with the lower values
indicating the personalized logical models most sensitive to this
targeted inhibition. This ratio is considered as a proxy for drug
sensitivity. In the experimental cell line data mentioned above, drug
sensitivities are measured with different metrics, each of which is also
standardized: AUC is calculated on relative viability for drugs and
Bayes factor is computed from fold-changes and then scaled. It is
therefore possible to qualitatively compare the drug sensitivity proxy
built from the personalized models and the experimental values, all
measuring for each cell line the relative variation in proliferation in
response to drug inhibition. Since mechanistic models are essentially
qualitative, it is difficult to give a precise interpretation of the
magnitude of their proliferation variations. Subsequently, the ratio
presented above and the experimental values will be compared simply by
calculating the linear correlation between these variables, in order to
verify whether the ratio is able to reproduce the same trends and to
identify the most or least sensitive lines. Predictive approaches would
require a better prior calibration of the mechanistic models.

\subsubsection{Differential sensitivities to BRAF targeting explained by
personalized logical models}\label{diff-BRAF}

Once the logical model consistency has been validated, personalized
models are generated for each cell line by integrating their interpreted
genomic features directly as model constraints or parameters.
\textbf{Sensitivities to BRAF inhibition inferred from models are then
compared to experimentally observed sensitivities} (Figure
\ref{fig:BRAF-results}). In all the following analyses, we focus on
three different personalization strategies using: only mutations as
discrete personalization (Figure \ref{fig:BRAF-results}A, upper row),
only RNA as continuous personalization (Figure \ref{fig:BRAF-results}A,
middle row) or mutations combined with RNA (Figure
\ref{fig:BRAF-results}A, lower row). These choices reflect first of all
the following \emph{a priori}: mutations are much more drastic and
permanent changes than RNA, whose expression levels are more subject to
fluctuation and regulation. The objective is also to answer the
following questions: What type of data is most likely to explain the
differences in responses? Is it relevant to combine them? Figure
\ref{fig:BRAF-results} shows an example of the type of analyses possible
with personalized models, zooming in more and more on the details from
panel A to panel C.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/BRAF-results} 

}

\caption[Validation of personalized models of BRAF inhibition with cell lines data]{\textbf{Validation of personalized models of
BRAF inhibition with cell lines data.} (A) Pearson correlations between
normalized \emph{Proliferation} scores from models and experimental
sensitivities to BRAF inhibition (drug or CRISPR); only significant
correlations are displayed. (B) Scatter plots with non-overlapping
points corresponding to correlations of panel A for one drug (PLX-4720)
and one CRISPR dataset (Broad) only. (C) Enlargement of one scatter plot
in B (left) with the table describing the omics profiles used for each
cell line to explore the response mechanisms (right); interactive
version in Figure \ref{fig:BRAF-interactive} or
\href{https://github.com/sysbio-curie/PROFILE_BRAF_Model/blob/master/Analysis.html}{GitHub
files}.}\label{fig:BRAF-results}
\end{figure}














The first approach consists in using only mutations as discrete
personalization (Figure \ref{fig:BRAF-results}, A, upper row): the
mutations identified in the dataset and that are present in the
regulatory network are set to 1 for activating mutations and set to 0
for inactivating mutations. In this case, the \emph{Proliferation}
scores from personalized models significantly correlate with both BRAF
drug inhibitors (PLX-4720 and Dabrafenib) and both CRISPR datasets
(using Pearson correlations). Note that the opposite directions of the
correlations for the drug and CRISPR datasets are due to the fact that
cell lines sensitive to BRAF inhibition result in low AUCs, and high
scaled Bayesian factors, respectively, and, if the models are relevant,
to low standardized \emph{Proliferation} scores. Looking more closely at
the corresponding scatter plot for PLX-4720 (Figure
\ref{fig:BRAF-results}B, upper left), it can be seen that this
correlation results from the \textbf{model's ability to use mutations'
information to recover the highest experimental sensitivities of the
BRAF-mutated cell lines} that form an undifferentiated cluster on the
left side. These cell lines are indeed relatively more sensitive than
non-mutated BRAF cell lines. However, the integration of mutations alone
does not explain the significant differences within this subgroup (AUC
between 0.55 and 0.9). A very similar behaviour can be observed when
comparing model simulations with CRISPR data (Figure
\ref{fig:BRAF-results}B, upper right).

Using only RNA data as continuous personalization (Figure
\ref{fig:BRAF-results}A and B, middle rows) is both less informative and
more difficult to interpret. For continuous data such as RNA-sequencing
data, we normalize the expression values and set both the initial
conditions and the transition rates of the model variables to the
corresponding values. Correlations with experimental BRAF inhibitions
appear weaker and more uncertain. The key point, however, is that the
\textbf{combination of mutations and RNA, as depicted in Figure
\ref{fig:BRAF-results} A and B lower rows, seems to be more relevant}.
This is partially true in quantitative terms but it is even easier to
interpret in the corresponding scatter plots (Figure
\ref{fig:BRAF-interactive}). Comparing first the Broad CRISPR scatter
plots using mutations only (Figure \ref{fig:BRAF-results}B, upper right)
and using both mutations and RNA (Figure \ref{fig:BRAF-results}B, lower
right), we can observe that non-responsive cell lines (scaled Bayesian
factor below 0), grouped in the lower right corner and correctly
predicted using only mutations stayed in the same area: these strong
mutational phenotypes have not been displaced by the addition of RNA
data. Other cell lines previously considered to be of intermediate
sensitivity by the model (\emph{e.g.}, COLO-678 or SK-MEL-2) were
shifted to the right, consistent with the lack of sensitivity observed
experimentally. Finally, BRAF-mutated cell lines, previously clustered
in one single column on the left using only mutations (with normalized
\emph{Proliferation} scores around 0.5), have been moved in different
directions. Many of the most sensitive cell lines (scaled Bayesian
factor above 2) have been pushed to the left in accordance with the high
sensitivities observed experimentally (\emph{e.g.}, HT-29 or SK-MEL-24).
It is even observed that the model corrected the position of the two
BRAF mutated cell lines, but whose sensitivity is experimentally low
(melanoma cell line HT-144 and colorectal cell line HT-55). Only one
cell line (SK-MEL-30) has seen its positioning evolve
counter-intuitively as a result of the addition of RNA in the
personalization strategy: relatively sensitive to the inhibition of
BRAF, it has, however, seen its standardised \emph{Proliferation} score
approach 1. All in all, this contribution of RNA data results in
significant correlations even when restricted to BRAF-mutated cell lines
only (\(R=0.69\), \(p.value=0.006\)).

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{06-Drugs_files/figure-latex/BRAF-interactive-1} 

}

\caption[Multi-omics integration and enhanced value of RNA in addition to mutations]{\textbf{Multi-omics integration and
enhanced value of RNA in addition to mutations.} For each cell line, an
arrow shows the impact of adding RNA in the customization strategy. This
graph is present in an
\href{https://jonasbeal.github.io/files/PhdThesis/personalized-logical-models-to-study-an-interpret-drug-response.html\#fig:BRAF-interactive}{interactive
format} in the online version of the thesis in order to give easy access
to the omic profile corresponding to each point.}\label{fig:BRAF-interactive}
\end{figure}









A similar analysis can be made of the impact of adding RNA data to
personalization when comparing with the experimental response to
PLX-4720 (Figure \ref{fig:BRAF-results}B, upper and lower left). Most of
the non-sensitive cell lines (upper right corner) have not seen the
behaviour of the personalized models change with RNA addition. However,
the numerous BRAF-mutated cell lines previously grouped around
standardized \emph{Proliferation} scores of 0.5, are now better
differentiated and their sensitivity predicted by personalized models
has generally been revised towards lower scores (\emph{i.e.}, higher
sensitivity). Similar to the CRISPR data analysis, three sensitive cell
lines have been shifted to the right and are misinterpreted by the
model. As a result, the correlation restricted to BRAF-mutated cell
lines is no longer significant (R=0.26, p.value=0.1).

\subsubsection{An investigative tool}\label{an-investigative-tool}

These \textbf{personalized models are not primarily intended to be
predictive tools but rather used to reason and explore the possible
mechanisms and specificities of each cell line, for example by studying
the molecular alterations at the origin of the observed behaviour}
(Figure \ref{fig:BRAF-interactive}). To continue on the previous
examples, the two melanoma cell lines, HT-144 and SK-MEL-24, share the
same mutational profiles but have very different sensitivities to BRAF
targeting (Figure \ref{fig:BRAF-results}C). This inconsistency is
partially corrected by the addition of the RNA data, which allows the
model to take into account the difference in CRAF expression between the
two cell lines. In fact, CRAF is a crucial node for the network since it
is necessary for the reactivation of the MAPK pathway after BRAF
inhibition. Therefore, the high sensitivity of SK-MEL-24 may be
explained by its low CRAF expression level, which makes the reactivation
of the MAPK pathway more difficult for this cell line. Conversely, in
HT-144, the high level of CRAF expression allows the signal to flow
properly through this pathway even after BRAF inhibition, thus making
this cell line more resistant. The importance of CRAF expression is also
evident in HT-29, a CRC BRAF mutated cell line with other important
mutations (PI3K activation and p53 deletion). However, it remains
sensitive to treatment, due to its very low level of CRAF expression.

Another interesting contribution of RNA appears in the melanoma cell
line UACC-62, which is particularly sensitive to treatment. The model is
able to correctly predict its response once RNA levels are integrated.
In this case, the reason for sensitivity seems to be due to the low
level of PDPK1, which makes it difficult to activate p70 and thus to
trigger the resistance linked to PI3K/AKT pathway activation. Similarly,
the CRC resistant cell line, HT55, which carries only the BRAF mutation,
expresses high levels of PDPK1, in addition to high levels of CRAF,
supporting the idea that the presence of both MAPK and PI3K/AKT pathways
may confer resistance to BRAF inhibition treatments. We can also mention
a cluster of RAS mutated cell lines, usually NRAS mutated for melanomas
(\emph{e.g.}, SK-MEL-2) and KRAS for colorectal cancers (\emph{e.g.},
COLO-678), which are classified by the model as resistant.
Interestingly, in these cell lines, a low level of CRAF is not enough to
block the signal of the MAPK pathway, which is stronger in the model
because of the simulation of the RAS mutation (RAS is set to \(1\)).
Only SK-MEL-30 appears to be incorrectly classified and is observed to
be more sensitive than the other cell lines with a similar mutation
profile. This could be due to the fact that our network is incomplete
and not able to account for some alterations responsible for this cell
line sensitivity. The problem may also come from the fact that this cell
line contains a frameshift mutation of RPS6KB2 (p70 node) not referenced
in OncoKB and therefore not included in the simulation.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{06-Drugs_files/figure-latex/BRAF-results-add-1} 

}

\caption[Application of personalized models to other CRISPR targets]{\textbf{Application of personalized
models to other CRISPR targets.} (A) Personalization strategies using
either mutations only (as discrete data) or combined with RNA (as
continuous data) with their corresponding scatter plots in panels B and
C. (B) Scatter plot comparing normalized \emph{Proliferation} scores of
p53 inhibition in the models with experiment sensitivity of cell lines
to TP53 CRISPR inhibition, indicating p53 mutational status as
interpreted in the model. Pearson correlations and the corresponding
p-values are shown. (C) Similar analysis as in panel B with PI3K model
node and PIK3CA CRISPR inhibition.}\label{fig:BRAF-results-add}
\end{figure}












The versatility of the logical formalism makes it possible to test other
node inhibitions as in Figure \ref{fig:BRAF-results-add}, but remains
limited by the scope of the model. Since the present model has been
designed around BRAF, its regulators have been carefully selected and
implemented, which is not necessarily the case for other nodes of the
model. Therefore, these personalized models can be used to study how
comprehensive the descriptions of the regulation of other nodes or parts
of the model are. Thus, model simulations show that response trends to
TP53 inhibition are consistently recovered by the model (Figure
\ref{fig:BRAF-results-add}B) but the simple regulation of p53 in the
model results in coarse-grained patterns, although slightly improved by
addition of RNA data. Similar analyses regarding the targeting of PIK3CA
(in CRISPR data) simulated, in the model, by the inhibition of PI3K
node, can be performed (Figure \ref{fig:BRAF-results-add}C). \textbf{Low
correlations are an indication highlighting the insufficient regulation
of the node, probably confirming the scope issues raised in the
pan-cancer-preliminary analysis}.

\subsection{Comparison of the mechanistic approach with machine learning
methods}\label{ML-comp}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{06-Drugs_files/figure-latex/BRAF-ML-1} 

}

\caption[Random forests to predict and explain sensitivity to BRAF inhibition]{\textbf{Random forests to predict and explain
sensitivity to BRAF inhibition.} (A) Performances of random forests for
BRAF sensitivity prediction measured with percentage of explained
variance; different learning task with unprocessed original data
(thousands of genes), unprocessed original data for model-related genes
only (tens of genes), and processed profiles of cell lines (tens of
genes); \(n\) samples and \(p\) variables per learning task. (B)
Variable importance for drug prediction only, with the 10 best variables
with positive importance for each case.}\label{fig:BRAF-ML}
\end{figure}











In order to provide comparison elements unbiased by prior knowledge or
by the construction of the model, we performed some simple machine
learning algorithms. Random forests are used as an example of a machine
learning approach to compare with mechanistic models and are implemented
with \emph{randomForestSRC} R package \citep{breiman2001random}. Random
forests can be seen as an aggregation of decision trees, each trained on
a different training set formed by uniform sampling with replacement of
the original cohort. Prediction performances are computed using
out-of-the bag estimates for each individual (i.e, average estimate from
trees that did not contain the individual in their bootstrap training
sample) and summarized as percentage of variance explained by the random
forest. In this case, random forests have been fitted with inputs
(mutations and/or RNA data) and outputs (sensitivities to drug or CRISPR
BRAF inhibition) similar to those of logical models and the
corresponding predictive accuracies are reported in Figure
\ref{fig:BRAF-ML}A. The first insight concerns data processing. The
percentages of variance explained by the models are similar (around 70\%
of explained variance for drug sensitivity prediction) in the following
three cases: unprocessed original data (thousands of genes), unprocessed
original data for model-related genes only (tens of genes), and
processed profiles of cell lines (tens of genes). This supports the
choice of a model with a small number of relevant genes, which appear to
contain most of the information needed for prediction. Second, the
absolute level of performance appears much lower for CRISPR (between 30
and 50\%) probably suffering from the lower number of samples,
especially in cases where the number of variables is the highest. This
tends to \textbf{reinforce the interest of mechanistic approaches that
do not use any training on the data for smaller datasets, less suitable
for learning}. Finally, while mutations and RNA data seem to provide the
same predictive power (especially for drugs), using the two together
does not necessarily result in a better performance in this case.

It is also possible to compute the variable importance that assesses the
contribution of variables to the overall performance. The solution
adopted in this paper to measure it, and called VIMP in the package,
consists in introducing random permutations between individuals for the
values of a variable and quantifying the variation in performance
resulting from this addition of noise. In the case of key variables for
prediction, this perturbation will decrease the performance and will
result in a high variable importance \citep{ishwaran2007variable}.
\textbf{Variable importance in these different random forests are
reported in Figure \ref{fig:BRAF-ML}B and are consistent with the
analysis of mechanistic models}. The mutational status of BRAF is
definitely the most important variable followed by mutations in RAS or
TP53. Concerning RNA levels, the most explanatory variables seem to be
FOXD3 or PTEN, in line with the definition of the logical model.

\section{Application on prostate cancer study and
challenges}\label{prostate-model}

Before summarizing the potential and limitations of the PROFILE
approaches described in this and the previous chapter, a final example
may be mentioned. Indeed, another application of the PROFILE method,
quite similar to the examples presented in the previous and this
chapter, has been carried out on prostate cancer. Chronologically, this
project was one of the first applications of the method. However, as
this project was more collaborative than personal, the previous chapters
have been illustrated by more exclusively personal work when they were
equivalent. We will therefore only briefly mention here the differences
and insights specific to this study.

First, a logical model specific to prostate cancer was developed by some
collaborators (Pauline Traynard and Arnau Montagud) over a long period
of time, resulting in a large and comprehensive model of 146 nodes,
which is described in more detail in the appendix
\ref{appendix-montagud} and Figure \ref{fig:Montagud}. Using the TCGA
prostate cancer dataset (\ref{appendix-prostate}) prognostic validation
of the model was first carried out, similarly to Figure
\ref{fig:PROFILE-METABRIC-Grade}, by comparing individualized scores of
some phenotypes in the model (\emph{i.e.}, \emph{Proliferation}) with
clinical markers, in this case Gleason score, a grading system specific
to prostate cancer. The qualitative evolution of the personalized
\emph{Proliferation} scores is also qualitatively validated (predicted
proliferating tumors are on average of higher grade) but, despite the
specificity and magnitude of the model, much of the variability is not
explained.

The use of cell line data was also explored using Cell Model Passports
data, restricted to the 7 prostate cell lines. The size of the model
then allows qualitative predictions to be made on the proliferative,
apoptotic and metastatic qualities of the different lines. Except for
proliferation, however, experimental validation of the relevance of
these predictions is difficult using public data or the literature. But
again, after these preliminary validations, the focus of the study was
on treatment response with a slightly different rationale than in the
previous example. Focusing on a particular cell line (LnCaP) and its
corresponding personalized logical model, the idea is to
\textbf{simulate with the models all possible inhibitions or
combinations of inhibitions in order to identify possible
vulnerabilities or relevant treatment synergies}. Experimental
validation on the cell line was then carried out for certain genes that
could be targeted depending on the existence of the treatments. The
\textbf{efficacy of certain inhibitions highlighted by the simulations,
such as that of HSP90, was confirmed experimentally} on this particular
cell line. Despite the limitations of the approach in this application
to prostate cancer, the study demonstrates the feasibility of the method
for investigating the complexity of therapeutic responses and guiding
experimental validation.

\section{Limitations and perspectives}\label{part2-limits}

The emergence of high-throughput data has made it easier to generate
models for prognostic or diagnostic predictions in the field of cancer.
The numerous lists of genes, biomarkers and predictors proposed have,
however, often been difficult to interpret because of their sometimes
uncertain clinical impact and little overlap between competing
approaches \citep{domany2014using}. Methods that can be interpreted by
design, which integrate \emph{a priori} biological knowledge, therefore
appear to be an interesting complement able to reconcile the omics data
generated and the knowledge accumulated in the literature.

These benefits come at the cost of having \textbf{accurate expert
description of the problem} to provide a relevant basis to the
mechanistic models. This is particularly true in this work since the
personalized models all derive from the same structure (the initial
generic logical model) of which they are partially constrained versions.
It is therefore necessary to have a generic model that is both
sufficiently accurate and broad enough so that the data integration
allows the expression of the singularities of each cell line. If this is
not the case, the learning of logical rules or the use of ensemble
modeling could be favoured, usually including perturbation time-series
data \citep{razzaq2018computational}. It should also be noted that, in
the logical models presented here, the translation of biological
knowledge into a logical rule is not necessarily deterministic and
unambiguous. The choices here have been made based on the interpretation
of the literature only. And the presence of certain outliers,
\emph{i.e.}, cell lines whose behaviour is not explained by the models,
may indeed result from the limitations of the model, either in its scope
(important genes not integrated), or in its definition (incorrect
logical rules). More global or data-driven approaches to define the
model would be possible but would require different training/validation
steps and different sets of data.

The \textbf{second key point is the omics data used}. For practical
reasons, we have focused on mutation and RNA data. The legitimacy of the
former is not in doubt, but their interpretation is, on the other hand,
a crucial point whose relevance must be systematically verified. The
omission or over-interpretation of certain mutations can severely affect
the behaviour of personalized models. Validation using sensitivity data
provides a good indicator in this respect. However, the question is
broader for RNA data: are they relevant data to be used to personalize
models, \emph{i.e.}, can they be considered as \textbf{good proxies for
node activity?} The protein nature of many nodes in the model would
encourage the use of protein level data instead, or even phosphorylation
levels if they were available for these data. One perspective could even
be to push personalization to the point of defining different types of
data or even different personalization strategies for each node
according to the knowledge of the mechanisms at work in the
corresponding biological entity. A balance should then be found to allow
a certain degree of automation in the code and to avoid overfitting.

Despite these limitations, the results described above support the
\textbf{importance of combining the integration of different types of
data to better explain differences in drug sensitivities}. There was no
doubt about this position of principle in general
\citep{azuaje2017computational}, and in particular in machine learning
methods \citep{costello2014community, aben2016tandem}. The technical
implementation of these multi-omic integrations is nevertheless more
difficult in mechanistic models where the relationships between the
different types of data need to be more explicitly formulated
\citep{klinger2013network}. The present work therefore reinforces the
possibility and value of integrating different types of data in a
mechanistic framework to improve relevance and interpretation and
illustrates this by highlighting the value of RNA data in addition to
mutation data in predicting the response of cell lines to BRAF
inhibition. In addition, one piece of data that could be further
exploited is that of the specific behaviour of the drugs or inhibitors
studied, since for instance some BRAF inhibitors have affinities that
vary according to mutations in the BRAF gene itself. The integration of
truly precise data on the nature of the drug is nevertheless limited by
logical formalism and is more often found in more flexible approaches,
\emph{e.g.}, in deep learning \citep{manica2019toward}.

The application presented in this chapter, focused on BRAF inhibitors,
made it possible to verify the good performance of the models through
different types of data (drug or CRISPR/Cas9). However, the molecular
profiles used to personalize the models were all derived from cell
lines, reported in the same database \citep{van2019cell}. It would be
possible to use different types of data such as organoids,
patient-derived xenografts (PDX), etc. The critical clinical question
will then be: \textbf{do the mechanisms highlighted for cell lines
transfer easily to tumours in vivo?} The ability to identify common
reasons explaining the response to treatments has been studied by
different statistical approaches with the aim of promoting translational
medicine \citep{mourragui2019precise, kim2019genomic}. The ability of
personalized mechanistic models to follow this path remains to be
explored.

To conclude, we provide a comprehensive pipeline from clinical question
to a validated mechanistic model which uses different types of omics
data and adapts to dozens of different cell lines. This work, which is
\textbf{based only on the interpretation of data and not on the training
of the model}, continues some previous work that has already
demonstrated the value of mechanistic approaches to answer questions
about response to treatment, especially using dynamic data
\citep{saez2020personalized}, and sometimes about similar pathways
\citep{klinger2013network}. In this context, our approach proves the
interest of logical formalism to make use of scarce and static data
facilitating application to a wide range of issues and datasets in a way
that is sometimes complementary to learning-based approaches.

\BeginKnitrBlock{conclubox}
\subsubsection*{Summary}\label{summary-5}
\addcontentsline{toc}{subsubsection}{Summary}

The explicit representation of biological entities in mechanistic models
makes them attractive for studying the impact of treatments: if the
mechanism of action of a drug is known, it can be added to the structure
and the effect of the induced perturbation on the system can be
measured. This approach has been applied to personalized logical models,
in particular to decipher differences in sensitivity to BRAF inhibitors
in patients with colorectal and melanoma cancers. Based on a generic
network of the surrounding pathways common to both cancers, the
personalized models were able to capture a significant proportion of
these differential sensitivities with untrained personalized models. The
best correlations result from the joint use of mutation and RNA data
supporting the integration of multi-omic data. Personalized models also
offer an opportunity to highlight and explain some of the mechanisms at
work in these differences in sensitivities. Finally, a comparison with
machine learning approaches requiring training, such as random forests,
shows the complementarity of mechanistic apparoches, particularly in the
case of small sample sizes.
\EndKnitrBlock{conclubox}

\part{Statistical quantification of the clinical impact of
models}\label{part-statistical-quantification-of-the-clinical-impact-of-models}

\chapter{Information flows in mechanistic models of
cancer}\label{information-flows-in-mechanistic-models-of-cancer}

\epigraph{"Et l'effet qui s'en va nous decouvre les causes."}{Alfred de Musset (Poésies nouvelles, 1843)}

\initial{T}he mechanistic models of cancer presented in the previous
section have allowed us to integrate the omics data, to ``make them
speak'' in order to better understand the clinical characteristics of
cell lines or patients. But beyond their undeniable intellectual and
scientific interest, do they have a direct clinical utility? Given the
abundance and complexity of patient data available to physicians, the
use of computer tools and mathematical models is inevitable and
increasingly frequent. Because of their explicit representation of
phenomena, mechanistic models can provide a more easily understood
alternative for physicians or patients. Is it therefore desirable and
relevant to use these models in support of medical decision making? And
how can their clinical validity and impact be rigorously measured?

First of all, the purpose of this chapter is to outline some of the
limitations of the previously presented evaluations of mechanistic
models, together with some recommended statistical tools. These
evaluations answered the question: do the models have any clinical
utility? We will show that an additional question could be: \textbf{do
mechanistic models have an incremental clinical utility}, in comparison
to the direct use of the data used to construct or specify them? This
chapter is intended as a statistical introduction for systems biologists
to some of the problems encountered in model evaluation.

\BeginKnitrBlock{summarybox}
\subsubsection*{Scientific content}\label{scientific-content-4}
\addcontentsline{toc}{subsubsection}{Scientific content}

This chapter is relying on literature for the first section and
unpublished content for the second. The exploratory analyses presented
below have helped to clarify considerations expressed qualitatively in
previous chapters and formed the starting point for subsequent chapters
on the clinical impact of cancer models.
\EndKnitrBlock{summarybox}

\section{Evaluation of models as
biomarkers}\label{evaluation-of-models-as-biomarkers}

\subsection{Evaluation framework and general
principles}\label{evaluation-framework-and-general-principles}

First of all, mechanistic models of cancer should be considered as
biomarkers among others, and therefore evaluated as such. This means
focusing on the clinical information provided by the model outputs. In
the previous examples, these outputs would be for example the
\(H\)/\(K_{50}\)/\(A\) biomarkers from Fey's model (described in section
\ref{prognostic}) or the personalized \emph{Proliferation} scores from
the mechanistic models in the examples in sections
\ref{validation-METABRIC} or \ref{diff-BRAF}. The prognostic or
predictive value of model outputs can then be evaluated according to the
methods and recommendations present in the literature on prognostic or
predictive biomarkers. Without going into too much detail, guidelines in
this area are quite numerous and detailed, both for prognostic
biomarkers \citep{mcshane2005reporting, sauerbrei2018reporting} and
predictive biomarkers \citep{janes2014approach}. Most of the points
mentioned in these articles should apply identically for the particular
type of biomarker that are the outputs of mechanistic models of cancer.
The purpose of this thesis is not to exhaustively list these
recommendations for the evaluation of biomarkers, so we will simply
highlight the \textbf{most salient issues identified in the systems
biology literature}.

\subsection{Some frequent problems and recommended statistical
tools}\label{some-frequent-problems-and-recommended-statistical-tools}

Concerning continuous and prognostic model outputs/biomarkers, they are
sometimes confronted with naturally binary data (\emph{e.g.}, event or
not). In this case, many methods exist, among which the \textbf{Area
Under the receiver operating Curve (ROC), usually denoted as AUC}
\citep{soreide2009receiver}. With a continuous biomarker \(X\) and a
binary outcome to predict \(D\), the ROC curve plots sensitivity,
\(P(X > c | D = 1)\), against
\((1 -~\text{specificity})= (1-P(X\leq c|D= 0))\), for all possible
values \(c\). the AUC is then simply computed as the area under this
curve. The resulting AUC is computed as the area under this curve and
measures the ability of the biomarker to discriminate between the two
classes of interest and is a common tool for the evaluation of
biomarkers.

However, prognostic validation often requires time-to-envent data such
as survival data. Very schematically, if we study patients suffering
from cancer and we are interested in the \emph{death} event from a time
\(t_0\), which we define to be common to all patients, different cases
are possible. Some patients have died and we therefore know their status
and the time of their death \(t_1\). Others are still alive at the time
\(t_{max}\) when the study stops (administrative censorship) or have
withdrawn from the study at time \(t_2\) so that their fate is then
unknown. These patients are said to be right-censored. These data are
extremely frequent and require specific methods such as Cox's
proportional risk model \citep{cox1972regression} or accelerated failure
time models. The \textbf{reasoned use of these dedicated survival
models, and the associated assumptions, should be preferred to the
forced binarization of survival data}, sometimes encountered in an
apparent concern for simplification. In general, the validation of
prognostic biomarkers using survival data therefore requires specific
metrics such as \textbf{time dependent AUC for censored survival data}
\citep{heagerty2000time}. This measure, however, requires a more complex
definition of sensitivity and specificity to accommodate censored data
\citep{heagerty2005survival} and to be applicable to real biomarker
validation data \citep{buyse2006validation}. Despite fairly frequent use
\citep{ching2018cox}, the use of another metric called c-index is not
recommended for assessing a model's ability to predict risk over a given
time horizon \citep{blanche2019c}. However, the clinical interpretation
of AUC values is not straightforward and the complementarity of other
approaches that focus on \textbf{the usefulness of risk models at the
population level} rather than the ability to discriminate has been
highlighted \citep{pepe2008integrating}. The costs and benefits of
prognostic models with various AUC values have been studied in an
applied context by \citet{gail2018breast}.

Another frequent issue, already encountered in the examples from
previous chapters, is the discretization of continuous markers. This is
often done in order to classify patients into high and low risk groups
for example for prognostic biomarkers. In the first place, although
discretization may be required clinically, it is not necessary to
evaluate the clinical value of the biomarker beforehand. Secondly, the
choice of thresholds is crucial. In particular, in the case of
biomarkers derived from mechanistic models, the artificial nature of the
markers often makes difficult a binarization based on an \emph{a priori}
interpretation of the values. Choosing the cut-off point in order to
maximise the significance or separation of the survival curves, as
proposed in \citet{fey2015signaling} and presented in Figure
\ref{fig:fey}, is however not recommended \citep{altman1994dangers},
among other things because it can be interpreted as uncorrected multiple
testing. Such practices may thus contribute to the low clinical
reproducibility of the contribution of certain biomarkers
\citep{hilsenbeck1992so}. For this problem in particular, tools have
been proposed in the literature on clinical biomarkers, such as the
\textbf{predictivenesse curve} \citep{mboup2020evaluating}. Similarly
but in a more general framework, \citet{janes2014approach} propose the
use of \textbf{risk curves} to better evaluate predictive biomarkers
beyond the crude computation of statistical interaction between the
biomarker and the treatment in randomized clinical trial.

Another potential issue, particularly important in subsequent analyses
(see section \ref{information}), is the \textbf{incremental value of
biomarkers}. For instance, in the context of prognostic biomarkers, it
is of course necessary to present univariable analyses showing the
relationship between the marker and the outcome, which is almost
systematically done, but also to question the value of this biomarker
compared to other prognostic factors already known: does it add
information or is it redundant? It is theoretically possible to consider
the potential increase in AUC resulting from the addition of the new
biomarker to the model. The increase in AUC, however, requires the
addition of very strong markers \citep{gail2018breast}, which has
prompted the emergence of popular alternative metrics (Net
Reclassification Index NRI, integrated discrimination improvement IDI)
to evaluate the added predictive ability of a new marker
\citep{pencina2008evaluating}. However, these metrics have been
criticized as being unsafe since they can improve with the addition of
non-informative markers \citep{hilden2014note, pepe2014net}.

All in all, the first step in a good evaluation of mechanistic models
would be that the \textbf{standards recommended for the evaluation of
biomarkers can be applied in the same way to mechanistic models} that
have certain applications or validation based on prognostic or
predictive values. As these topics are well covered in the relevant
literature, we will subsequently focus on a \textbf{specific vision of
incremental value of biomarkers that is more specific to mechanistic
models}.

\section{Processing of biological
information}\label{processing-of-biological-information}

Mechanistic models, and their outputs in particular, have so far been
considered and evaluated as biomarkers. A comprehensive appreciation
requires that they be seen as \textbf{information processing tools} in
relation to the biological data they use. In this section, we will focus
on a toy example to introduce some concepts. We will thus speak in
general terms of the clinical value of this model, understood in the
sense of a prognostic or predictive value depending on the application.
The next section (\ref{reanalysis}) will extend the same analyses to
published models. The purpose of these two sections is to question the
way in which mechanistic models process information. These
\textbf{qualitative questions have been written essentially for those
who design mechanistic models}. For the sake of technical
simplification, the statistical tools chosen for illustration are
therefore simpler than those presented in the previous section.

\subsection{Information in, information out}\label{information}

Indeed, the mechanistic models presented in this thesis (Figures
\ref{fig:fey}, \ref{fig:PROFILE-METABRIC-Survival} and
\ref{fig:BRAF-results}) can be schematically represented by Figure
\ref{fig:box-mech}: inputs \(X\) (often omics data) are processed
through a mechanistic model (here the grey box) to result in an output
\(Y\). These models can thus be assimilated to a mathematical
transformation, often non-linear, of \(X\) in \(Y\). Thus, when
validating the biological or clinical relevance of \(Y\), either by
calculating a correlation with the ground truth or by using it to
stratify survival curves, only the univariate value of \(Y\) is checked.
This is an important step and a prerequisite for a well-constructed
model. On the other hand, it is not sufficient information to understand
how the model works. Indeed, the inputs \(X\) can themselves be
prognostic biomarkers, and this from the outset and before being
transformed into Y: \emph{e.g.}, if the mechanistic model uses different
inputs, each of which has a prognostic value, the fact that the output
also has a prognostic value does not necessarily indicate the relevance
of the model in its ability to make sense of the information. In short,
\textbf{measuring only the output value of the model does not
necessarily reveal the model's ability to make sense of the data it
uses}. Put more figuratively, it is important to know whether the model
turns lead into gold by giving a clinical interpretation (\(Y\)) based
on information that was devoid of it (\(X\)), or whether it simply turns
gold into gold by refactoring information that is already clinically
relevant. In the latter case, the interest of the model may lie in the
way it better summarizes or makes the information understandable, but
its incremental clinical value is low.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{fig/box-mech} 

}

\caption[Evaluation of a mechanistic model]{\textbf{Evaluation of a mechanistic model.}
Adapted from Figure \ref{fig:boxes}.}\label{fig:box-mech}
\end{figure}




Therefore, the question of the incremental value of the model can be
explained as follows: what does the output of the model represent in
relation to the inputs? If we restrict ourselves to cases where the
absolute biological/clinical value of \(Y\) is positive, we can then
identify two families of situations. First we can imagine a situation
where the mechanistic model has ``improved'' the value of the inputs:
the output would then have a higher value than the inputs (better
biological validation, better pronostic value etc.), or in any case a
complementary value, a value not present in the inputs. This would
correspond to the \textbf{capture by the model of emerging or non-linear
effects}. For the sake of simplification, we will here assimilate the
two in the sense that a non-linear effect resulting from the interaction
between certain variables was indeed not predictable from the components
taken individually, and therefore emergent. Note, for example, that the
identification and capture by statistical models of non-linear
components of treatment response is important in the ability to
generalize findings from preclinical models to human tumours
\citep{mourragui2020precise}. In the second situation, the output does
not capture emergent properties but summarizes, totally or partially,
the information present in the inputs. This would correspond to a
\textbf{knowledge-informed dimensionality-reduction}. Even in the latter
case, the scientific value of the model as a tool for understanding is
not necessarily questioned. The analyses presented below are simply
intended to supplement the understanding of models and how they process
information.

\subsection{Emergence of information in artificial
examples}\label{emergence-of-information-in-artificial-examples}

These questions can be illustrated using a very simple artificial model
represented in Figure \ref{fig:model-simulation}. On the one hand there
are two latent biological variables called \emph{Proliferation (P)} and
\emph{Apoptosis (A)} resulting in our biological ground truth,
\emph{Growth}. On the other hand, the modeler has access to three
different random variables \(N_1\), \(N_2\) and \(N_3\) respectively
associated with the sign of \emph{P}, the absolute value of \emph{P} and
the value of \emph{A}. Two mechanistic models are defined, one linear
(with its output \(O_{linear}\)) and one non-linear (with its output
\(O_{non-linear}\)). We note that the two outputs are sufficiently well
defined to be correlated with \emph{Growth} but only the non-linear
model makes use of \(N_2\) by multiplying it with \(N_1\).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{fig/model-simulation} 

}

\caption[Definition of two distinct mechanistic models]{\textbf{Evaluation of a mechanistic model.}
Adapted from Figure \ref{fig:boxes}.}\label{fig:model-simulation}
\end{figure}







The ability of models to use inputs to create or summarize information
through outputs will be studied using the \textbf{explained variation
metric \(R^2\)}. If a linear model is defined as
\(y_i=\beta_0+\beta_1x_i+e_i\), linear coefficients \(\beta\) are
estimated by minimizing the sum of squared differences between predicted
and real values of \(y\). The fitted model is written
\(\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_i\) and \(R^2\) also called
coefficient of determination is defined as:

\[R^2=1-\dfrac{\sum_{i=1}^{n} (\hat{y_i}-\bar{y_i})^2}{\sum_{i=1}^{n}  (y_i-\bar{y})^2}\]

Therefore \(R^2\) measures the proportion of variation in \(y\) that is
explained by the regressors. A different way of expressing this is to
say that \(R^2\) compares the null model without covariate (observations
are compared to their mean) to the linear model with covariates. By
extension, it has been proposed to use R2 to assess the effect of adding
a new biomarker to a previously established model\footnote{An
  unpublished note by Frank Harrell details and illustrates the
  possibilities and limitations of \(R^2\) for this type of analysis
  (\href{https://www.fharrell.com/post/addvalue/}{link})}
\citep{schemper2003predictive}. In order to avoid overfitting, it is
possible to calculate the adjusted \(R^2\) that corrects with the number
of regressors or to fit the model on training data and calculate the
\(R^2\) on validation data. The latter option was chosen using cross
validation and averaging over the \(R^2\) obtained in the different
folds. Metrics with an interpretation similar to \(R^2\) have been
defined for logistic regressions or survival analysis
\citep{choodari2012simulation}. In the case of regressions with several
variables \(x_i\), it is possible to \textbf{decompose \(R^2\) into
different components associated with each of the variables}. This
decomposition is carried out here by averaging over orderings according
to the method proposed by \citet{lindeman1980introduction} and applied
in R code by \citet{gromping2006relative}. The precise formulas are
detailed in appendix \ref{appendix-decomp}.

Here is an example of schematic reasoning that can be carried out with
\(R^2\) about the two models in Figure \ref{fig:model-simulation}. We
will \textbf{denote \(R^2_{X_1+X_2}\) the \(R^2\) corresponding to the
linear model \(Growth=\beta_0+\beta_1 X_1+\beta_2 X_2\)} (written more
compactly \(Y\sim X_1+X_2\), by analogy to its implementation in
\emph{R}). Using only the outputs of the models to predict
\emph{Growth}, explained variations are \(R^2_{O_{non-linear}}=0.455\)
and \(R^2_{O_{linear}}=0.379\). The mechanistic models are thus
correctly defined since the mechanistic output partly recover the
biological read-out. However, the inputs of the model also have an
important predictive value since \(R^2_{N_1+N_2+N_3}=0.514\). How can we
understand the relationship between these values? First, the model
including the \(N_i\) inputs and the output \(O_{linear}\) as regressors
show identical performances with

\[R^2_{N_1+N_2+N_3+O_{linear}}=0.514=R^2_{N_1+N_2+N_3},\]

which means that \(O_{linear}\) has no incremental value compared to a
linear combination of the inputs. This is perfectly obvious from a
statistical point of view since the two models are equivalent:

\begin{equation*}
\begin{aligned}
Growth &=\beta_0+\beta_1 N_1+\beta_2 N_2 + \beta_3 N_3+\beta_4 O_{linear}\\
       &=\beta_0+(\beta_1+2\beta_4) N_1+\beta_2 N_2 + (\beta_3 - \beta_4) N_3
\end{aligned}
\end{equation*}

The purpose of this example is to explicitly underline what is done
implicitly in the study of certain mechanistic models. The complexity of
the described mechanisms sometimes hides more or less linear
combinations of inputs that may make it possible to obtain meaningful
biomarkers but without incremental value by construction. On the other
hand, \(O_{non-linear}\) has allowed to extract an emergent information
which improves the global prediction when combined linearly with the
inputs: \[R^2_{N_1+N_2+N_3+O_{non-linear}}=0.586>R^2_{N_1+N_2+N_3}.\]

We can go further in understanding by breaking down the \(R^2\). In
Figure \ref{fig:R2-artificial}A and B (left columns), \(R^2\) of the
inputs' models
(\(Growth = \beta_0+\beta_1 N_1+\beta_2 N_2 + \beta_3 N_3\)) are
decomposed to show that \(N_1\) and \(N_3\) contribute most to the
prediction in a linear model. By using the same strategies for
decomposing the \(R_2\) and calculating the incremental \(R^2\), it is
also possible to \textbf{decompose the \(R^2\) of \(O_{linear}\) and
\(O_{non-linear}\) according to its origin: its component \(N_1\)
(\(0.22\) in Figure \ref{fig:R2-artificial}A) is the proportion of
\(R^2\) that is also explained by \(N_1\), so it can be interpreted as
being the part of the value of \(N_1\) captured by \(O\)}. In the
non-linear case, we can see in the decomposition that \(O_{non-linear}\)
has an additional created component (\(0.07\)), it is the non-linear
component that is not shared with any of the inputs.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{07-Evaluation_files/figure-latex/R2-artificial-1} 

}

\caption[Decomposition of $R^2$ for inputs and output of example models]{\textbf{Decomposition of \(R^2\) for inputs
and output of example models.} (A) Results for the non-linear model
inputs and output \(O_{non-linear}\) as defined in Figure
\ref{fig:model-simulation}: the left column represents the \(R^2\)
decomposition of model
\(Growth = \beta_0+\beta_1 N_1+\beta_2 N_2 + \beta_3 N_3\) and the right
column the \(R^2\) decomposition of
\(Growth = \beta_0+ \beta_4 O_{non-linear}\). (B) Same with the linear
model and the corresponding \(O_{linear}\). For both (A) and (B), colors
represent the origin of \(R^2\) contribution according to the
decomposition. In particular, for right colums (model\(Y\sim O\)), the
red share represent the proportion of the \(R^2\) of the regressor \(O\)
that does not come linearly from the inputs, and therefore its emerging
part. The horizontal reference line corresponds to the maximal \(R^2\)
obtained from the model
\(Growth = \beta_0+\beta_1 N_1+\beta_2 N_2 + \beta_3 N_3 + \beta_4O\)}\label{fig:R2-artificial}
\end{figure}


















In conclusion, if these two models generate meaningful outputs that are
correlated with the biological read-out \emph{Growth}, the analysis of
their information processing classifies them into two different
categories outlined in the previous sub-section. The linear model
summarizes some of the information present in the inputs, without
creating any. It can be likened to a relevant dimensionality reduction.
The output of the non-linear model also fails to avoid some information
losses, but at the same time it extracts new non-linear information.
Thus, in combination with the inputs, it provides incremental value
measured by the increase in total \(R^2\). Note that \(R^2\) is used
here as one tool among others to illustrate the reflection on
personalized mechanistic models as information processing tools. The
point to remember is not technical but rather methodological: these
\textbf{mechanistic models using on omics data cannot be evaluated for
themselves but must be evaluated in comparison with the data they use in
order to better explain the way they process information}. Following
this rationale of model selection, other tools such as the Akaike
Information Criterion (AIC) have been proposed and could allow to
quantify if the reduction of dimension carried out by the models (from
many omics inputs to one mechanistic output) allows a more parsimonious
description of biology than the direct use of inputs
\citep{kirk2013model}.

\section{Reanalysis of mechanistic models of cancer}\label{reanalysis}

Using the tools presented above, it is possible to deepen the analysis
of some mechanistic models already presented in this thesis.

\subsection{\texorpdfstring{ODE model of JNK pathway by
\citet{fey2015signaling}}{ODE model of JNK pathway by @fey2015signaling}}\label{ode-model-of-jnk-pathway-by-fey2015signaling}

One of the first applications of personalized mechanistic models to
cancer is the one proposed by \citet{fey2015signaling} regarding JNK
pathways in patients with neuroblastomas. This work has been described
in section \ref{prognostic} and is recalled in Figure \ref{fig:fey2}.
The evaluation of the mechanistic models in the original paper was
performed by assessing the clinical value of the inputs (RNA levels of
ZAK, MKK4, MKK7, JNK and AKT genes) and outputs (\(H\), \(A\) and
\(K_{50}\)) separately by comparing them with survival data. The outputs
were binarized to optimize the separation between the curves in a
log-rank test. In this section we propose to \textbf{quantify the value
of the output in relation to those of the inputs}, leaving the output
continuous, using the tools described in the previous section. In the
context of survival data, different measures called \(R^2\) by analogy
have been described in the literature. The one used thereafter was
described by \citet{royston2004new}, its detailed definition is given in
Appendix \ref{appendix-r2surv} and its properties have been studied and
validated in previous studies using simulated data
\citep{choodari2012simulation}. \(R^2\) is not the preferred tool for
survival data and is only used here to allow a qualitative description
in line with the previous ones without introducing new tools. A formal
and rigorous analysis should favour the tools presented at the beginning
of the chapter.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/fey} 

}

\caption[Mechanistic modeling of JNK pathway and survival of neuroblastoma patients, as described by Fey \emph{et al}.]{\textbf{Mechanistic modeling of JNK pathway and
survival of neuroblastoma patients, as described by
\citet{fey2015signaling}.} (A) Schematic representation (as a process
description \citep{le2015quantitative}) for the ODE model of JNK
pathway. (B) Response curve (phosphorylated JNK) as a function of the
input stimulus (Stress) and characterization of the corresponding
sigmoidal function with maximum amplitude \(A\), Hill exponent \(H\) and
activation threshold \(K_{50}\). (C) Survival curves for neuroblastoma
patients based on binarized \(A\), \(K_{50}\) and \(H\); binarization
thresholds having been defined based on optimization screening on
calibration cohort.}\label{fig:fey2}
\end{figure}













Thus, the \(R^2\) of the output \(H\) is \(0.39\) while that of the
combined inputs is \(0.60\). We can see from the decompositions that
\(H\) derives most of its the value from ZAK, MKK4 and AKT (Figure
\ref{fig:R2-Fey}A, right column), which were already the largest
contributors in the combined evaluation of the inputs (Figure
\ref{fig:R2-Fey}A, left column). However, \(H\) also includes an
emerging non-linear share (\(R^2=0.08\)) that was not explained by the
linear combination of inputs. Thus, incorporating \(H\) with the inputs
in a survival prediction model does indeed allow to observe an added
value with a global \(R^2\) of \(0.68\). In addition, the authors in the
original study stressed the importance of positive feedback from JNK to
MKK7 (Figure \ref{fig:fey2}A). In its absence, we find that the value of
\(H\) is almost reduced to zero, since not only its non-linear part
(Figure \ref{fig:R2-Fey}, red share), but also its parts derived from
inputs, disappear. Analyzing the other outputs of the model (\(A\) and
\(K_{50}\)) reveals similar but less dramatic trends underlining the
importance of this feedback which allows the model to capture a
clinically relevant behaviour, assimilated by the authors to the
capacity of cells to trigger apoptosis in case of stress. In the case of
this model, the analyses provide a better understanding of how the model
works with respect to survival prediction: \textbf{the outputs partly
summarize clinical information already present in the inputs but also
reveal relevant emerging information}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{07-Evaluation_files/figure-latex/R2-Fey-1} 

}

\caption[Decomposition of $R^2$ for inputs and output for ODE model in Fey \emph{et al}.]{\textbf{Decomposition of \(R^2\) for inputs and
output for ODE model in \citet{fey2015signaling}.} (A) Results for the
Fey model inputs and output \(H\) as defined in Figure \ref{fig:fey2}A
and B. (B) Same using the model without positive feedback between JNK
and MKK7. Colors represent the origine of \(R^2\) contribution. In
particular, for right colums (model\(Y\sim H\)), the red share represent
the proportion of the \(R^2\) of the output \(H\) that does not come
linearly from the inputs, and therefore its emerging part.}\label{fig:R2-Fey}
\end{figure}










\subsection{Personalized logical models: BRAF inhibition in melanoma and
colorectal
cancers}\label{personalized-logical-models-braf-inhibition-in-melanoma-and-colorectal-cancers}

Similarly, it is appropriate to assess the relevance of the personalized
logical models presented so far. Unlike the models of the previous
sub-section, however, they integrate a much larger number of variables
and the decomposition of \(R^2\) is no longer accessible, because of its
computational cost, which increases exponentially with the number of
variables. If we focus on the example best suited to these models, that
of BRAF inhibition sensistivity, we can however reformulate the question
more simply. Given that the most important predictor of the answer is
the status of the BRAF mutation itself, \textbf{do the personalized
models allow us to do better or provide additional information?} In the
case of CRISPR data, the \(R^2\) of BRAF alone is \(0.75\), the \(R^2\)
of the personalized scores from the models is \(0.73\), while the
combination of the two increases the \(R^2\) to \(0.83\). In the absence
of a precise decomposition, this gain can come either from the
contribution of the other variables used in the model (the RNA levels of
CRAF for example) or from the emergence of non-linear effects. In both
cases, these figures are another way of expressing the remarks in
section \ref{diff-BRAF}: thanks to the integration of other data and
their organization in a framework based on literature knowledge, the
model provides a more precise and complete vision of the response
mechanisms. As positive as it is, this increase in \(R^2\) remains
modest, illustrating that the \textbf{main interest of these models is
not necessarily a pure gain in predictive performance. Rather, it lies
in their explanatory capacity and in their ability to support the
investigation of mechanisms} such as in section \ref{diff-BRAF}. In a
complementary way, one could imagine extending these analyses to other
nodes of the model and not only to its output in order to dissect even
more precisely the information processing within the model.

\BeginKnitrBlock{conclubox}
\subsubsection*{Summary}\label{summary-6}
\addcontentsline{toc}{subsubsection}{Summary}

The use of certain mechanistic models of cancer for clinical purposes
reinforces the importance of a rigorous evaluation of their performance.
One of the main recommendations is to consider the outputs of these
models as classical biomarkers, ignoring initially the mechanistic
architecture that generated them. Numerous methods and metrics then
exist to quantify the clinical information they provide, whether
prognostic or predictive. In a complementary way, it is then insightful
to examine the particularities of this kind of biomarker which does not
come out of nowhere but often results from the knowledge-based
transformation of biological data, including possibly other biomarkers.
The prognostic or predictive value of mechanistic models must then be
interpreted against the value of the biological data used to build or
customize the models.s
\EndKnitrBlock{conclubox}

\chapter{Clinical evidence generation and causal
inference}\label{causal-chap}

\epigraph{"Maudit

soit le père de l'épouse

du forgeron qui forgea le fer de la cognée

avec laquelle le bûcheron abattit le chêne

dans lequel on sculpta le lit

où fut engendré l'arrière-grand-père

de l'homme qui conduisit la voiture

dans laquelle ta mère

rencontra ton père!"}{Robert Desnos (La colombe de l'arche, 1923)}

\initial{T}he previous chapter introduced some tools to evaluate and
quantify the value of mechanistic models, and in particular their
outputs, with simple statistical tools. The latter, such as \(R^2\), are
by no means specific to medical applications. One of the particularities
of mechanistic cancer models, on the other hand, is the possibility of
simulating treatments that imitate therapeutic interventions. Before
tackling more precise questions, this chapter will therefore introduce
certain clinical or statistical methods used to evaluate the effect of
different types of treatments on patients. A more specific issue related
to the evaluation of mechanistic models will be explored in the next
chapter using these methods.

\BeginKnitrBlock{summarybox}
\subsubsection*{Scientific content}\label{scientific-content-5}
\addcontentsline{toc}{subsubsection}{Scientific content}

This short chapter introduces the framework of causal inference based on
the literature and the description of causal inference in the preprint
\citet{beal2020causal}.
\EndKnitrBlock{summarybox}

\newcommand{\indep}{\perp \!\!\! \perp}

\section{Clinical trials and beyond}\label{clinical-trials-and-beyond}

\subsection{Randomized clinical trials as gold
standards}\label{randomized-clinical-trials-as-gold-standards}

When it comes to evaluating the effect of a therapeutic intervention,
the reference method in most cases in modern medicine is the randomized
clinical trial, which will be described now in its simplest version.
Without loss of generality, the rationale for this approach can be
detailed for one drug, which will be referred to as \(A\) in the
remainder of the chapter (Figure \ref{fig:trials}). The patients who can
benefit from this drug, and therefore those eligible for the clinical
trial, are first of all defined (specific disease, characteristics,
etc.). Then, they are randomly separated into two distinct groups, one
receiving the new treatment to be evaluated (\(A=1\)) and the other
generally receiving the treatment considered as standard of care, or a
placebo if no validated treatment is available (\(A=0\)). A predefined
treatment response criterion \(Y\) (viral load, tumor size, etc.) is
then compared for the two groups to quantify the average treatment
effect (ATE):

\[ATE= E[Y|A=1]-E[Y|A=0]\]

Thus it will be possible to say, for example, that ``compared to
patients who received the standard treatment, those treated with the new
drug have a 20\% lower tumor volume''. In this example, \textbf{randomly
choosing how the two groups of patients, treated and untreated, are
constituted ensures \emph{a priori} that the two groups are comparable}.
Indeed, it should be verified that the untreated patients were not on
average suffering from more advanced cancers that are more likely to
proliferate and grow. In this case, the difference in outcome between
the groups could simply come from a difference in initial composition
and not from a difference derived from therapeutic interventions. Random
assignment of treatments therefore offers minimum guarantees concerning
the characteristics of the two subgroups.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/trials} 

}

\caption[Principles of randomized clinical]{\textbf{Principles of randomized clinical trials.}.
This trial evaluates the impact of treatment \(A\).}\label{fig:trials}
\end{figure}




\subsection{Observational data and confounding
factors}\label{observational-data-and-confounding-factors}

The problem of comparability between the two groups is reinforced when
the data used does not come from a randomized clinical trial. In the
remainder of this thesis these data will be called \textbf{observational
data}. This means that in the available data, some patients were treated
with the new drug (\(A=1\)) and others received the reference treatment
(\(A=0\)). However, the assignment of treatment was not decided by the
observer. This assignment was therefore made according to a protocol
unknown to the observer which has no guarantee that the two groups are
in fact comparable.

The situation can be illustrated with a simple simulated example
involving a confounding variable \(C\) in addition to the treatment
variable \(A\) and the outcome variable \(Y\). If \(Y\) represents tumor
volume and \(A\) the treatment to be evaluated, \(C\) could be a
biomarker of cancer agressiveness. 1000 patients have been simulated for
all variables in two different settings represented in Figures
\ref{fig:causality-example} and \ref{fig:causality-example2}. In the
first case (Figure \ref{fig:causality-example}), the outcome \(Y\) is
positively correlated to \(C\) (more agressive tumors have bigger
volume) and decreased when \(A=1\) (treatment decreases tumor volume).
\textbf{\(C\) has no influence on \(A\)}. The causal relationships
between the variables and the associated coefficients used to simulate
data are summarized in the directed acyclic graphs (DAG) in Figure
\ref{fig:causality-example}A. The observed relations between variables
in simulated data are shown in Figure \ref{fig:causality-example}B, C
and D. In particular, the \textbf{theoretical influence of \(A\) on
\(Y\) is recovered in the observed data} since
\(E[Y|A=1]-E[Y|A=0]=-5.05\)

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{08-Causal_files/figure-latex/causality-example-1} 

}

\caption[Analysis on observed data without confounder]{\textbf{Analysis on observed data
without confounder.} (A) Directed acyclic graphs with causal relations
between variables and parameters used to simulate data. (B) Influence of
\(C\) on \(A\) in observed simulated data. (C) Same with \(C\) and
\(Y\). (D) Same with \(A\) and \(Y\).}\label{fig:causality-example}
\end{figure}







In the second case (Figure \ref{fig:causality-example2}), \(C\) has an
influence on \(Y\): the more aggressive the tumor, the more likely the
patient is to be treated with the new drug. In this case the
\textbf{simultaneous influence of \(C\) on \(A\) and \(Y\) makes it a
real confounder}. The direct observation of the differences in outcomes
between treated and untreated patients reveals only a small benefit of
the new treatment which does not correspond to the underlying reality
used in these simulations since the theoretical causal influence of
\(A\) on \(Y\) remained the same as in the previous case. The
\textbf{confounding factor prevents the nature of the causal link
between A and Y from being simply inferred}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{08-Causal_files/figure-latex/causality-example2-1} 

}

\caption[Analysis on observed data with confounder]{\textbf{Analysis on observed data with
confounder.} (A) Directed acyclic graphs with causal relations between
variables and parameters used to simulate data. (B) Influence of \(C\)
on \(A\) in observed simulated data. (C) Same with \(C\) and \(Y\). (D)
Same with \(A\) and \(Y\).}\label{fig:causality-example2}
\end{figure}







\section{Causal inference methods to leverage data}\label{causal-basic}

Despite these difficulties, some statistical methods have been developed
to derive estimates with a causal interpretation from observational
data, under precise assumptions. This work will focus on the potential
outcomes framework \citep{rubin1974estimating}. We will first describe
briefly the fundamentals of this framework and different methods that
are part of it.

\subsection{Notations in potential outcomes
framework}\label{notations-in-potential-outcomes-framework}

First of all, the notations used in this and the next chapter are
defined as follows. We will use \(j=1,...,N\) to index the individuals
in the population. \(A_j\) and \(Y_j\) correspond respectively to the
actual treatment received by individual \(j\) and the outcome. In the
most simple case, treatment takes values in \(\mathcal{A}=\{0, 1\}\),
\(1\) denoting the treated patients and \(0\) the control ones. \(Y_j\)
corresponds to the patient's response to treatment. In the case of
cancer it may be a continuous value (\emph{e.g.}, size of tumor), a
binary value (\emph{e.g.}, status or event indicator), or even a
time-to-event (\emph{e.g.}, time to relapse or death). Only the first
two cases will be discussed later. Finally, it is necessary to take into
account the possible presence of confounders influencing both \(A\) and
\(Y\) and denoted \(C_j\) for individual \(j\).

The \textbf{potential outcomes framework is also described as
counterfactual} because it defines variables like \(Y_j(a)\) to denote
the potential outcome of individual \(j\) in case he/she has been
treated by \(A=a\) which may be different from what we observe if
\(A_j\neq a\). This definition can be illustrated at the individual
level, for patient \(j\), where \(A\) is the smoking status (\(1\) for
smokers, \(0 otherwise\)) and \(Y\) is the outcome, \emph{e.g.}, cancer
status at a given date. If patient \(j\) is a smoker then
\(Y_j=Y_j(A=1)\). \(Y_j(A=0)\) would be the outcome if this same patient
had not been a smoker, all other things being equal. This counterfactual
outcome is therefore not observed in the data. These counterfactual
variables make it possible to write the causal estimands. For instance,
in this context, we can easily compute the difference in outcome between
treated patients and control patients (Figure \ref{fig:causality}, left
part): \(E[Y | A=1] - E[Y | A=0].\) However, this difference has no
causal interpretation as it does not offer any guarantees as to the
confounding factor, as an unbalanced distribution of \(C\) can induce
biases. Thus we define another estimate: \(E[Y(1)] - E[Y(0)].\) In this
case, we compare between two ideal cohorts (Figure \ref{fig:causality},
right part), one in which all patients have been treated (possibly
contrary to the fact) and one in which all patients have been left in
the control arm (once again, possibly contrary to the fact).

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/causality} 

}

\caption[Association, causation and their associated cohorts]{\textbf{Association, causation and their
associated cohorts.} Association analyses are based on observed cohorts
and conditional probabilities. Causation analyses are based on
counterfactual variables and cohorts.}\label{fig:causality}
\end{figure}






\subsection{Identification of causal
effects}\label{causal-identification-simple}

The next question is whether it is possible to estimate the
counterfactual variables \(Y(A)\) and under what conditions. The
potential outcomes framework explicits \textbf{assumptions of
consistency, positivity and conditional exchangeability to estimate
these counterfactual variables} and therefore infer causal estimates
from observational (non-randomized) data
\citep{rubin1974estimating, hernan2020causal}.

\emph{Consistency} means that values of treatment under comparison
represent well-defined interventions which themselves correspond to the
treatments in the data:
\(\textrm{if} \: A_j=a, \textrm{then} \: Y_j(a)=Y_j.\)

\emph{Exchangeability} means that treated and control patients are
exchangeable, \emph{i.e.}, if the treated patients had not been treated
they would have had the same outcomes as the controls, and conversely.
Since we usually observe some confounders we define conditional
exchangeability to hold if cohorts are exchangeable for same values of
confounding \(C\). Therefore conditional exchangeability will hold if
there is no unmeasured confounding: \(Y(a) \indep A | C.\)

\emph{Positivity} assumption states that the probability of being
administered a certain version of treatment conditional on \(C\) is
greater than zero: \(\textrm{if} \: P[C=c] \neq 0, P[A=a | C=c] >0.\)
Intuitively, this positivity condition is required to ensure that the
defined counterfactual variables make sense and do not represent
something that cannot exist.

Under these three assumptions, there are different methods and
estimators available to evaluate causal effects from observational data.
Two of them will be described and applied to the same example as above:
the description of the example and the failure of the direct methods are
recalled in Figure \ref{fig:causality-example3}A and B and two causal
inference methods are illustrated in Figure
\ref{fig:causality-example3}C, D and E.

\subsubsection{Standardization or parametric
g-formula}\label{std-classic}

The first method is called standardization or parametric g-formula and
it is the one that will be described in more detail in this chapter and
the following one. It is based on the following equations:

\begin{equation*}
\begin{aligned}
  E[Y(a)] & = \sum_{c} E[Y(a)|c] \times P[c] \\
          & = \sum_{c} E[Y(a)|a,c] \times P[c]
          &&\text{ (exchangeability } Y(a) \perp \!\!\! \perp A | C \text{ )} \\
          & = \sum_{c} E[Y|a,c] \times P[c]
          &&\text{ (consistency)}
\end{aligned}
\end{equation*}

Thus the average effect of treatment on the entire cohort can be written
with standardized means:

\begin{equation}
\begin{aligned}
  E[Y(A=1)] - & E[Y(A=0)] = \\ 
   \sum_{c} \Big( & E[Y | A=1, C=c]-E[Y | A=0, C=c]\Big) \times P[C=c]
\end{aligned}
\end{equation}

Computationally, non-parametric estimation of \(E[Y | A=a, C=c]\) is
usually out of reach. Thus, on real-world dataset, \(E[Y | A=a, C=c]\)
is estimated through \textbf{outcome modeling} and explicit computation
\(P[C=c]\) is replaced by its empirical estimate. The nature of the
statistical model used will be specified in the various applications
presented. In the simple example depicted in Figure
\ref{fig:causality-example3}A, a linear model of the outcome
(\(Y\sim C+A\)) is fitted on observed data. This model is then used to
infer \(E[Y | A=1, C=c]\) and \(E[Y | A=0, C=c]\) for each patient with
covariate \(C=c\) (Figure \ref{fig:causality-example3}C). By averaging
these values over the whole cohort the confounding effect is corrected
and the estimator is much closer to the true value \(-5\) than the naive
estimates (Figure \ref{fig:causality-example3}D and B).

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{08-Causal_files/figure-latex/causality-example3-1} 

}

\caption[Causal inference methods on a simple example]{\textbf{Causal inference methods on a
simple example.} (A) Directed acyclic graphs with causal relations
between variables and parameters used to simulate data. (B) Association
between \(A\) and \(Y\) from observed data. (C) Some simulated
samples/patients with their original variables (\(C\), \(A\) and \(Y\)),
variables from outcome model (\(E[Y|A=0,c)]\), \(E[Y|A=1,c)]\)) and
weights from treatment model (\(W^A\)). (D) Standardized causal effect
of \(A\) on \(Y\) based on and outcome modeling. (E) IPW causal effect
of \(A\) on \(Y\) based on weights derived from treatment modeling; in
this panel weights are taken into account in boxplots and estimations.}\label{fig:causality-example3}
\end{figure}












\subsubsection{Inverse probability weighting (IPW) and propensity
scores}\label{IPW-classic}

Based on the same counterfactual framework, it is possible to build
another class of models, called marginal structural models
\citep{robins2000marginal}, from which we derive estimators different
from the standardized estimators called inverse-probability-of-treatment
weighted (IPW) estimators \citep{cole2008constructing}. IP weighting is
equivalent to creating a \textbf{pseudo-population where the link
between covariates and treatment is cancelled}. In the case of binary
treatment \(A \in {0 ,1}\), weights are defined for each patient as the
inverse of the probability to have received the version of treatment he
or she actually received, knowing his or her covariates:

\[W^A=\dfrac{1}{f[A|C]} \text{ with } f[a|c]=P[A=a|C=c],\]

\(f[a|c]\) being called the propensity score, \emph{i.e.}, the
probability to have received the treatment \(A=a\), given the covariates
\(C=c\). Again, propensity scores will be estimated in later examples
using a parametric model. In this case with a binary treatment \(A\), a
logistic \textbf{treatment model} is used (\(A\sim C\)) to derive the
weights \(W^A\) (Figure \ref{fig:causality-example3}C). Note that
propensity scores are also useful for positivity investigations since
values very close to \(0\) or \(1\) may indicate (quasi-)violations of
positivity. Under the same hypothesis of exchangeability, positivity and
consistency, we can derive the modified Horvitz-Thompson estimator
\citep{horvitz1952generalization, hernan2020causal}:

\begin{equation}
E[Y(a)]=\dfrac{\hat{E}[I(A=a)W^{A}Y]}{\hat{E}[I(A=a)W^A]},
\label{eq:ipweq2}
\end{equation}

\(I\) being the indicator function, such as \(I(A=a)=1\) if \(A=a\) and
\(I(A=a)=0\) if \(A\neq a\). Once again, this method brings estimates
closer to the true causal effect by correcting for the influence of the
confounder (Figure \ref{fig:causality-example3}E).

\subsubsection{Limitations and additional
methods}\label{limitations-and-additional-methods}

These causal inference methods therefore allow to correct some biases
due to observed confounders, at the cost of strong hypotheses that it is
not possible to verify. The plausibility of these hypotheses, and
therefore of the resulting estimates, requires a good knowledge of the
context. Furthermore, the estimates are largely based on statical models
of outcome or treatment. The correct specification of these models is
therefore imperative to ensure unbiased causal estimates. In order to
limit the risks of misspecification, some \textbf{doubly robust}
approaches have also been developed. They require estimating both an
outcome model and a treatment model, but the resulting estimates are
consistent if at least one of the two models is correctly specified. One
of these methodologies among others, called Targeted Maximum Likelihood
Estimation (TMLE), will be mentioned in the next section and is detailed
in appendix \ref{appendix-TMLE}.

In summary, evaluating the effect of a treatment requires isolating its
impact from that of all confounding factors. This can be done in a
randomized clinical trial designed for this purpose. However, there is a
great amount of other data available that may not have been generated in
this rigorous framework. It is nevertheless possible to draw causal
interpretations from them, under certain hypotheses, thus offering
insights for \emph{a posteriori} statistical evaluation of specific
therapeutic strategies.

\BeginKnitrBlock{conclubox}
\subsubsection*{Summary}\label{summary-7}
\addcontentsline{toc}{subsubsection}{Summary}

In the process of evaluating a treatment, it is crucial to monitor the
effect of confounding factors in order to identify the causal effect of
the treatment and not spurious associations. This may involve randomized
clinical trials specifically designed for this evaluation. In the case
of observational data generated outside this framework, it is
nevertheless possible to use causal inference methods to estimate, under
certain conditions, the causal effect of the treatment.
\EndKnitrBlock{conclubox}

\chapter{Causal inference for precision
medicine}\label{chapter-precision}

\epigraph{"Felix qui potuit rerum cognoscere causas."}{Virgil (Georgics, 29 BC)}

\initial{T}hroughout this manuscript, we first described the complexity
of cancer mechanisms, through the diversity of genetic alterations or
non-linear signaling pathways. This complexity naturally led to the
choice of systemic modeling approaches and in particular mechanistic
models whose explicit nature facilitates the study of the effects of new
molecular perturbations such as treatments. The simple study of the
response to BRAF inhibitors has thus required the consideration of many
other genes and pathways.

This final chapter proposes to take the complexity a step further by
considering different treatments. The diversity of patients' molecular
profiles suggests that the best treatment is not necessarily the same
for all patients: this is what is known as precision medicine. This is
already a clinical reality in oncology that could be reinforced in the
future by the emergence of new computational models of cancer, whether
mechanistic or not. \textbf{How then can we assess the relevance of
these models in their ability to guide patient treatment?}

\BeginKnitrBlock{summarybox}
\subsubsection*{Scientific content}\label{scientific-content-6}
\addcontentsline{toc}{subsubsection}{Scientific content}

This chapter presents an extension of the causal inference framework to
quantify the value of precision medicine strategies. This work is
currently under revision and is available as a preprint in
\citet{beal2020causal}. All code is available in the dedicated
\href{https://github.com/JonasBeal/Causal_Precision_Medicine}{GitHub
repository}
\EndKnitrBlock{summarybox}

\section{Precision medicine in
oncology}\label{precision-medicine-in-oncology}

It is first important to understand what is meant by the concept of
precision medicine in the treatment of cancer patients in order to place
subsequent questions in a plausible clinical framework.

\subsection{An illustration with patient-derived
xenografts}\label{main-PDX}

Precision medicine stems from the diversity of treatment responses
observed in different tumors. It has already been observed in previous
chapters about BRAF inhibition that \textbf{different cell lines respond
differently to a particular treatment}. A broader analysis of
pre-clinical data shows that the same is true for the vast majority of
treatments. It would be possible to illustrate this using the same data
from cell lines extended to other drugs. However, because of the more
directly clinical impact of the issues discussed in this chapter, the
analyses presented below will focus on another type of data that is
closer to patient data: patient-derived xenografts (PDX).

A \textbf{PDX is a tumor tissue that has been removed from a patient and
implanted into immunodeficient mice} \citep{hidalgo2014patient}. Unlike
cell lines, which are \emph{in vitro} models, PDXs are \emph{in vivo}
models that allow cancer cells to evolve in a more realistic
microenvironment. In the same way as for cell lines, PDX can be used for
drug screening. The data used in this chapter come from a study by
\citet{gao2015high} which contains several hundred tumors and more than
fifty drugs. Not all drugs have been tested for all tumors; details of
the drugs and types of cancer tested are available in the appendix
\ref{appendix-PDX}. This dataset was generated following the ``one
animal per model per treatment'' approach (\(1 \times 1 \times 1\)), the
principles of which are summarized in Figure \ref{fig:PDX-principles}A.
It should be noted that different drug response metrics are computed in
the source data, two of which will be used in the analyses. The first
one is continuous and called \emph{Best Average Response} in the data,
it is based on the variation of the tumor volume after treatment, the
lower values (and especially negative) corresponding to better
responses. The second one is originally categorical and based on a
modified Response Evaluation Criteria In Solid Tumors (RECIST) criteria.
It was binarized for this study so that the responders have a score of
\(1\) and non-responders \(0\). The details of the definition and
distribution of these metrics are given in appendix \ref{appendix-PDX}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/PDX} 

}

\caption[Principles of PDX screening]{\textbf{Principles of PDX screening.} (A)
Schematic pipeline for PDX screening with tumor biopsies from one
patient divided in several pieces later implanted in similar
immunodeficient mice. Each mouse is then treated with a different drug;
the collection of mice that have received tumor samples from the same
patient but have been treated with different drugs therefore gives
access to several outcomes for the same tumor of origin. (B)
Corresponding counterfactual variables.}\label{fig:PDX-principles}
\end{figure}










In order to illustrate the diversity of response to treatment, the
database is momentarily restricted to the 4 most widely tested drugs and
the 180 tumors (or PDX models) that were evaluated for all four drugs.
The four chosen drugs target different pathways: binimetinib (MAPK
inhibitor), BKM120 (PIK inhibitor), HDM201 (MDM2 inhibitor) and LEE011
(CDK inhibitor). In Figure \ref{fig:PDX-dense}A the 4 treatments show a
high variability of response, with a slight advantage for BKM120 and
binimetinib on average over all tumors. However, \textbf{each of the
treatments was found to be the most effective of the 4 for a significant
proportion of tumors} (Figure \ref{fig:PDX-dense}B), with binimetinib
and BKM being the best treatment for one-third of tumors each and
LEE011/HDM201 sharing the remaining one-third of tumors. It thus appears
that in view of the molecular diversity of tumors and the increasing
number of treatments available, it does not seem advisable, according to
these preclinical data, to treat all tumors with the same gold-standard
treatment. Furthermore, the tissue of origin of the tumors in this
example does not appear to be the main determinant of tumor preference
for certain treatments.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{09-Precision_files/figure-latex/PDX-dense-1} 

}

\caption[Differences in drug response for 4 drugs and 180 tumors: a call for precision medicine]{\textbf{Differences in drug response for 4 drugs
and 180 tumors: a call for precision medicine.} (A) Distribution of
treatment response for the 4 different drugs, each with all 180 tumors.
(B) Number of times each of the 4 drugs is the most effective for a
given tumor, distribution by tissue of origin.}\label{fig:PDX-dense}
\end{figure}







\subsection{Clinical trials and treatment
algorithms}\label{clinical-trials-and-treatment-algorithms}

These remarks can be extended to patients. Thus, precision medicine (PM)
consists in \textbf{assigning the most appropriate treatment to each
patient according to his or her characteristics, usually genomic
alterations for cancer patients}
\citep{friedman2015precision, de2015pragmatic}. At the individual level,
targeted treatments have provided relevant solutions for patients with
specific mutations \citep{abou2003overview}. Putting together these
various treatments, some precision medicine strategies can be defined.
Based on the genomic profile of the patient, the treatment most likely
to be successful is chosen. If the information available is reliable,
\textbf{precision medicine can thus be reduced to a treatment algorithm
that takes as input the molecular characteristics of the patient's tumor
and outputs a recommendation of treatment}. An example of such a
treatment algorithm from the SHIVA clinical trial by
\citet{le2015molecularly} is shown in Figure \ref{fig:SHIVA} where
different treatments are associated with different alterations. In this
case, the treatment algorithm can be considered as an aggregation of the
medical knowledge accumulated on the individual biomarkers.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{fig/SHIVA_algo} 

}

\caption[An example of a precision medicine treatment algorithm: the SHIVA clinical trial]{\textbf{An example of a precision medicine treatment
algorithm: the SHIVA clinical trial.} Specific molecular alterations and
their associated treatments, as proposed in the SHIVA clinical trial
\citep{le2015molecularly}.}\label{fig:SHIVA}
\end{figure}






\subsection{Computational models to assign cancer
treatments}\label{computational-models-to-assign-cancer-treatments}

The treatment algorithm example in Figure \ref{fig:SHIVA} could,
however, be more complex. Indeed, previous chapters have stressed, for
example, that being mutated for the BRAF gene is not the only predictor
of response to an inhibitor of BRAF (here Vemurafenib). The same is true
for most treatments that could benefit from more global and systemic
analyses, taking into account more variables and their interactions.
This complexity would require the use of computational methods.

It is on this point that this chapter links to the previous ones.
\textbf{Some of the cancer models studied throughout this thesis, or
their future developments, could be interpreted as treatment
algorithms}. Indeed, a model capable of predicting the response to a
single treatment does not necessarily allow the inference of precision
medicine strategies. On the other hand, a model capable of predicting a
patient's response to different treatments is also capable of indicating
which one is the best. Such models would then move from systems biology
to systems therapeutics \citep{hansen2013computation}, taking patients'
genomic features as inputs and outputting a treatment recommendation. In
theory, mechanistic models seem to be suitable for this purpose since
their explicit representation of genes and proteins makes it possible to
simulate the effect of different therapeutic interventions. However, the
feasibility of designing and calibrating such a model has yet to be
demonstrated. Other types of models are being studied that could achieve
these goals. For example, some recent approaches propose the use of deep
learning to provide a computational tool for predicting the growth of
cells \citep{ma2018using}, or even the sensitivity of cell lines to
different treatments \citep{manica2019toward}.

In short, if no computational model is sufficiently developed to date to
replace the clinician, the emergence of this type of tool is likely in
the medium term. This raises the question of \textbf{how to assess the
clinical value of the precision medicine strategies (and corresponding
treatment algorithms) derived from these models}. For the sake of
generality, this question will be addressed more broadly in the
following without reference to models as a possible source of the
treatment algorithm: how to evaluate the clinical impact of a precision
medicine strategy and the treatment algorithm? The methods presented
will indeed be the same, whether the algorithm evaluated comes from a
model or from the knowledge of clinicians as in Figure \ref{fig:SHIVA}.
In the spirit of this thesis, the question nevertheless finds its origin
in the first hypothesis related to models.

\section{Emulating clinical trials to evaluate precision medicine
algorithms}\label{emulating-clinical-trials-to-evaluate-precision-medicine-algorithms}

\subsection{Objectives and
applications}\label{objectives-and-applications}

The question then arises of how to quantify the clinical benefit
provided by these treatment algorithms. Some \textbf{precision medicine
clinical trials} have been proposed, demonstrating both the feasibility
of collecting information about mutations \citep{le2015molecularly} or
RNA \citep{rodon2019genomic} in real-time and the clinical benefit that
can be expected from these approaches for some patients
\citep{coyne2017defining}. However, the increasing abundance of genomic
data and biological knowledge make it progressively easier to establish
new algorithms for precision medicine, either directly based on
physician knowledge or provided by computational models
\citep{hansen2013computation}. For practical reasons it is not possible
to propose a real clinical trial for each new precision medicine
algorithm or for any variants, comparing standard of care with new
algorithm-based therapeutic strategies.

Therefore, this work provides a method to assess the clinical impact of
proposed PM treatment algorithm based on already generated data,
\textbf{emulating precision medicine clinical trials and analyzing them
in the causal inference framework} \citep{hernan2016using}. First we
will define the causal estimates of the precision medicine effects
(later referred to as causal estimates) we want to assess, and the
corresponding ideal clinical trials one would like to perform. Next, we
will define the notations and the causal framework we use to infer the
causal effects from observational data with multiple versions of
treatment, based on the previous work by \citet{vanderweele2013causal}.
The main principles of the potential outcome framework having been
introduced in the previous chapter, an extension to the case of
precision medicine will be described, focusing on the multiplicity of
treatment versions, \emph{i.e.}, targeted drugs. Then we will apply
these methods to simulated data in order to investigate the different
biases of the candidate methods. An example scenario will be presented
and a RShiny interactive application has been developed to further
explore other user-defined settings. Finally, the analysis of data from
patient-derived xenografts (PDX) makes it possible both to apply the
methods to pre-clinical situation and to have data approximating the
counterfactual responses, thus enabling further validation of the
proposed estimation methods.

\subsection{Target trials for precision medicine: definition of causal
estimates}\label{causal-estimates}

We first specify the precision medicine effects that are to be
estimated. These effects will finally be estimated based on
observational data through the causal framework and target trial
emulation \citep{hernan2016using}. In this context the notion of
\textbf{target trial refers to the real clinical trial whose estimates
are sought to be reproduced through causal inference}. Thus, if we think
in terms of clinical trials, we are not trying to prove or quantify the
superiority of one treatment over another but rather to evaluate the
clinical utility of a precision medicine strategy assigning treatments
based on genomic features of patients. This is therefore closer to the
well-studied biomarker-based designs for clinical trials
\citep{freidlin2010randomized}. In a way, it is a matter of extending
these unidimensional biomarker-based designs to multidimensional
strategies that allow a choice between quite a number of different
treatments. The potentially large number of treatments thus prompts us
to draw more inspiration from scalable biomarker-strategy designs than
biomarker-stratified designs \citep{freidlin2010randomized}. We can draw
a methodological parallel with some trials like the Tumor
Chemosensitivity Assay Ovarian Cancer study in which a biochemical assay
guides the choice of preferred chemotherapy for patients in a panel of
twelve different treatments \citep{cree2007prospective}. More recently,
some clinical trials have been proposed that include precision medicine
strategies, particularly in oncology
\citep{von2010pilot, le2015molecularly, flaherty2020molecular}.

On the basis of these clinical examples, we propose three different
target trials and their corresponding causal estimates, the clinical
relevance of which may vary according to medical contexts. Each target
trial contains a \textbf{precision-medicine directed arm} in which
patients are treated in accordance with the precision medicine algorithm
recommendations but they are differentiated from each other by
\textbf{alternative control arms} (Figure \ref{fig:target-trials}).
Causal effects will be estimated solely on patients eligible for the
assignment of a personalized treatment, \emph{i.e.}, those for whom the
treatment algorithm is able to recommend a drug.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/target-trials} 

}

\caption[Target trials to estimate causal effect of precision medicine (PM) algorithm versus different controls]{\textbf{Target trials to estimate causal
effect of precision medicine (PM) algorithm versus different controls.}
Patients are first screened according to their eligibility for the
algorithm: based on their genomic characteristics patients are
recommended a specific treatment (eligible) or not (no eligible). Then
eligible patients are randomized and assigned either to PM-directed arm
or to one of the alternative control arms (\(\text{CE}_1\),
\(\text{CE}_2\) or \(\text{CE}_3\))}\label{fig:target-trials}
\end{figure}










\subsubsection{\texorpdfstring{First causal effect (\(\text{CE}_1\)):
comparison with a single
standard}{First causal effect (\textbackslash{}text\{CE\}\_1): comparison with a single standard}}\label{first-causal-effect-textce_1-comparison-with-a-single-standard}

The first possible target trial is to compare the precision medicine arm
with a control arm in which \textbf{all patients have been treated with
the same single treatment}. This could classically be the current
standard of care applied to all patients (\emph{e.g.}, chemotherapy
cancer treatment).

\subsubsection{\texorpdfstring{Second causal effect (\(\text{CE}_2\)):
comparison with physician's assignment of
drugs}{Second causal effect (\textbackslash{}text\{CE\}\_2): comparison with physician's assignment of drugs}}\label{second-causal-effect-textce_2-comparison-with-physicians-assignment-of-drugs}

Then, in order to propose a more comprehensive clinical assessment, we
propose a second causal effect, \textbf{comparing the PM arm with the
current clinical practice}, \emph{i.e.}, the assignment of the same
targeted treatments by physicians in the absence of the algorithm. This
implicitly means comparing two PM strategies: the one derived from the
algorithm and the one that corresponds to current physician's knowledge.
Unlike the former, the latter may not be perfectly deterministic
depending on the heterogeneity of medical knowledge or practices. This
way of defining \(\text{CE}_2\) by focusing on the doctor's assignment
of the same treatments stems from our question of interest: to quantify
the relevance of the algorithm itself. Another possibility would have
been to compare the precision medicine arm with the doctor's treatments,
allowing him to use treatments other than those of the PM arm, such as
the gold-standard one described in \(\text{CE}_1\). But the differences
between the arms could then be biased by the use of treatments with
different overall efficacy, changing the focus of the question. We will
therefore stick to the first definition, which is more focused on the
relevance of the algorithm.

\subsubsection{\texorpdfstring{Third causal effect (\(\text{CE}_3\)):
comparison with random assignment of
drugs}{Third causal effect (\textbackslash{}text\{CE\}\_3): comparison with random assignment of drugs}}\label{third-causal-effect-textce_3-comparison-with-random-assignment-of-drugs}

Finally, we define the \(\text{CE}_3\) effect \textbf{comparing the PM
arm with a control arm using exactly the same pool of treatments
assigned randomly}. In this case, we measure the ability of the PM
algorithm to assign treatments effectively based on genomic features of
patients. This comparison has already been considered in the context of
biomarker-based clinical trials \citep{sargent2005clinical}. Although
this comparison with random assignment is methodologically relevant, it
may not make sense from a clinical point of view if the common clinical
practice already contains strong indications (or contraindications) for
some patient-treatment associations.

\section{Causal inference methods and precision
medicine}\label{causal-inference-methods-and-precision-medicine}

\subsection{A treatment with multiple
versions}\label{a-treatment-with-multiple-versions}

The statement of the potential outcomes framework implicitly implies the
uniqueness of the versions of the treatment
\citep{rubin1980randomization} or at least the treatment variation
irrelevance \citep{vanderweele2009concerning}. \textbf{In the precision
medicine case, the multiplicity of treatment versions is inherent}: a
given treatment status may encompass several drugs since a patient may
be associated with several molecular agents based on his or her genomic
characteristics. \(A\) can be seen as a compound treatment
\citep{hernan2011compound} or a treatment with multiple versions
\citep{vanderweele2013causal}.

Therefore, we define a variable \(K_j\) denoting the version of
treatment administered to individual \(j\). If \(A_j=a\) is the arm to
which the patient is assigned, \(K^a_j\) is the molecule received, the
version of treatment \(A=a\) (\emph{e.g.}, a specific anti-cancer drug)
and \(K^a_j \in \mathcal{K}^a\), the set of versions of treatment
\(A=a\). In our precision medicine problem, \(A=0\) will denote control
patients and \(A=1\) the patients treated with an anti-cancer drug of
the precision medicine pool. \(\mathcal{K}^1=\{k^1_1, ..., k^1_P\}\) is
the set of \(P\) possible targeted treatments for \(A=1\) patients. For
the sake of simplicity we will assume that there is only one treatment
version for \(A=0\) controls, \(\mathcal{K}^0=\{k^0\}\). We also need to
define other counterfactual variables like \(K^a_j(a)\), the
counterfactual version of treatment \(A=a\) if the subject had been
given the treatment level \(a\). Thus, we finally write the
counterfactual outcome as \(Y_j(a,k^a)\) for individual \(j\) when
treatment \(A\) has been set to \(a\), using \(k^a\) as the version of
treatment \(a\), with \(k^a \in \mathcal{K}^a\). Causal relations
between variables \(C\), \(A\), \(K\) and \(Y\) are depicted in the
causal diagram in Figure \ref{fig:DAG-multiple}. It should be noted that
\(A\) has no direct influence on \(Y\), its only effect is entirely
mediated by \(K\), which is the real treatment in the pharmacological
sense.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/DAG-multiple} 

}

\caption[Causal diagram illustrating relations between variables under multiple versions of treatment]{\textbf{Causal diagram illustrating relations
between variables under multiple versions of treatment.} Treatment
\(A\), version of treatment \(K\), outcome \(Y\), and confounding
variables \(C\) and \(W\) are placed in a causal digram, along with
their interpretation in the precision medicine application.}\label{fig:DAG-multiple}
\end{figure}







In this context, we can also define the assignment of a version of
treatment for patients eligible for precision medicine algorithm. It is
important to note that not all patients are necessarily eligible for the
precision medicine strategy. Indeed, the treatment assignment algorithm
relies on targetable alterations to establish its recommendations. In
the absence of these, no recommendation can be offered to the patient.
We denote \(\mathcal{C}^{PM}\) the set of eligible patient profiles and
consequently define the \textbf{drug assignment algorithm as the
function \(r\) which associates to each \(C\) a precision medicine
treatment version \(K\)} such as:

\[\forall j \in [[ 1, N ]], \: \textrm{if} \: C_j \in \mathcal{C}^{PM}, r(C_j) \in \mathcal{K}^{1}\]

\subsection{Causal inference with multiple
versions}\label{causal-inference-with-multiple-versions}

Consequently, the multiplicity of versions prevents direct application
of the framework as described in section
\ref{causal-identification-simple}. The \textbf{theoretical framework
has however been extended to causal inference under multiple versions of
treatment} and some identifiability conditions and properties have been
studied, especially in the seminal article by
\citet{vanderweele2013causal}. One of the first required adaptation to
identify some causal effects is to distinguish between confounders \(C\)
and \(W\) (Figure \ref{fig:DAG-multiple}). \(W\) indicates a collection
of covariates that may be causes of treatment \(A\) or version of
treatment \(K\) but are not direct causes of \(Y\). These covariates are
of special interest for causal effects identification under multiple
versions of treatment. \(C\) indicates all other covariates. In our
precision medicine settings, the genomic features of patients may define
the eligibility to precision medicine and therefore affect \(A\). They
may also be used to define the version of treatment \(K\). And finally
they can influence the response to treatment \(Y\). Thus, the genomic
features of patients are a typical example of type \(C\) confounders.
All causal relationships are summarized in Figure
\ref{fig:DAG-multiple}. Please note that no \(W\) variable is present in
the applications provided later because all the covariates considered in
this situation were likely to influence \(A\), \(K\) and \(Y\) and
therefore belonged rather to the covariates of type \(C\). However all
subsequent formulas and definitions have been derived taking into
account \(W\).

We summarize here some general observations from
\citet{vanderweele2013causal} regarding the extension of the framework
to multiple versions before discussing specific estimates of interest of
our precision medicine settings in the next section. These two sections
will be based exclusively on the method called \textbf{standardization
or parametric g-formula} described in section \ref{std-classic}. The
adaptation of other methods to precision medicine will be discussed more
briefly in section \ref{PM-others}. First of all, the identifiability
conditions have to be adapted. The \emph{consistency} assumption for
instance is extended to \(K\):

\[\textrm{if} \: A_j=a,  \textrm{then} \: K_j^a(a)=K_j^a\].

Then, the \emph{conditional exchangeability} or no-unmeasured
confounding assumptions, may be stated in two different ways, either
without or with versions of treatment:

\begin{equation}
Y(a) \perp \!\!\! \perp A | (C,W)
\label{eq:exchan-noK}
\end{equation}

\begin{equation}
Y(a, k^a) \perp \!\!\! \perp \{A, K\} | C
\label{eq:exchan-K}
\end{equation}

If equation \eqref{eq:exchan-noK} holds, we can derive a new version of
the standardised estimator with multiple versions of treatment
\citep{vanderweele2013causal}:

\begin{equation}
E[Y(a)] = E[Y(a, K^a(a))] = \sum_{c,w} E[Y | A=a, C=c, W=w] \times P[c,w]
\label{eq:overall-treatment-effect}
\end{equation}

Specifically, it should be noted that we need to add \(W\) in the set of
covariates that must be taken into account in standardization, and we
need \emph{positivity} to hold for \(C\) and \(W\), \emph{i.e.},
\(0<P[A=a|C=c,W=w]<1\). Detailed proof of equation
\eqref{eq:overall-treatment-effect} is provided in appendix
\ref{appendix-overall-treatment-effect}. Equation
\eqref{eq:overall-treatment-effect} paves the way to overall treatment
effect assessment since \(E[Y(1, K^1(1))]-E[Y(0, K^0(0))]\) would
estimate the effect of treatment \(A=1\) compared to \(A=0\) with
current versions of treatment.

Conversely, estimating a treatment effect for a given unique version of
treatment \(E[Y(a,k^a)]\) would require to check the exchangeability
with regard to versions \(K\) and therefore to hold equation
\eqref{eq:exchan-K} true \citep{vanderweele2013causal}:

\begin{equation}
E[Y(a, k^a)] = \sum_{c} E[Y | A=a,K^a=k^a, C=c] \times P[c]
\label{eq:version-treatment-effect}
\end{equation}

Similarly, we can define \(G^a\) a random variable for versions of
treatment with conditional distribution \(P[G^a=k^a| C=c]=g^{k^a,c}\)
and assuming the equation \eqref{eq:exchan-K} to be true we can derive the
following formula and its formal proof in appendix
\ref{appendix-distrib-treatment-effect}:

\begin{equation}
E[Y(a, G^a)] = \sum_{c,k^a} E[Y | A=a,K^a=k^a, C=c] \times g^{k^a,c} \times P[c]
\label{eq:distrib-treatment-effect}
\end{equation}

In this case, to allow estimation of the right-hand side of the
equation, positivity will be defined as \(0<P[A=a, K^a=k^a|C]<1\).

\subsection{Application to precision
medicine}\label{identification_causal_PM}

In the context of the potential outcomes framework extended to
treatments with multiple versions, it is therefore possible to apply
equations \eqref{eq:overall-treatment-effect} and
\eqref{eq:distrib-treatment-effect} in order to define and estimate the
precision medicine causal effects previously described in section
\ref{causal-estimates}.

\(A=0\) corresponds to control patients with \(\mathcal{K}^0=\{k^0\}\)
and \(A=1\) to patients treated with a targeted treatment. It is
important to notice that from this point on we systematically restrict
ourselves to patients eligible for the precision medicine algorithm,
\emph{i.e.}, to individuals \(j\) such as \(C_j \in \mathcal{C}^{PM}\).

\subsubsection{\texorpdfstring{\(\text{CE}_1\)
estimation}{\textbackslash{}text\{CE\}\_1 estimation}}\label{textce_1-estimation}

\(\text{CE}_1\) is a comparison between the precision medicine arm and a
single version control arm:

\begin{equation}
\text{CE}_1 = E[Y(1, r(C)] - E[Y(0, k^0)]
\label{eq:CE1}
\end{equation}

In details, \(E[Y(1, r(C)]\) can be derived from equation
\eqref{eq:distrib-treatment-effect} in the case where \(g^{k^a,~c}=1\) if
\(k^a=r(c)\) and \(g^{k^a,~c}=0\) otherwise:

\[E[Y(1, r(C)] = \sum_{c} E[Y | A=1,K^1=r(c), C=c] \times P[c]\]

Then, \(E[Y(0, k^0)]\) and \(E[Y(1, k^1_{ref})]\) can be derived from
equation \eqref{eq:version-treatment-effect}:

\[ E[Y(0, k^0)] = \sum_{c} E[Y | A=0, C=c] \times P[c] \]

Alternatively, if one wants to use as control only one of the treatments
used in the PM arm the previous estimate could be replaced by the
following one:
\[E[Y(1, k^1_{ref})] = \sum_{c} E[Y | A=1,K^1=k^1_{ref}, C=c] \times P[c]\]

It should be noted that \(\text{CE}_1\), like \(\text{CE}_2\) and
\(\text{CE}_3\) presented later, depends on the PM algorithm of interest
\(r\). \(\text{CE}_i\) could therefore also be written
\(\text{CE}_1(r)\).

\subsubsection{\texorpdfstring{\(\text{CE}_2\)
estimation}{\textbackslash{}text\{CE\}\_2 estimation}}\label{textce_2-estimation}

Then, \(\text{CE}_2\) is written using \(K^1(1)\) the PM targeted
treatment that would have been assigned to the patient by the physician
if the patient had been allocated in arm \(A=1\) with PM targeted
treatments:

\begin{equation}
\text{CE}_2 = E[Y(1, r(C)] - E[Y(1, K^1(1))]
\label{eq:CE2}
\end{equation}

\(E[Y(1, K^1(1))]\) is derived from equation
\eqref{eq:overall-treatment-effect}:

\[E[Y(1, K^1(1))] = \sum_{c,w} E[Y | A=1, C=c, W=w] \times P[c,w]\]

\subsubsection{\texorpdfstring{\(\text{CE}_3\)
estimation}{\textbackslash{}text\{CE\}\_3 estimation}}\label{textce_3-estimation}

Defining \(G^1\) as the random distribution of versions of treatment
\(k^1 \in \mathcal{K}^{1}\), \(\text{CE}_3\) expresses as:

\begin{equation}
\text{CE}_3 = E[Y(1, r(C)] - E[Y(1, G^1)] \text{ with } P[G^1=k^1_i]=\dfrac{1}{|\mathcal{K}^1_{PM}|},
\label{eq:CE3}
\end{equation}

\(|.|\) denoting the cardinality of the set. In this formula,
\(E[Y(1, G^1)]\) can be derived from equation
\eqref{eq:distrib-treatment-effect}:

\[E[Y(1, G^1)] = \dfrac{1}{|\mathcal{K}^1_{PM}|} \times \sum_{c,k^1_i} E[Y | A=1, K^1=k^1_i, C=c] \times P[c]\]

\subsection{Alternative estimation methods}\label{PM-others}

For the sake of simplicity and brevity, we only detailed the
standardization in previous sections. However, other popular candidate
methods can be used. Estimators based on the \textbf{inverse probability
weighting (IPW)} and \textbf{targeted maximum likelihood estimation
(TMLE)} will also be computed in the following sections. IPW has the
particularity of not trying to model the outcome but rather the process
of assigning treatments. Its theoretical bases have been described in
section \ref{IPW-classic} and the details of its adaptation to multiple
versions of treatment is provided in appendix \ref{appendix-IPW}.

The TMLE methods are of a different nature \citep{van2011targeted}. They
combine an outcome model and a treatment model in order to obtain a
doubly robust estimate, \emph{i.e.}, an estimate that is robust to a
possible misspecification of either model. Moreover, the estimation is
done in several steps in order to optimize the equilibrium
bias-variance, not for the overall distribution of the data but
specifically for the causal effect of interest. These methods also have
the particularity of being very often used with machine learning
algorithms to fit the outcome or treatment models, instead of the
parametric models classically used in standardization and IPW methods. A
more detailed description of TMLE properties and the choices that have
been made to adapt it to the problem of precision medicine are available
in appendix \ref{appendix-TMLE}.

\subsection{Code}\label{code}

The methods detailed above have been implemented in R and applied to
simulate data and PDX data. The code is provided in the form of R
notebooks (simulated data and PDX data) as well as in the form of an
RShiny interactive application (simulated data only). All of these files
are available in the dedicated
\href{https://github.com/JonasBeal/Causal_Precision_Medicine}{GitHub
repository}.

\section{Application to simulated
data}\label{application-to-simulated-data}

The proposed methods are first tested on simulated data in order to
check the performance of the estimators in finite sample sizes.

\subsection{General settings}\label{general-settings}

\begin{table}

\caption{\label{tab:simparam}\textbf{Intercepts and linear coefficients in the
linear models specified to simulate data}}
\centering
\begin{tabular}[t]{l|r|r|r}
\hline
Response variable & Intercept & Lin. coeff. $Y \sim C_1$ & Lin. coeff. $Y \sim C_2$\\
\hline
$Y(0,k^0)$ & 0 & 0 & 15\\
\hline
$Y(1,k^1_1)$ & -25 & -15 & 10\\
\hline
$Y(1,k^1_2)$ & 0 & 0 & -20\\
\hline
\end{tabular}
\end{table}




Using the R package \emph{lava} based on latent variable models, we
simulate a super-population of 10,000 patients with variables \(C\),
\(A\), \(K\) and \(Y\) as in Figure \ref{fig:DAG-multiple}. We first
define two independent binary variables \(C_1\) and \(C_2\),
representing mutational status of genomic covariates, with a mutation
prevalence of 40\%. By analogy with the PDX data, \(Y\) represents the
evolution of tumor volume and a low value (\emph{a fortiori} negative)
corresponds to a better response. \emph{Y} is therefore defined as a
continuous gaussian variable. For each counterfactual variable of
response \(Y(a, k^a)\), we specify the intercept and the linear
regression coefficients regarding influence of \(C_i\) as described in
Table \ref{tab:simparam}. Lower intercepts correspond to better
responses/more efficient drugs. Similarly, a negative regression
coefficient between \(Y(a, k^a_i)\) and \(C_j\) means that the gene
\(C_j\) improves the response to \(k^a_i\). So all in all, \(k^1_1\) has
the best basal response (lowest intercept). \(C_1\) (resp. \(C_2\))
improves the response to \(k^1_1\) (resp. \(k^1_2\)). The treatment
algorithm of precision medicine is in line with these settings since
patients mutated in \(C_1\) (regardless their \(C_2\) status) are
recommended to take \(k^1_1\) and patients mutated for \(C_2\) only are
recommended to take \(k^1_2\). Patients without mutations are not
eligible for precision medicine and not taken into account in the
computations. Since \(k^1_1\) has the best basal response we assume it
is assigned with greater probability by the physician and implement the
following distribution of observed treatments:

\[P[K=k^1_1]=0.5 \text{ and } P[K=k^1_2]=P[K=k^0]=0.25\]

A super-population of 10,000 patients is then generated. 1,000 cohorts
of 200 patients are sampled without replacement within this
super-population which, with the prevalences defined for the mutations,
corresponds to an effective sample size of about 130 patients eligible
for the PM algortithm. The causal effects \(\text{CE}_1\),
\(\text{CE}_2\) and \(\text{CE}_3\) are computed based on different
methods on the sub-cohort eligible for precision medicine (Figure
\ref{fig:causal-struct}):

\begin{itemize}
\tightlist
\item
  \textbf{True effects}, using all simulated counterfactuals for all
  patients
\item
  \textbf{Naive effects}, using observed outcomes only for both PM and
  control arms
\item
  \textbf{Corrected effects}: using observed outcome and standardized
  estimators (Std), inverse probability weighting (IPW) and targeted
  maximum likelihood estimators (TMLE).
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/causal-struct} 

}

\caption[Generation and use of simulated data for causal inference]{\textbf{Generation and use of simulated data
for causal inference.} A super-population is first generated (left
table) with for each patient all the covariates \(C\), versions of
treatment \(K\) and outcomes \(Y\) (counterfactual and observed).
Multiple smaller subcohorts are then sampled (right table) where causal
effects are estimated either from observed outcomes alone (causal
inference and naive methods) or by using individual counterfactual
outcomes (true effects).}\label{fig:causal-struct}
\end{figure}










\subsection{Simulation results}\label{simulation-results}

First, the distribution of data in the super-population of 10,000
patients can be observed in Figure \ref{fig:simulation-results}A,
illustrating the different relations and differences described above. In
particular, \(Y(1, k^1_1)\) (resp. \(Y(1, k^1_2)\)) is lower for
\(C_1\)-mutated (resp. \(C_2\)-mutated) patients. It can also be seen
that the response to precision medicine (\(Y(1, r(C))\)) differs
according to the groups: patients mutated for \(C_1\) only have the best
response, followed by patients mutated for both \(C_1\) and \(C_2\) and
patients mutated for \(C_2\) only. There is therefore a heterogeneity of
responses to PM which encourages to take into account the groups of
patients and their PM versions. The right side of Figure
\ref{fig:simulation-results}A shows the deterministic assignment of the
recommended PM treatment (\(r(C)\)) to each patient profile and the
unbalanced distribution of observed treatments (\(K\)) with a
predominance of \(k^1_1\).

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{09-Precision_files/figure-latex/simulation-results-1} 

}

\caption[Causal effects of precision medicine strategy with simulated data]{\textbf{Causal effects of precision
medicine strategy with simulated data.} (A) Main variables and relations
in the simulated super-population. From left to right: categories of
patient based on their mutations; responses to \(k^0\), \(k^1_1\),
\(k^1_2\) and precision medicine \(K=r(C)\); repartition of patients
regarding their precision medicine drug and their assigned treatment in
observed data. (B) Distribution and deviation of \(\text{CE}_1\)
estimates based on different methods, deviation scores being computed
based on mean absolute error (MAE). (C) Same for \(\text{CE}_2\). (D)
Same for \(\text{CE}_3\).}\label{fig:simulation-results}
\end{figure}












In the first target trial, true \(\text{CE}_1\) estimates in the sampled
cohorts are distributed around -40 (Figure
\ref{fig:simulation-results}B), confirming the \textbf{superiority of
the PM arm over the control arm} as defined in the simulation
parameters. Not all methods of estimating the causal effect perform
equally well. The so-called naive estimate and the one based on IPW show
a net bias. The over-representation of the most advantaged patients by
PM tends to cause these methods to overestimate the benefit of PM, as
can also be seen in the deviation plots. The same trends are observed
for \(\text{CE}_2\) and \(\text{CE}_3\) (Figure
\ref{fig:simulation-results}C and D) where the differences are even more
drastic. The \textbf{mean absolute error of the naive method is thus
divided by more than 2 when using standardized estimates or the TMLE}.

In order to further dissect the influence of simulation parameters on
estimation performances, a slightly different simulation scenario with
equal probabilities of observed treatments has been studied:
\[P[K=k^0]=P[K=k^1_1]=P[K=k^1_2]=\dfrac{1}{3}\] In this case, the random
and balanced assignment of the observed treatments logically removes the
systematic biases of the naive method by providing them with more
randomized data. However, the corrections made by the proposed methods
of causal inference, and in particular standardization and TMLE, still
\textbf{reduce the variances in the estimates} due to the heterogeneity
of the effects of precision medicine as a function of molecular
profiles. Randomly, some sampled cohorts are indeed found with an
association between \(C\) and observed \(K\), thus generating a
confounding effect that the causal methods partially correct.

The simulated data allow us to imagine an almost infinite number of
scenarios depending on the number of biomarkers taken into account in
the algorithm, the number of different treatments, the dependencies of
their responses or the distribution of treatments observed. In order to
allow easy exploration of these scenarios without having to master the
underlying R code, an \textbf{interactive RShiny application has been
developped}. It can be accessed by locally running the
\href{https://github.com/JonasBeal/Causal_Precision_Medicine/blob/master/Application_Causal_PM.R}{R
source file} or by using the online version embedded in Figure
\ref{fig:Shiny}. Readers with the ability to run the application locally
are encouraged to favor this option because the hosting of the online
application is limited to a maximum amount of time per month. The
application allows certain additional analyses not presented in this
manuscript, in particular the linking of biases observed in the sampled
cohorts according to their composition (prevalence of mutations,
treatments, etc.). It is thus possible to trace the origin of the
biases.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/screenshot-shiny} 

}

\caption[RShiny interactive application to investigate various simulation scenarios of precision medicine evaluation]{\textbf{RShiny interactive application to
investigate various simulation scenarios of precision medicine
evaluation.} It is possible to run the application locally with the
\href{https://github.com/JonasBeal/Causal_Precision_Medicine/blob/master/Application_Causal_PM.R}{source
R file} or online with the version hosted on the
\href{https://jonasbeal.shinyapps.io/application_causal_pm/}{shinyapps.io
server}}\label{fig:Shiny}
\end{figure}









\section{Application to PDX}\label{application-to-pdx}

The method is then applied to public data from patient-derived
xenografts \citep{gao2015high}, described in section \ref{main-PDX} and
appendix \ref{appendix-PDX}. One of the major interests of this type of
data in the context of this chapter is to provide access to treatment
response values otherwise considered as hypothetical (or
counterfactual). It is indeed possible to have the response of the same
tumor (or more precisely of distinct samples from the same tumor) to
different treatments, thus representing \textbf{proxies for
counterfactual variables}, as described in Figure
\ref{fig:PDX-principles}. Availability of these data provides a unique
ground truth to assess the validity of proposed causal estimates in a
pre-clinical context.

Based on the analysis accompanying the published data
\citep{gao2015high}, some biomarkers of treatment response have been
selected and resulted in \textbf{an example of treatment algorithm}:
binimetinib (MEK inhibitor) is recommended to KRAS/BRAF mutated tumors,
and BYL719 (alpha-specific PI3K inhibitor, also known as Alpelisib) to
PIK3CA mutated tumors. PTEN is also included as a covariate because of
its detrimental impact on the response to these two treatments. LEE011
drug (a cell cycle inhibitor also known as Ribociclib) is chosen as the
reference drug treatment (\(k^0\)). Among the sequenced tumors, 88 are
eligible for this precision medicine algorithm (\emph{i.e.}, mutated for
BRAF, KRAS or PIK3CA) and have been tested for all 3 drugs of interest,
thus ensuring the availability of all corresponding responses. The
following analyses will focus exclusively on this sub-cohort for which a
descriptive analysis is provided in Figure \ref{fig:PDX-subcohort}A. As
expected BRAF/KRAS-mutated tumors have a better response to binimetinib
and PIK3CA-mutated tumors have a better response to BYL719 (Figure
\ref{fig:PDX-subcohort}B). In addition, it can be noted that these
biomarkers have deleterious cross-effects.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{09-Precision_files/figure-latex/PDX-subcohort-1} 

}

\caption[Description of the 88 PDX models cohort]{\textbf{Description of the 88 PDX models
cohort.} (A) Tissue of origin and prevalences of the drug biomarkers.
(B) Drug response to precision medicine targeted treatments in the 88
PDX models cohort depending on the mutational status of biomarkers}\label{fig:PDX-subcohort}
\end{figure}






The analysis settings are similar to the ones used for simulated data.
1,000 different cohorts of 70 tumors (out of 88) are sampled without
replacement assuming each time that only the response to one of the
treatments is known for each tumor, reproducing the classical clinical
situation. The \textbf{distribution of the observed treatments was
defined randomly}: \[P[K=k^0]=P[K=k^1_1]=P[K=k^1_2]=\dfrac{1}{3}\] It
should be noted that, contrary to analyses based on simulated data, all
the statistical models used for standardization (outcome model), for the
IPW (treatment model) and for the TMLE are no longer generalized linear
models (GLM) but \textbf{random forests (RF)}. Indeed, it was observed
that the performance of GLM-based methods was lower than that of the
naive method, supporting the importance of relevant model specification
consistent with real data. The RF algorithms then allow to limit
misspecification due to the largely non-linear nature of the data.
Random forests were chosen for their speed and versatility, especially
in view of their ability to handle multinomial classification as well.

The results of estimations are then presented in Figure
\ref{fig:PDX-results}. In the presence of randomly assigned and balanced
observed treatments, none of the methods (including the naive one) has
significant systematic bias. On the other hand, \textbf{more
sophisticated methods, and in particular TMLE, allow to reduce the gap
between estimates and true values}, as visible on the mean absolute
errors in Figure \ref{fig:PDX-results} right column. An additional
analysis using the binary version of outcome \(Y\) is presented in
\citet{beal2020causal} with similar trends and conclusions:
standardized, IPW and TMLE estimates are closer than naive methods to
the true values from PDX. It supports the validity of the extension of
the method to binary outcomes. In the same way as before with the
simulated data, it would be possible to study the impact of non-random
assignment of the observed treatments, which could systematically bias
the results of the naive methods.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{09-Precision_files/figure-latex/PDX-results-1} 

}

\caption[Causal estimates with PDX data]{\textbf{Causal estimates with PDX data.}
Distribution and deviation of \(\text{CE}_1\) (A), \(\text{CE}_2\) (B)
and \(\text{CE}_3\) (C) estimates based on different methods as in
Figure \ref{fig:simulation-results}B.}\label{fig:PDX-results}
\end{figure}






\section{Limitations and perspectives}\label{part3-limits}

In synthesis, this work proposes a conceptual framework for evaluating a
precision medicine algorithm, taking advantage of data already generated
using adapted causal inference tools. However, in a clinical context,
these data were not generated in a purely observational manner. Patients
were cared for and treated by physicians who probably took into account
some of their characteristics. However, the reasoning, formalized or
not, behind the physicians' decisions does not correspond to that which
a new investigator might want to test. In the eyes of this new
investigator, the data can therefore be considered as observational in
that they do not correspond to the randomization he would have liked to
have carried out. The possibility for this new investigator to estimate
the impact of his PM algorithm using the proposed estimators depends,
however, on the consistency, exchangeability and positivity hypotheses.

The hypothesis of consistency has been made more plausible by taking
into account the treatment versions, which makes it possible to explicit
the heterogeneity of the molecules administered. Exchangeability remains
questionable. The simulations and calculations described above underline
the importance of taking into account at least the genomic covariates
used in the processing algorithm. The inclusion of additional covariates
is likely to be necessary in many real-world applications.
\textbf{Positivity, on the other hand, can be violated in a much more
obvious way in certain situations}. Thus, equation
\eqref{eq:distrib-treatment-effect} requires positivity to be extended to
versions of treatment: \(0<P[A=a, K^a=k^a|C]<1\). If the assignment of
the observed treatments was done on a deterministic basis with respect
to the variables used by the treatment algorithm, each patient's
molecular profile will have been treated with a single drug, thus
preventing any subsequent causal inference within the defined framework.
The eventual use, by the boards of physicians in charge of assigning the
observed treatments, of variables different from those used by the
algorithm could then make it possible to verify the positive condition.
But these variables would represent unmeasured confounding factors. It
is therefore \textbf{essential to have an in-depth knowledge of the
rationales at work in the assignment of the observed treatments}.

We developed a user-friendly application that extends the scope of the
simulations and makes possible to study and quantify the impact of
different situations, including possible (quasi-)violations of
positivity or unmeasured confounding. It is thus a \textbf{tool for
empirically framing cases where this causal inference is reasonable or
not}. The analysis of the PDX data provides an illustration and proof of
feasibility for these methods on pre-clinical data, closer to the human
clinical data generally of interest. Beyond feasibility, this
implementation leads to some remarks. Firstly, the improvement of causal
inference methods compared to naive estimation of PM effects is
conditioned in this case to the use of flexible and non-linear learning
algorithms. This underlines the \textbf{importance of a proper
specification of the outcome and treatment models} whose imperfection,
especially when trained on small samples, could explain the modesty of
the results compared to the simulated data. The particular nature of the
PDX data design used should also be kept in mind: each tumor is tested
only once for each drug, which may lead to greater variability of
results due to tumor heterogeneity \citep{gao2015high}. Some studies,
with smaller numbers of tumors and treatments, propose to form groups of
several mice for each treatment-drug combination
\citep{hidalgo2014patient}. The use of these mean effects could
contribute to more accurate data. In spite of these limitations, which
may diminish their ability to provide values with counterfactual
interpretation, \textbf{PDX data are thus a dataset of interest for
studying and validating methods of causal inferences about treatment
response}. It can also be noted that the very nature of these data, due
to the multiplicity of drugs tested for each tumor, can provide a
framework in which the constraints of positivity are singularly
alleviated. Even if all drugs were not tested on all patients,
considering each tumor-drug combination as a different unit increases
the coverage of the data. It is then necessary to take into account the
clustered nature of the data, each tumor being present several times.

Finally, beyond the pre-clinical data presented here, the theoretical
framework developed in this chapter should be more directly applicable
to data from clinical trials if these data do not violate the
requirements of positivity. If it is necessary to consider several
trials, the heterogeneity of practices must be taken into account. The
use of different drug lists from one trial to another or from one
medical centre to another could also provide an example of confounding
factor \(W\), included in the theoretical framework presented here but
not used in applications.

\BeginKnitrBlock{conclubox}
\subsubsection*{Summary}\label{summary-8}
\addcontentsline{toc}{subsubsection}{Summary}

The emergence of targeted treatments coupled with a better understanding
of patients' molecular profiles has fostered the development of
precision medicine for cancer: the most appropriate treatment is defined
according to the patient's genetic alterations. Because they can in some
cases allow the simulation and comparison of different treatments for a
given molecular profile, mechanistic models can thus be considered as an
example of those treatment assignment algorithms that represent
precision medicine. It is therefore desirable to evaluate the clinical
benefits of these algorithms using clinical trials or causal inference.
In the second case explored here, the theoretical framework is extended
to allow the existence of different drugs, or versions of treatment, in
the treated patients. Different statistical estimators are defined to
emulate several clinical trials and estimate the corresponding causal
effect of precision medicine algorithms. These methods are tested on
simulated data according to different scenarios and an interactive
application is proposed to explore others. A second application is
presented on preclinical data from PDX, which provides for each tumor
the experimental response to different treatments. Access to these data,
otherwise considered as counterfactual, allows to validate the capacity
of the developed methods to reduce bias in the estimation of effects.
\EndKnitrBlock{conclubox}

\chapter*{Conclusion}\label{conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

\section*{One path among others}\label{one-path-among-others}
\addcontentsline{toc}{section}{One path among others}

The aim of this thesis was to trace \textbf{a path to link the
biological knowledge of cancer to the clinical impact through
mechanistic models}. At each of the stages that separate the creation of
a mechanistic model of cancer from its possible clinical application,
choices were made that determined the obstacles encountered and the
final shape of this work. Before elaborating on the subject, it is worth
summarizing one last time the thread followed during this thesis.

Among the many possible orientations, it was chosen to take the opposite
side of the data-intensive machine learning methods. The main approach
proposed uses a qualitative logical formalism and \textbf{integrates the
data by interpreting them} rather than by optimizing the parameters with
respect to a particular objective. As a result, the resulting
personalized mechanistic models have proven to be \textbf{more of an
interpretive than a predictive tool}. Their versatility and low data
requirements nevertheless allow them to be applied to a wide range of
questions, particularly concerning the response to treatments that their
mechanistic nature facilitates. This seemingly limitless versatility
can, however, prove to be a trap because, while all kinds of
applications are theoretically possible, the need to rely on detailed
biological knowledge and appropriate data limits its scope.

In the case of mechanistic molecular signaling models, this interpretive
nature of the models is confirmed by statistical analyses. The main
value of these models is to provide an understandable framework for
extracting relevant biological information in the context of current
biological knowledge. The \textbf{ability of these models to detect
emerging non-linear information is also proven, but is rarer and of
relatively smaller magnitude}. Given the influx of biological knowledge
and data, computational models of cancer, with various formalisms, are
nevertheless multiplying, particularly with medical aims. In the context
of cancer, their use to recommend personalised treatment for each
patient is a possible horizon. The evaluation of these models could then
become increasingly acute. This thesis proposes the \textbf{adaptation
of causal inference methods in order to simulate their evaluation in
clinical trials} and to come as close as possible to medical evaluation
standards.

\section*{In other directions}\label{in-other-directions}
\addcontentsline{toc}{section}{In other directions}

Along this path, many forks are conceivable, as well as the further
pursuit of some of the paths that have been explored. Without going back
over the technical limits and perspectives of parts II and III, already
mentioned in sections \ref{part2-limits} and \ref{part3-limits}
respectively, it is possible to paint a broader picture of some of the
obstacles and alternatives.

If we question the different choices and presuppositions of this thesis
we must first mention that \textbf{mechanistic models of cancer are not
limited to the modeling of molecular networks}. Other fields have been
explored, and some applications are already more mature and closer to
direct use in clinical oncology. This is essentially the case for
pharmacological approaches focusing on the pharmacokinetics or
pharmacodynamics of certain compounds and the modeling of tumor size
\citep{benzekry2020artificial}. The two approaches answer different
questions from those raised in this thesis, but they are nevertheless
closely linked and we must consider the enrichments they can bring. For
example, pharmacokinetic and pharmacodynamic modeling represents a very
clear blind spot of the approaches presented in chapter
\ref{logical-drug-chap} where all BRAF inhibitors are modeled in an
identical and crude way: quantitative parameters of BRAF inhibition are
not translated. All these aspects constitute the core of mechanistic
pharmacokinetic/pharmacodynamic models that are biochemically more
accurate and fine-grained. On the other hand, tumor size models present
a more macroscopic approach that is easier to relate to patient-level
clinical interpretations.

All these considerations thus incite us not to consider a single-layer
model and to try to make it cover the entire path, from the molecule to
the patient, as proposed in this thesis. Rather than building huge
mechanistic models covering all scales, it seems appropriate to work on
\textbf{linking different models in a modular multi-scale approach}
making it easier to cooperate between fields of modeling that often
require different knowledge. As an example, it is possible to combine
MaBoSS and its molecular network models with PhysiCell, a 3D
multicellular system simulator \citep{ghaffarizadeh2018physicell}. The
PhysiBoss framework schematically proposes to encapsulate a logical
model in each cell to simulate its internal functions and then let
PhysiCell model the three-dimensional interactions between multiple
cells of potentially different types \citep{letort2019physiboss}.
Probably the most promising prospects for mechanistic models lie in
synergies of this kind, which, however, require broader collaborations
and longer design times to truly enable the different modeling scales to
communicate. The idea of proposing multi-scale models encapsulated in a
hierarchical way to ensure the modeling from the genetic scale to the
phenotype is also found in intermediate approaches between mechanistic
and deep learning models that force neural networks to follow this
architecture in order to ease \emph{post hoc} interpretation
\citep{ma2018using}.

The previous mention of quantitative pharmacology models also underlines
the \textbf{intrinsic limitations of qualitative mechanistic models},
\emph{a fortiori} for clinical applications. Thus the outputs of the
personalized logical models can only be interpreted in relation to each
other; it is difficult to give each one an absolute interpretation.
Without proper calibration, what does a \emph{Proliferation} score of
0.2 or 0.8 mean? On first reading, they have only a relative
significance that precludes direct clinical interpretation. However, it
seems difficult to make these logical models much more quantitative. The
very nature of formalism limits the precise translation of biological
mechanisms retrieved from literature. Likewise, the partial integration
of some omics data only in logical models, presented in chapter
\ref{personalization-chap}, required a specific methodology to allow the
data to be adapted to the discrete and qualitative formalism. Obtaining
truly quantitative and well-calibrated mechanistic models has until now
always required intrinsically more quantitative formalisms such as
ordinary differential equations \citep{frohlich2018efficient}, or
Bayesian frameworks allowing a more flexible representation of entities
and their relationships \citep{jastrzebski2018integrative}. In these two
cases, as in most others, the \textbf{quantitative quality of the
predictions relies on model training and parameter fitting}, contrary to
the approach proposed in this thesis.

These methods therefore depend very heavily on the available data to
train their models. One of their limitations is to use data from cell
line screening. Schematically it is necessary to have the response to
different treatments for each line in order to obtain personalized
models or interpretations. The abundance of this type of screening data
makes them the preferred methods for these applications where they
greatly outperform qualitative models without learning. On the other
hand, in a more prospective way, it is interesting to ask the question
of the availability of patient data with the view to one day apply these
models as close as possible to the clinic. Today it is inconceivable to
obtain such rich data for patients. This would imply being able to
develop cell lines for each patient, then screen all of them for drugs
and use the results. This is a prospect whose feasibility is being
studied \citep{lee2018pharmacogenomic}, but it is not a routine clinical
procedure today. If it were possible, the models would probably be
replaced by the experiments themselves. More realistically, it is hoped
that once the model has been sufficiently trained, the data required to
make predictions for a new patient will be more modest. However, this
new patient still needs to remain within the training data field.
Conversely, logical models personalized without learning are based on
other assumptions, mainly related to the correct mathematical
translation of biological knowledge and interpreted omics data.
Continued growth in the quantity and quality of data is likely to give
an overall advantage in the medium term to the above mentioned
quantitative mechanistic methods, more data-intensive but more accurate.
\textbf{Qualitative models such as those presented in chapters
\ref{personalization-chap} and \ref{logical-drug-chap} may nonetheless
remain a complementary approach, used beforehand or in more data-poor
cases that do not allow for learning}, as discussed when comparing the
results with machine learning approaches in section \ref{ML-comp}.

In such a world of abundant data, is it only then relevant to continue
to propose mechanistic models instead of biologically agnostic machine
learning models? The latter have indeed become much more frequent with a
particular emphasis in recent years on deep learning approaches
\citep{angermueller2016deep, eraslan2019deep}. The comparison between
quantitative mechanistic approaches and machine learning approaches then
uses terms similar to the previous comparison between qualitative and
quantitative mechanistic approaches. Machine learning approaches can be
even more flexible because their degrees of freedom are not constrained
by an \emph{a priori} biological structure, but they consequently
require larger amounts of data. The loss of any internal mechanistic
structure can however hamper the interpretability of the results,
although many methods have been developed to facilitate this
interpretation of machine learning models in biology
\citep{azuaje2019artificial, manica2019toward}. The loss of the
mechanistic nature in most machine learning approaches also makes it
impossible to test perturbations or hypotheses that are clearly outside
the scope of training data. If one can imagine the predominance of
well-calibrated machine learning models for routine decisions,
\textbf{more exploratory and prospective investigations could however
favour mechanistic models more capable of extrapolation}. Moreover,
while the question of the acceptability of computational decision
support models in oncology arises \citep{vollmer2020machine}, the
explicit representation of the internal mechanisms may present a
reassuring aspect. It is simpler for both the patient and the physician
to imagine the functioning of a model that replicates human bodies or
cells than a more exclusively mathematical black box. However, this
argument only seems valid in the case of very similar performances.

Finally, most of the remaining questions that arise concern all
computational models of cancer in the same way, whatever their nature.
\citet{wiens2019no} thus propose a \textbf{roadmap for the responsible
use of the models in the clinic} that targets a number of challenges.
One of them is to know how computational models can be seamlessly
integrated into medical practice while leaving the various stakeholders,
from medical experts to patients, in the loop. Physicians, for example,
need to be informed in a relevant way about the nature and limitations
of these models, which can facilitate the decision but whose technical
details may escape them. The presentation of these models to them is
therefore particularly crucial in order to ensure their proper use, as
\citet{sendak2020presenting} point out by proposing standardized methods
of presentation and communication. Numerous ethical or data privacy
issues also arise when precise genetic information about patients needs
to be processed.

One of the challenges mentioned by \citet{wiens2019no} which resonates
particularly with this thesis is that of the rigorous evaluation of the
clinical contribution of these models, mechanistic or not. The choice of
metrics is therefore very important and must be made to optimize
clinical information. For example, while the discrimination capability
of models is frequently measured, the calibration performance of risk
prediction models is also crucial to inform clinical practice
\citep{van2019calibration}. In general, although computational cancer
models are changing the cancer research landscape, \textbf{proven
standards from epidemiology or clinical statistics for evaluation should
be maintained} to allow comparison and quantification of improvements
beyond scientific trends \citep{christodoulou2019systematic}. These
evaluations require real clinical expertise to measure the extent to
which a gain in algorithmic performance can be translated into clinical
improvement depending on the patients concerned and the resources
mobilized. Concerning the mechanistic models in particular, for example,
their capacity to extrapolate and possibly reason about new treatments
for which data are missing or few have been underlined several times.
However, the clinical evaluation of their impact always requires data at
the end of the day. The methods of causal inference applied in chapter
\ref{chapter-precision} make it possible to correct the effects and
bring them closer to the standards of clinical trials, but they can only
rearrange clinical strategies on the basis of existing data.
\textbf{Mechanistic models are therefore there to guide or enlighten
experiments, but are not intended to replace them}.

To conclude in a word, cancer models still have a bright future ahead of
them. Mechanistic models will continue to be attractive because of their
ability not only to predict but, more importantly, to explain. However,
the transparency of their mechanisms should not prevent them from being
rigorously evaluated statistically. It is not enough for them to
explain, they must also be well understood.

\appendix \addcontentsline{toc}{chapter}{\appendixname}


\chapter{About datasets}\label{appendix-datasets}

\section{Cell lines}\label{appendix-cl}

Several analyses in previous chapters are based on data derived from
cell lines. Among the different databases, the ones used in the thesis
are briefly described below. Please refer to corresponding references
for additional details.

\subsection{Omics profiles}\label{omics-profiles}

The omics profiles of cancer cell lines have been downloaded from Cell
Model Passports \citep{van2019cell} containing genotypic and phenotypic
information about more than 1,000 cell lines. Among the available data
used in this thesis are the exome sequencing, copy number variations and
RNA-sequencing.

\subsection{Drug screenings}\label{appendix-GDSC}

Information about response to treatments is retrieved from Genomics of
Drug Sensitivity in Cancer Database (GDSC, \citet{yang2012genomics}). In
order to allow detailed analyses at the level of cancer types, we will
restrict ourselves here to tissues represented by at least 20 cell lines
and highlighted in dark grey in Figure \ref{fig:GDSC}A. Most of the 663
cell lines in this subcohort have a complete profile with all omics data
(mutations, CNA and expression) and drug responses. However, not all
cell lines have necessarily been tested for all drugs.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{10-Appendix_files/figure-latex/GDSC-1} 

}

\caption[Distribution of cancer types and data types in GDSC-associated dataset]{\textbf{Distribution of cancer types and data types
in GDSC-associated dataset.} (A) Distribution of cell lines per cancer
types, highlighting the ones selected in this thesis with more than 20
cell lines. (B) Availibility of data for the 663 selected cell lines in
17 different cancer types.}\label{fig:GDSC}
\end{figure}







The cell lines are treated with increasing concentration of drugs and
the viability of the cell line relative to untreated control is
measured. The dose-response relative viability curve is fitted and then
used to compute the half maximal inhibitory concentration (\(IC_{50}\))
and the area under the dose-response curve (AUC)
\citep{vis2016multilevel}, both being represented in Figure
\ref{fig:AUC}. Since the \(IC_{50}\) values are often extrapolated
outside the concentration range actually tested, we will focus on the
AUC metric for all validation with drug screening data. AUC is a value
between 0 and 1: values close to 1 mean that the relative viability has
not been decreased, and lower values correspond to increased sensitivity
to inhibitions. In cases where the ranges of concentrations tested for
different drugs vary, comparison of their AUC values does not have a
simple and straightforward interpretation.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{fig/AUC} 

}

\caption[Drug screening metrics in cell lines]{\textbf{Drug screening metrics in cell lines.} Based
on a tested drug concentration range, \(IC_{50}\) and area under the
dose-response curve (AUC) can be computed. For a given drug, red AUC
corresponds to a more sensitive cell line than blue AUC.}\label{fig:AUC}
\end{figure}






\subsection{CRISPR-Cas9 screening}\label{appendix-CRISPR}

On top the previous drug response characterization, some CRISPR-Cas9
screenings have been performed on cancer cell lines. Very basically,
this involves using single-guide RNAs (sgRNAs) to direct the targeted
inhibition of certain genes. Conceptually, screening is not very
different from drug screening since it allows the sensitivity of cell
lines to the inhibition of certain targets to be studied. However, this
technology makes it possible to target many more different genes since
it is based on RNA guide synthesis and not on the existence of drugs
with an affinity for the target of interest. Schematically, sreening is
therefore broader (thousands of genes), less biased (any gene can be
targeted \emph{a priori}) and more precise (much lower off-target
effect).

Among the various databases available, the ones used in this thesis have
been downloaded from Cell Model Passports and come from Sanger Institute
\citep{behan2019prioritization} and Broad Institute
\citep{meyers2017computational}. Both databases present CRISPR
inhibition results for thousands of genes for a few hundred cell lines
among those presented in the previous section. The Sanger dataset for
instance includes 324 cell lines, and 238 in common with the subcohort
previously described in the previous section and in Figure
\ref{fig:GDSC}.

Among the different metrics, the examples presented in this thesis will
focus on scaled Bayesian factors to assess the effect of CRISPR
targeting of genes. These scores are computed based on the fold change
distribution of sgRNA \citep{hart2016bagel}. The highest values indicate
that the targeted gene is essential to the cell fitness.

\section{Patient-derived xenografts}\label{appendix-PDX}

Another type of data exists, halfway between cell lines and patients,
and that is patient-derived xenografts (PDX). Each patient tumour is
divided into pieces later implanted in several immunodeficient cloned
mice treated with different drugs, thus providing access to
sensitivities to several different drugs for each tumour.

\subsection{\texorpdfstring{Overview of PDX data from
\citet{gao2015high}}{Overview of PDX data from @gao2015high}}\label{overview-of-pdx-data-from-gao2015high}

The PDX dataset used in this thesis is the one published by
\citet{gao2015high}. The original dataset contains 281 different tumours
of origin (sometimes called PDX models, in the sense of a biological
model) and 63 tested drugs, not all drugs having been tested for all
tumours and some drugs have been tested with tissue-specific patterns
(Figure \ref{fig:PDX-appendix}). 192 of these tumours have also been
characterized for their mutations, copy-number alterations and mRNA.
More detailed analyses of this dataset are available in the dedicated
\href{https://github.com/JonasBeal/Causal_Precision_Medicine}{Github
repository}, in the file \emph{Analysis\_PDX.Rmd} and its corresponding
HTML report.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{10-Appendix_files/figure-latex/PDX-appendix-1} 

}

\caption[Comprehensive overview of tumours and drugs screened in PDX dataset from @gao2015high]{\textbf{Comprehensive overview of tumours and
drugs screened in PDX dataset from \citet{gao2015high}.}}\label{fig:PDX-appendix}
\end{figure}




\subsection{Drug response metrics}\label{drug-response-metrics}

\subsubsection{A continuous outcome}\label{a-continuous-outcome}

The first drug response metric used in this article is called \emph{Best
Average Response}. For each combination tumour/drug, the response is
determined by comparing tumor volume change at time \(t\), \(V_t\) to
tumor volume at time \(t_0\), \(V_{t_0}\). Several scores are computed:

\[\text{Tumour Volume Change (\%)} = \Delta Vol_t = 100\% \times \dfrac{V_t-V_{t_0}}{V_t}\]

\[\text{Best Response} = min(\Delta Vol_t), t>10d\]

\[\text{Average Response}_t = mean(\Delta Vol_i, 0 \leq i\leq t)\]

\[\text{Best Average Response} = min(\text{Average Response}_t), t>10d\]

We will mainly focus on \emph{Best Average Response}. This metric
``captures a combination of speed, strength and durability of response
into a single value'' \citep{gao2015high}. Qualitatively, lower values
correspond to more efficient drugs.

\subsubsection{A binary outcome}\label{a-binary-outcome}

Thresholds of \emph{Best Response} and \emph{Best Average Response} are
also defined, inspired by RECIST criteria \citep{therasse2000new}, in
order to classify response to treatment into 4 categories: Complete
Response (CR), Partial Response (PR, Stable Disease (SD) and Progressive
Disease (PD). We designed a binary response status by combining the
response categories (CR, PR and SD) into a single ``responder''"
category (1), opposed to the ``non-responders'' progressive diseases
(0).

\section{Patients}\label{appendix-datasets-patients}

\subsection{METABRIC}\label{metabric}

METABRIC dataset is large breast cancer dataset with more than 2'000
patients \citep{pereira2016somatic}. Mutations, CNA, expression
(transcriptomics micro-array) and clinical data are available for a
majority of patients (Figure \ref{fig:METABRIC}A), with 1'904 patients
for whom all the data is available. One of the particular features of
these data is to propose a very long clinical follow-up, over more than
10 years (Figure \ref{fig:METABRIC}B).

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{10-Appendix_files/figure-latex/METABRIC-1} 

}

\caption[Available omics and survival in METABRIC Breast Cancer dataset]{\textbf{Available omics and survival in METABRIC
Breast Cancer dataset}. (A) Number of patients for each omics type and
their combinations, depicted as a Venn diagram. (B) Overall survival
probability for all patients with clinical follow-up, stratified per
breast cancer PAM50 subtype; administrative censoring at 180 months.}\label{fig:METABRIC}
\end{figure}







\subsection{TCGA: Breast cancer}\label{tcga-breast-cancer}

Another reference database for breast cancer is the one from the TCGA
consortium \citep{cancer2012comprehensive}. The cohort is smaller than
METABRIC and its clinical follow-up is more limited. In contrast, the
omics data are more comprehensive and include RNA sequencing and
relative quantification of proteins with RPPA technology (Figure
\ref{fig:TCGA-bp}A).

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{10-Appendix_files/figure-latex/TCGA-bp-1} 

}

\caption[Available omics for TCGA Breast and Prostate cancer]{\textbf{Available omics for TCGA Breast and
Prostate cancer}. (A) Number of patients for each omics type and their
combinations, depicted as a Venn diagram, in TCGA BRCA (Breast Invasive
Carcinoma) study. (B) Same for the TCGA PRAD (Prostate Adenocarcinoma)
study.}\label{fig:TCGA-bp}
\end{figure}







\subsection{TCGA: Prostate cancer}\label{appendix-prostate}

Similarly, for prostate cancer, reference can be made to data from the
TCGA study \citep{abeshouse2015molecular}, which has the same type of
data but for a smaller number of patients than the breast cancer (Figure
\ref{fig:TCGA-bp}B).

\chapter{About logical models}\label{about-logical-models}

Several logical models of cancer are used in this thesis and some
additional descriptive elements about them are given below.

\section{Generic logical model of cancer pathways}\label{appendix-fumia}

For this thesis, a published Boolean model from \citep{fumia2013boolean}
has first been used to illustrate our PROFILE methodology. This
regulatory network summarizes several key players and pathways involved
in cancer mechanisms such as RTKs, PI3K/AKT, WNT/\(\beta\)-catenin,
TGF-\(\beta\)/Smads, Rb, HIF-1, p53 and ATM/ATR. An input node
\emph{Acidosis} has been added, along with an output node
\emph{Proliferation} used as a readout for the activity of any of the
cyclins (\emph{CyclinA}, \emph{CyclinB}, \emph{CyclinD} and
\emph{CyclinE}). This slightly extended model contains 98 nodes and 254
edges and its inputs are \emph{Acidosis}, \emph{Nutrients}, \emph{Growth
Factors} (GFs), \emph{Hypoxia}, \emph{TNFalpha}, \emph{ROS},
\emph{PTEN}, \emph{p14ARF}, \emph{GLI}, \emph{FOXO}, \emph{APC} and
\emph{MAX}. Its outputs are \emph{Proliferation}, \emph{Apoptosis},
\emph{DNA\_repair}, \emph{DNA\_damage}, \emph{VEGF},
\emph{Lactic\_acid}, \emph{GSH}, \emph{GLUT1} and \emph{COX412}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/Fumia2013} 

}

\caption[GINsim representation of the logical model described in Fumia et al. (2013)]{\textbf{GINsim representation of the logical model
described in \citet{fumia2013boolean}.}}\label{fig:Fumia}
\end{figure}




\section{Extended logical model of cancer
pathways}\label{appendix-verlingue}

Another logical model of similar size and scope was also used, primarily
for the study of treatment responses. This model was built by Loïc
Verlingue, a medical oncologist and member of the laboratory and
preliminary versions of the model are described in
\citet{verlingue2016comprehensive} and \citet{verlingue2016silico}. One
of the interests of this model is that it has been designed with a more
clinical perspective, notably centred on the response to MTOR
inhibitors. In addition, it presents more biological read-outs used for
interpretation, and we will use mainly \emph{Proliferation} (also called
\emph{G1\_S} in the model files to designate the associated stage of the
cell cycle), \emph{Apoptosis} and \emph{Quiescence} in particular. In
addition, being able to discuss and collaborate directly with the model
autor has helped to avoid potential errors in use.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/Verlingue} 

}

\caption[GINsim representation of the "Verlingue" logical model described in Verlingue et al.]{\textbf{GINsim representation of the `Verlingue'
logical model described in \citet{verlingue2016silico}.}}\label{fig:Verlingue}
\end{figure}




\section{Logical model of BRAF pathways in melanoma and colorectal
cancer}\label{appendix-pantolini}

Here are some details about the regulations represented in Figure
\ref{fig:BRAF-model}. The MAPK pathway encompasses three families of
protein kinases: RAF, MEK, ERK. If RAF is separated into two isoforms,
CRAF and BRAF, the other two families MEK and ERK are represented by a
single node. When BRAF is inhibited, ERK can still be activated through
CRAF, and BRAF binds to and phosphorylates MEK1 and MEK2 more
efficiently than CRAF \citep{wellbrock2004raf}, especially in his
V600E/K mutated form. When PI3K/AKT pathway is activated, through the
presence of the HGF (Hepatocyte Growth Factors), EGF (Epidermal Growth
Factors) and FGF (Fibroblast Growth Factors) ligands, it leads to a
proliferative phenotype. The activation of this pathway results in the
activation of PDPK1 and mTOR, both able to phosphorylate p70 (RPS6KB1)
which then promotes cell proliferation and growth
\citep{uniprot2019uniprot}. There has been some evidence of negative
regulations of these two pathways carried out by ERK itself
\citep{lake2016negative}: phosphorylated ERK is able to prevent the
SOS-GRB2 complex formation through the activation of SPRY
\citep{edwin2009intermolecular}, inhibit the EGF-dependent GAB1/PI3K
association \citep{lehr2004identification} and down-regulate EGFR signal
through phosphorylation \citep{lake2016negative}. The model also
accounts for a negative regulation of proliferation through a pathway
involving p53 activation in response to DNA damage (represented by ATM);
p53 hinders proliferation through the activation of both PTEN, a PI3K
inhibitor, and p21 (CDKN1A) responsible for cell cycle arrest.

We hypothesize that a single network is able to discriminate between
melanoma and CRC cells. These differences may come from different
sources. One of them is linked to the negative feedback loop from ERK to
EGFR. As mentioned previously, this feedback leads to one important
difference in response to treatment between melanoma and CRC:
\(BRAF^{(V600E)}\) inhibition causes a rapid feedback activation of
EGFR, which supports continued proliferation. This feedback is observed
only in colorectal since melanoma cells express low levels of EGFR and
are therefore not subject to this reactivation
\citep{prahallad2012unresponsiveness}. Moreover, phosphorylation of
SOX10 by ERK inhibits its transcription activity towards multiple target
genes by interfering with the sumoylation of SOX10 at K55, which is
essential for its transcriptional activity \citep{han2018erk}. The
absence of ERK releases the activity of SOX10, which is necessary and
sufficient for FOXD3 induction. FOXD3 is then able to directly activate
the expression of ERBB3 at the transcriptional level, enhancing the
responsiveness of melanoma cells to NRG1 (the ligand for ERBB3), and
thus leading to the reactivation of both MAPK and PI3K/AKT pathways
\citep{han2018erk}. Furthermore, it has been shown that in colorectal
cells, FOXD3 inhibits EGFR signal \emph{in vitro} \citep{li2017foxd3}.
Interestingly, SOX10 is highly expressed in melanoma cell lines when
compared to other cancer cells. In the model, we define SOX10 as an
input because of the lack of information about the regulatory mechanisms
controlling its activity. The different expression levels of SOX10 have
been reported to play an important role in melanoma (high expression)
and colorectal (low expression) cell lines.

Besides a list of formalized biological assertions, retrieved from
literature, has been used during the model building to ensure the
consitency of the model with some qualitative behaviours. These
assertions, listed below, are all verified when the logical model is
simulated (details are available on the corresponding
\href{https://github.com/sysbio-curie/MaBoSS_test}{GitHub repository}):

\begin{itemize}
\tightlist
\item
  BRAF inhibition causes a feedback activation of EGFR in colorectal
  cancer and not in melanoma \citep{prahallad2012unresponsiveness}
\item
  MEK inhibition stops ERK signal but activates the PI3K/Akt pathway and
  increases the activity of ERBB3
  \citep{gopal2010basal, lake2016negative}
\item
  HGF signal leads to the reactivation of the MAPK and PI3K/AKT
  pathways, and resistance to BRAF inhibition \citep{wroblewski2013bh3}
\item
  BRAF inhibition in melanoma activates the SOX10/FOXD3/ERBB3 axis,
  which mediates resistance through the activation of the PI3K/AKT
  pathway \citep{han2018erk}
\item
  Overexpression/mutation of CRAF results in constitutive activation of
  ERK and MEK also in the presence of a BRAF inhibitor
  {[}\citet{manzano2016resistant}; johannessen2010cot{]}
\item
  Early resistance to BRAF inhibition may be observed in case of PTEN
  loss, or mutations in PI3K or AKT \citep{manzano2016resistant}
\item
  Experiments in melanoma cell lines support combined treatment with
  BRAF/MEK + PI3K/AKT inhibitors to overcome resistance
  \citep{manzano2016resistant}
\item
  BRAF inhibition (Vemurafenib) leads to the induction of PI3K/AKT
  pathway and inhibition of EGFR did not block this induction
  \citep{corcoran2012egfr}
\item
  Induction of PI3K/AKT pathway signaling has been associated with
  decreased sensitivity to MAPK inhibition \citep{corcoran2012egfr}
\end{itemize}

\section{Logical model of prostate cancer}\label{appendix-montagud}

In the context of the European project
\href{https://precise-project.eu/}{PRECISE} (Personalized Engine for
Cancer Integrative Study and Evaluation), focused on the integrative
study of prostate cancer, an adapted logical model has been built. This
prostate cancer model is initially based on the generic structure of the
Fumia model presented in section \ref{appendix-fumia}, which has been
considerably enriched and extended with genes and mechanisms specific to
prostate cancer such as ERG, SPOP or AR. The model contains 133 nodes
and 449 edges (Figure \ref{fig:Montagud}) and includes pathways like
androgen receptor and growth factor signalling, several signaling
pathways (Wnt, NFkB, PI3K/AKT, MAPK, mTOR, SHH), cell cycle,
epithelial-mesenchymal transition (EMT), Apoptosis, DNA damage, etc. The
model has 9 inputs (EGF, FGF, TGF beta, Nutrients, Hypoxia, Acidosis,
Androgen, TNF alpha and Carcinogen presence) and 6 outputs
(\emph{Proliferation}, \emph{Apoptosis}, \emph{Invasion},
\emph{Migration}, (bone) \emph{Metastasis} and \emph{DNA repair}).

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{fig/Montagud} 

}

\caption[GINsim representation of the "Montagud" logical model of prostate cancer]{\textbf{GINsim representation of the `Montagud'
logical model of prostate cancer.}}\label{fig:Montagud}
\end{figure}




\chapter{About statistics}\label{about-statistics}

\section{\texorpdfstring{\(R^2\) and
beyond}{R\^{}2 and beyond}}\label{r2-and-beyond}

\subsection{\texorpdfstring{Decomposition of
\(R^2\)}{Decomposition of R\^{}2}}\label{appendix-decomp}

The decomposition of \(R^2\) according to the method of
\citet{lindeman1980introduction} is detailed below. The presentation is
taken directly from \citet{gromping2006relative}.

A linear model is written
\(y_i=\beta_0+\beta_1x_{i1}+...+\beta_px_{ip}+e_i\) and the
corresponding \(R^2\) is:

\[R^2=\dfrac{\sum_{i=1}^{n} (\hat{y_i}-\bar{y_i})^2}{\sum_{i=1}^{n}  (y_i-\bar{y_i})^2}\]
Additionally, we define \(R^2(S)\) for a model with regressors in set S.
The additional \(R^2\) when adding the regressors in set \(M\) to a
model with the regressors in set \(S\) is given as:

\[seqR^2(M|S)=R^2(M\cup S)-R^2(S)\]

The order of the regressors in any model is a permutation of the
available regressors \(x_1, ..., x_p\) and is denoted by the tuple of
indices \(r = (r_1, ..., r_p)\). Let \(S_k(r)\) denote the set of
regressors entered into the model before regressor \(x_k\) in the order
\(r\). Then the portion of \(R^2\) allocated to regressor \(x_k\) in the
order \(r\) can be written as

\[seqR^2(\{x_k\}|S_k(r))=R^2(\{x_k\}\cup S_k(r))-R^2(S_k(r))\]

All in all, the \(R^2\) allocated to \(x_k\) after decomposition is:

\[R^2_{decomp}(x_k)=\dfrac{1}{p!}\sum_{r\text{ permutations}}seqR^2(\{x_k\}|r)\]

\subsection{\texorpdfstring{\(R^2\) for survival
data}{R\^{}2 for survival data}}\label{appendix-r2surv}

Among the different \(R^2\) analogues that have been proposed to measure
the variation explained by survival models, the one described by
\citet{royston2004new}, called \(R^2_D\) appears to be one of the most
relevant with respect to the following criteria: independance from
sensoring, interpretability and robustness to model misspecification
\citep{choodari2012simulation}.

The description is given below in the context of a Cox proportional
hazards (PH) survival model with \(n\) individuals with \(T_i\) and
\(C_i\) corresponding respectively to potential death (or relapse) and
censoring times, with \(i=1,2,...,n\). In this time-to-event setting,
\(X_i=min(T_i,C_i)\) is the time variables and
\(\delta_i=I(T_i \leq C_i)\) the status variable, \(I\) being the
indicator function. The Cox PH model then expresses the hazard function
as follows:

\[h(t|X)=h_0(t).\text{exp}(\beta'X)\],

with \(t\) the time to a death event, \(X\) the covariate vector and
\(beta\) the parameter vector. The adapted \(R^2\) called \(R^2_D\) is
given by \citep{royston2006explained}:

\[R^2=\dfrac{D^2/\kappa^2}{D^2/\kappa^2 + \sigma^2_{\epsilon}}\],

with the following component:

\begin{itemize}
\tightlist
\item
  \(D\) quantifies the separation of survival curves. It is computed by
  ordering the estimated prognostic index, \(\beta'X\), calculating the
  expected standard normal order statistics corresponding to these
  values, dividing the latter by a factor \(\kappa\), and performing an
  auxiliary regression on the scaled scores: the resulting regression
  coefficient is \(D\).
\item
  \(\kappa=\sqrt{8/\pi}\approx 1.60\) \citep{royston2004new}
\item
  \(\sigma^2_{\epsilon}\) is the variance of the error term,
  \(\sigma^2_{\epsilon}=\pi^2/6\) for Cox PH models
\end{itemize}

For a better understanding of this formula, it is interesting to note
that in a linear regression model \(Y\sim N(\beta'X, \sigma^2)\), it is
also possible to write \(R^2\) equivalently as follows:

\[R^2=\dfrac{\text{Var}(\beta'X)}{\text{Var}(\beta'X)+\sigma^2}\]

This formula underlines the analogy with \(R^2_D\), with
\(D^2/\kappa^2\) being interpreted as an estimate of the variance of the
prognostic index \(\beta'X\) for the Cox PH model.

\section{Causal inference with multiple versions of
treatment}\label{causal-inference-with-multiple-versions-of-treatment}

This section gathers the demonstrations of the equations present in
chapter \ref{chapter-precision} when they are not present in this
chapter and additional details about other estimators based on IPW and
TMLE.

\subsection{Overall treatment effect with multiple versions of treatment
(equation
\eqref{eq:overall-treatment-effect})}\label{appendix-overall-treatment-effect}

Here is the formal proof for equation \eqref{eq:overall-treatment-effect},
mostly derived from the proof of Proposition 3 in
\citep{vanderweele2013causal}.

\begingroup
\footnotesize

\begin{equation*}
\begin{aligned}
  E[ & Y(a, K^a(a))] = E[Y(a)] &&K^a \text{ actually received}\\
                  & = \sum_{c,w} E[Y(a)|c,w] \times P[c,w]&& \\
                  & = \sum_{c,w} E[Y(a)|a,c,w] \times P[c,w]
                  && Y(a) \perp \!\!\! \perp A | (C,W)\\
                  & = \sum_{c,w} E[Y(a, K^a(a))|a,c,w] \times P[c,w]&& \\
                  & = \sum_{c,w,k^a} E[Y(a, k^a)|a,K^a(a)=k^a,c,w]\times P[K^a(a)=k^a|a,c,w] \times P[c,w] &&\\
                  & = \sum_{c,w,k^a} E[Y(a, k^a)|a,K^a=k^a,c,w]\times P[K^a=k^a|a,c,w] \times P[c,w]
                  &&\text{consistency K} \\
                  & = \sum_{c,w,k^a} E[Y|a,K^a=k^a,c,w]\times P[K^a=k^a|a,c,w] \times P[c,w]
                  &&\text{consistency Y} \\
                  & = \sum_{c,w} E[Y|a,c,w]\times P[c,w]
\end{aligned}
\end{equation*}

\endgroup

Then, the overall treatment effect can be defined and computed by:

\[E[Y(a, K^a(a))] - E[Y(a^*, K^{a^*}(a^*))]\]

\subsection{Treatment effect with predefined distributions of versions
of treatment (equation
\eqref{eq:distrib-treatment-effect})}\label{appendix-distrib-treatment-effect}

Here is the formal proof for equation \eqref{eq:distrib-treatment-effect},
partially derived from the proof of Proposition 5 in
\citep{vanderweele2013causal}.

\begingroup
\footnotesize

\begin{equation*}
\begin{aligned}
  E[ & Y(a, G^a)] = \sum_{c} E[Y(a, G^a)|C=c] \times P[c]\\
             & = \sum_{c, k^a} E[Y(a, k^a)|G^a=k^a, C=c] \times P[G^a=k^a|C=c] \times P[c]\\
             & = \sum_{c, k^a} E[Y(a, k^a)| C=c] \times g^{k^a,c} \times P[c]
             &&\text{since } P[G^a=k^a] = g^{k^a,c}\\
             & = \sum_{c, k^a} E[Y(a, k^a)| A=a, K^a=k^a, C=c] \times g^{k^a,c} \times P[c] 
             &&\text{with } Y(a,k^a) \perp \!\!\! \perp \{A,K\} | C\\
             & = \sum_{c, k^a} E[Y| A=a, K^a=k^a, C=c] \times P[c]
             &&\text{by consistency for Y}
\end{aligned}
\end{equation*}

\endgroup

\subsection{Inverse probability of treatment weighted (IPW) estimators
for precision medicine}\label{appendix-IPW}

An extension of IPW methods described in section \ref{IPW-classic} to
multi-valued treatments (only treatment \(K\) with different modalities
and no \(A\)) has already been studied and the different formulas and
estimators adapted accordingly
\citep{imbens2000role, feng2012generalized}, defining in particular a
generalized propensity score:

\[f(k|c)=P[K=k|C=c]=E[I(k)|C=c]\]

\begin{equation*}
\text{with }
I(k) = \left\{
\begin{array}{ll}
1 & \quad \text{if } K = k \\
0 & \quad \text{otherwise}
\end{array}
\right.
\end{equation*}

and a subsequent estimator:

\[E[Y(k)]=\dfrac{\hat{E}[I(K=k)W^{K}Y]}{\hat{E}[I(K=k)W^K]} \text{ with } W^K=\dfrac{1}{f[K|C]}\]

In our precision medicine settings, to be consistent with the previously
defined causal diagram (Figure \ref{fig:DAG-multiple}), we have both
\(A\), binary status depending on the class of drugs, and \(K\), the
multinomial variable for versions of treatments, \emph{i.e.}, the
precise drug. Therefore we need to define a slightly different
propensity score with joint probabilities:

\begin{equation*}
\begin{aligned}
f(a,k|c) & =P[A=a,K=k|C=c]\\
         & =P[K=k|A=a, C=c].P[A=a|C=c]\\
         & =E[I(a,k)|C=c] 
\end{aligned}
\end{equation*}

\begin{equation*}
\text{with }
I(a,k) = \left\{
\begin{array}{ll}
1 & \quad \text{if } A = a, K = k \\
0 & \quad \text{otherwise}
\end{array}
\right.
\end{equation*}

From this we can deduce the estimator:

\[E[Y(a, k)]=\dfrac{\hat{E}[I(A=a,K=k)W^{A,K}Y]}{\hat{E}[I(A=a,K=k)W^{A,K}]} \text{ with } W^{A,K}=\dfrac{1}{f[A,K|C]}\]

In all the examples presented in this study and implemented in the code,
\(\mathcal{K}^{0} \cap \mathcal{K}^{1} = \emptyset\), it is therefore
possible to simplify the joint probabilities since the knowledge of K
automatically results in the knowledge of A allowing
\(P[A=a, K=k|C=c]=P[K=k|C=c]\). The above formulas with the attached
probabilities are still necessary in the general case and allow for the
derivation of causal effects \(\text{CE}_1\), \(\text{CE}_2\) and
\(\text{CE}_3\) previously described.

\subsection{TMLE}\label{appendix-TMLE}

Targeted maximum likelihood estimation is framework based on a doubly
robust maximum-likelihood--based approach that includes a ``targeting''
step that optimizes the bias-variance trade-off for a defined target
parameter. In particular, this method is perfectly compatible with the
use of machine learning algorithms for outcome or treatment models. A
detailed description of the method and its implementations can be found
in \citet{van2011targeted}.

The implementation proposed in this article is very similar to the one
proposed in a recent tutorial concerning the application to binary
processing \citep{luque2018targeted}. The specific characteristics of
the problem of precision medicine studied here lead to modify this
approach. In particular, the outcome and treatment models used in the
first steps are modified in the same way as the one explained for the
standardized estimators (outcome model) and for the IPW estimators
(treatment model). The step of updating the estimates is done on a model
similar to \citet{luque2018targeted}.

The algorithm used for the models internal to the TMLE are, as much as
possible, the same as those used for the standardised and IPW
estimators:

\begin{itemize}
\tightlist
\item
  For simulated data: generalized linear models in all cases except
  multinomial classification performed through the function
  \emph{multinom} in \emph{nnet} package.
\item
  For PDX data: random forests for all models. Use of SuperLearner
  \citep{van2007super} is made possible by simple modifications to the
  code but significantly slows down its execution.
\end{itemize}

\chapter{Résumé détaillé}\label{ruxe9sumuxe9-duxe9tailluxe9}

En vertu de
l'\href{https://www.legifrance.gouv.fr/affichCodeArticle.do?idArticle=LEGIARTI000006524389\&cidTexte=LEGITEXT000006071191}{article
L121-3 du code de l'éducation} la langue de rédaction privilégiée en
France pour les thèses est le français. Une thèse en anglais doit en
conséquence s'accompagner d'un résumé détaillé en français qui est
fourni ci-dessous en suivant une progression similaire au manuscrit qui
peut alors être titré:

\textbf{De la modélisation mécanistique des voies de signalisation dans
le cancer à l'interprétation des modèles et de leurs apports :
applications cliniques et évaluation statistique}

\section{Modélisation et cancer}\label{moduxe9lisation-et-cancer}

\textbf{La modélisation scientifique, la complexité et l'abstraction}

Il importe en premier lieu d'effectuer une brève clarification
sémantique et épistémologique concernant les \emph{modèles}, de loin le
mot plus fréquent de cette thèse. C'est en effet un terme polysémique, y
compris en se restreignant à la seule pratique scientifique. Si les
modèles y sont généralement reconnus comme des représentations des
phénomènes étudiés, ils peuvent recouvrir des réalités diverses, tantôt
objet physique manipulable (Figure \ref{fig:orrery}) et tantôt
construction purement formelle ou mathématique. C'est à cette deuxième
catégorie que nous allons nous attacher tout au long de cette thèse, en
la spécifiant davantage encore.

Ainsi, on distinguera par la suite les modèles \textbf{mécanistiques}
des modèles \textbf{statistiques}. Si tous deux sont des modèles
formels, ils diffèrent par leur structure et leurs objectifs (Figure
\ref{fig:boxes} et Tableau \ref{tab:mechstat}). Les premiers cherchent à
représenter les mécanismes internes du phénomène, quand les deuxièmes
cherchent à optimiser la prédiction du phénomène sans présupposer de
connaissances particulières. Une illustration de ces différences,
appliquée à la modélisation des interactions écologiques entre proies et
prédateurs, est proposée en section \ref{lotkasection}.

Un point crucial à retenir concernant ces modèles, et en particulier les
modèles mécanistiques, est leur nécessaire simplicité. Puisque leur
existence découle de la complexité des phénomènes étudiés, les modèles
sont par nature des \textbf{simplifications de la réalité}. Le recours
au bon niveau de détail et la justification des choix effectués sont
ainsi beaucoup plus importants qu'une impossible exactitude. Ou comme le
disait Paul Valéry\textasciitilde{}:

\begin{quote}
\emph{Ce qui est simple est toujours faux. Ce qui ne l'est pas est
inutilisable}
\end{quote}

\textbf{De la dérégulation de la machinerie cellulaire au cancer}

Le cancer, de par sa grande complexité cellulaire et moléculaire, est un
terrain de prédilection pour les modèles en tout genre. Les
manifestations cliniques de la maladie, caractérisée par une
prolifération incontrôlée de cellules, sont connues depuis des siècles.
La compréhension des mécanismes biologiques sous-jacents ne s'est elle
approfondie qu'à partir de la découverte de l'ADN comme support de
l'information génétique, au milieu du XX\textsuperscript{ème} siècle. Le
cancer est aujourd'hui reconnu comme une \textbf{maladie génétique} dont
l'origine réside essentiellement dans des altérations de l'ADN. C'est la
\textbf{combinaison de plusieurs altérations}, souvent accumulées au fil
du temps, qui permet d'inactiver les différentes protections biologiques
qui prémunissent en temps normal contre une prolifération cellulaire
excessive.

Ainsi, il apparait de plus en plus indispensable de considérer les
informations biologiques de chaque patient (mutations de l'ADN, niveaux
d'expression ARN etc.) non plus séparément mais conjointement afin de
comprendre le phénomène tumoral. La prise en compte des \textbf{réseaux
d'interactions} entre gènes, ARN et protéines éclaire également la
compréhension : le cancer n'est plus seulement une maladie génétique
mais également une maladie de réseaux (Figure \ref{fig:pathways}).

Sur le plan technologique, la recherche profite également depuis le
début du XXI\textsuperscript{ème} siècle de données beaucoup plus
abondantes, issues notamment du séquençage à haut débit, qui permettent
une vision plus globale en renseignant sur des milliers de gènes et ce à
travers \textbf{différents types de données omiques} : génomique (ADN),
transcriptomique (ARN), protéomique (protéines) etc. Ces données sont
notamment disponibles publiquement pour des \textbf{milliers de
patients} atteints de cancer (consortium TCGA par exemple) mais aussi
pour de \textbf{nombreux modèles précliniques} comme des lignées
cellulaires provenant de patients.

\textbf{Modélisation mécanistique du cancer : d'une maladie complexe à
la biologie des systèmes}

L'abondance des données et des relations entre les entités biologiques a
rendu nécessaire l'utilisation de méthodes computationnelles pour les
comprendre et les modéliser. Ce thèse se focalise sur la
\textbf{modélisation au niveau moléculaire}, celui des interactions
entre gènes, ARN et protéines au sein des cellules. Même en se
focalisant sur les modèles mécanistiques qui intègrent la connaissance
biologique et biochimique sur ces entités, plusieurs formalismes
mathématique existent pour écrire un modèle. Deux formalismes parmi les
plus fréquents concentreront l'essentiel des analyses de cette thèse. Le
premier est constitué d'\textbf{équations différentielles}, il est
quantitatif mais mobilise de nombreux paramètres. Le second est le
\textbf{formalisme logique}, plus parcimonieux mais qualitatif et qui
sera décrit en détail plus avant.

L'un comme l'autre ont pour vocation de répliquer le phénomène tumoral
étudié (activation d'une voie de signalisation, impact d'une mutations
etc.) tout en représentant explicitement les entités biologiques
impliquées. De par leur structure complexe, souvent non-linéaire, ils
peuvent mettre en évidence des comportements dit \textbf{émergents} qui
relèvent d'une réponse du système dans son ensemble et ne pouvaient se
déduire des entités biologiques prises séparément.

Au-delà de leur utilité intellectuelle et scientifique dans l'étude des
phénomènes tumoraux, ces modèles mécanistiques moléculaires du cancer
sont parfois utilisés pour des \textbf{analyses ou prédictions à portée
clinique et médicale} : survie d'un patient en l'absence de traitement
(valeur pronostique), réponse d'un patient à un traitement donné (valeur
prédictive). Cette thèse se focalise essentiellement sur ces questions
d'impact clinique des modèles mécanistiques de cancer, entre biologie
des systèmes et biostatistiques.

\section{Des modèles logiques personnalisés de
cancer}\label{des-moduxe8les-logiques-personnalisuxe9s-de-cancer}

\textbf{Principes de modélisation logique et intégration des données}

L'essentiel des modèles mécanistiques présentés dans cette thèse
relèvent du formalisme logique. Chaque entité biologique y est
représentée par une \textbf{variable discrète}, souvent binaire,
interprétée comme une abstraction de son activité: \(0\) si inactive
(gène non transcrit, ARN dégradé, protéine en trop faible concentration
etc.), \(1\) si active (gène transcrit, protéine phosphorylée etc.). Ces
entités sont reliées entre elles par des règles logiques composées à
partir des opérateurs ET (\&), OU (\textbar{}), NON (!). On pourra ainsi
définir qu'une entité A doit être activée si B l'est et que, dans le
même temps, C ne l'est pas (``B \& !C'').

Par la suite les modèles logiques seront simulés suivant une
\textbf{actualisation asynchrone} telle qu'encodée dans le logiciel
MaBoSS dont le fonctionnement est résumé en Figure \ref{fig:maboss}. En
bref, cela signifie que la mise à jour de l'état des variables du modèle
(par exemple le passage de l'état \(0\) à l'état \(1\) si la règle
logique est vérifiée) se fait une variable à la fois et que l'ordre des
mises à jour est défini stochastiquement à partir de \textbf{constantes
de réactions} associés aux variables (algorithme de Gillespie).

La plupart des modèles logiques utilisés par la suite ont été construits
à partir de la littérature comme source primaire d'information.
Cependant, dans ce formalisme comme dans les autres, les \textbf{données
biologiques sont de toute première importance} à différentes étapes du
modèles, que ce soit pour le définir, la paramétrer ou le valider
(Figure \ref{fig:logical-data}).

\textbf{Personnalisation des modèles logiques : méthode et validation
pronostique}

Les modèles définis à partir de la littérature sont par construction
assez génériques et ne permettent pas d'explorer, pour un même cancer,
les différences entre individus en termes d'agressivité de la tumeur ou
de réponse au traitement. À partir d'un réseau moléculaire générique,
une des possibilités est d'utiliser les données biologiques des
différentes tumeurs pour spécifier, personnaliser les modèles. Les
méthodes existantes proposent pour la plupart d'\textbf{entraîner et
d'optimiser les paramètres des modèles} à l'aide d'une fonction
d'objectif prédéfinie. Cela requiert cependant des données riches,
souvent des données de perturbations qui sont rarement accessibles en
dehors de certains modèles précliniques comme les lignées cellulaires.

Une autre approche a été suivie dans cette thèse consistant à
personnaliser les modèles en \textbf{interprétant biologiquement des
données statiques} (par opposition aux données de perturbation) issues
d'un seul prélèvement initial. Le formalisme logique est alors mobilisé,
sa nature qualitative et parcimonieuse étant adaptée aux données
utilisées. Peut-on alors obtenir de ces données des modèles
mécanistiques de cancer personnalisés et interprétables cliniquement ?
La \textbf{méthode PROFILE} conçue durant cette thèse cherche à répondre
à cette question \textbf{sans entraînement des modèles}.

Son principe peut se diviser en deux méthodes distinctes: \textbf{la
personnalisation discrète et la personnalisation continue} (Figure
\ref{fig:logical-personalization}). La première infère l'\textbf{effet
fonctionnel des altérations génétiques} considérées discrètes comme les
mutations ou les altérations du nombre de copies d'un gène. Une mutation
connue pour impliquer une perte (resp. un gain) de fonction de l'entité
biologique sera traduite en forçant la variable correspondante du modèle
à rester à \(0\) (resp. \(1\)) pour le patient concerné. La seconde
méthode consiste elle à interpréter des quantités continues comme
peuvent l'être les niveaux d'ARN, de protéines, de phosphorylation etc.
Il s'agit alors, \textbf{pour chaque variable, de détecter la forme de
distribution au sein de la cohorte puis de la normaliser}, c'est à dire
de la transformer en une variable continue comprise entre \(0\) et \(1\)
(Figure \ref{fig:logical-processing}). Cette valeur peut ensuite être
utilisée pour définir les constantes de réactions associées à chaque
variable du modèle et qui influent sur la rapidité avec laquelle ladite
variable sera mise à jour. Les deux types de personnalisations peuvent
être combinés, en intégrant par exemple les mutations et les niveaux
d'ARN chacun suivant la méthode adaptée. Cette combinaison sera celle
utilisée par défaut dans les analyses évoquées ultérieurement.

Une première validation de la méthode est faite en vérifiant la capacité
des modèles logiques personnalisés à différencier des tumeurs plus ou
moins agressives. Des analyses de survies de patientes atteintes de
cancers du sein sont notamment proposées en Figure
\ref{fig:PROFILE-METABRIC-Survival} et démontrent la capacité des
modèles personnalisés à \textbf{stratifier des patientes présentant des
pronostics différents}.

\textbf{Des modèles logiques personnalisés pour interpréter la réponse
aux traitements}

Mais l'intérêt principal des modèles mécanistiques personnalisés réside
dans leur représentation explicite des entités et mécanismes biologiques
sous-jacents. Une des façons de mettre à profit cet avantage est de
s'intéresser à l'\textbf{effet de certains traitements sur les modèles}.
Il est en effet possible de modéliser l'effet d'un traitement si sa
cible et son mode d'action sont suffisamment connus. Cette analyse n'est
\emph{a priori} pas possible dans un modèle statistique si le traitement
n'est pas compris dans les données d'entraînement.

Une première analyse à large spectre est menée à l'aide de données
concernant plusieurs centaines de lignées cellulaires (provenant de
différents types de cancer) et de traitements. L'utilisation d'un modèle
logique générique n'a cependant pas permis de tirer des enseignements
précis de cette analyse. Une étude plus ciblée a donc été menée par la
suite pour étudier la \textbf{réponse aux inhibiteurs de BRAF des
mélanomes et cancers colorectaux}, étant observé que les premiers
répondent assez bien au traitement et pas les seconds, en dépit de
profils moléculaires assez semblables. Un modèle centré autour de BRAF
tout d'abord construit et validé qualitativement. Il est ensuite
personnalisé à l'aide des données de mutations et d'ARN issues de
lignées cellulaires des deux cancers concernés. La réponse des modèles
personnalisés à l'inhibition de BRAF est ensuite évaluée en observant le
score de \emph{Prolifération} (variable de sortie phénotypique du
modèle) atteint par ces modèles avec et sans inhibiteurs. La réduction
de \emph{Prolifération} engendrée par l'inhibition simulée de BRAF est
comparée aux résultats expérimentaux issus de l'inhibition de BRAF sur
les mêmes lignées cellulaires par un traitement ou par CRISPR/Cas9. Les
corrélations significatives entre sensibilités expérimentales et
simulées valident la pertinence des modèles personnalisés, en
particulier ceux résultant de l'intégration conjointe des données de
mutation et d'ARN (Figure \ref{fig:BRAF-results}). Les modèles
personnalisés apparaissent par ailleurs comme un outil d'investigation
qualitative porteur de sens en permettant de mettre en lumière et en
contexte certains mécanismes de résistance, liés à CRAF par exemple
(Figure \ref{fig:BRAF-interactive}). Une comparaison avec des méthodes
d'apprentissage automatique, ici des forêts aléatoires, est effectuée
afin de souligner la complémentarité des approches et l'intérêt des
modèles logiques personnalisés sans entraînement en présence d'un nombre
d'échantillons réduit.

Une application de cette méthode de personnalisation au cancer de la
prostate est ensuite présentée. La stratégie est similaire au cas
précédent, en insistant sur l'identification de nouvelles cibles
thérapeutiques pour une lignée cellulaire en particulier. Certaines
cibles identifiées comme efficaces par les modèles personnalisés sont
ensuite testées et validées \emph{in vitro}.

\section{Quantification statistique de l'impact clinique des
modèles}\label{quantification-statistique-de-limpact-clinique-des-moduxe8les}

\textbf{Flux d'informations dans les modèles mécanistiques de cancer}

La valeur clinique, c'est à dire pronostique ou prédictive, des modèles
mécanistiques a été analysée jusqu'ici de manière assez simple en se
focalisant sur la capacité des sorties du modèle (souvent des variables
à interprétation phénotypique comme \emph{Prolifération}) à corréler
avec des résultats cliniques. Cependant les données utilisées en entrée
des modèles mécanistiques, pour les paramétrer ou les personnaliser,
sont souvent déjà des variables pertinentes cliniquement : statut d'une
mutation, biomarqueur ARN etc. Il est donc nécessaire de considérer à la
fois les variables d'entrée et de sortie du modèle à l'aune des
résultats cliniques afin de comprendre \textbf{comment le modèle traite
et transforme les informations cliniques qu'il ingère}.

Un premier exemple est fourni, fondé sur des données simulées et
l'utilisation du pourcentage de la variance exprimée (\(R^2\)), pour
mettre en exergue deux grandes catégories de modèles mécanistiques
(Figures \ref{fig:model-simulation} et \ref{fig:R2-artificial}). La
première correspond à ceux dont les sorties ont une valeur clinique
inférieure aux entrées. Les sorties du modèles, généralement en nombre
réduit, peuvent néanmoins constituer une \textbf{réduction de dimension}
pertinente des entrées, souvent nombreuses. La seconde catégorie est
celle des modèles dont les sorties ont une valeur supérieure ou
complémentaire aux entrées : ils ont \textbf{capturé un comportement
émergent} qui a une valeur clinique. Cette analyse est appliquée à des
modèles déjà publiés pour illustrer la nature de l'information clinique
qu'ils fournissent.

\textbf{Essais cliniques et inférence causale}

Si l'on se focalise sur la capacité des modèles mécanistiques à orienter
le choix des traitements, il devient nécessaire de développer d'autres
méthodes d'analyse des modèles, plus proches de la pratique clinique. En
la matière, l'évaluation des traitements et stratégies thérapeutiques se
fait souvent à travers des \textbf{essais cliniques randomisés} qui
permettent de comparer des populations similaires qui ne diffèrent que
par l'administration du traitement ou du contrôle. Dans le cas de
données non randomisées, que l'on appellera ici
\textbf{observationnelles}, le choix des patients qui ont reçu le
traitement ou le contrôle peut avoir été fait suivant des critères
particuliers qui deviennent ainsi des \textbf{facteurs de confusion}
dans l'analyse : la différence de résultats peut provenir de l'effet du
traitement ou d'une répartition déséquilibrée des patients (Figures
\ref{fig:causality-example} et \ref{fig:causality-example2}).

Il est toutefois possible d'utiliser des méthodes pour corriger certains
de ses déséquilibres et ainsi, sous certaines conditions, d'interpréter
les différences résiduelles de résultats entre patients traités et
contrôles comme résultant de l'effet causal du traitement. Trois
implémentations différentes de ces méthodes d'\textbf{inférence causale}
sont utilisées dans la thèse, chacune fondée sur un modèle statistique
différent pour contrôler l'effet des facteurs de confusion: la
standardisation (modèle du résultat du traitement), la pondération selon
l'inverse de la probabilité (modèle de l'assignation du traitement), le
\emph{targeted maximum likelihood estimation} TMLE (combinaison des deux
modèles précédents). Ces méthodes dépendent cependant toutes
d'hypothèses parfois difficiles à vérifier.

\textbf{Inférence causale pour la médecine de précision}

Mais les modèles mécanistiques peuvent théoriquement permettre de
choisir parmi plusieurs traitements pour chaque profil moléculaire de
patient. Ils sont ainsi assimilables à une \textbf{stratégie ou un
algorithme de médecine de précision}. Peut-on alors appliquer les
méthodes d'inférence causale à l'évaluation de ce genre de stratégies
cliniques qui englobent différentes molécules thérapeutiques ? Dans le
dernier temps de cette thèse, une extension de ces méthodes est proposée
pour s'adapter à des stratégies cliniques comprenant \textbf{différentes
versions de traitement}. Cela revient à utiliser des données
observationnelles pour simuler des essais cliniques comparant un bras où
les patients sont traités suivant les recommandations de l'algorithme de
médecine de précision (qui peut être un modèle mécanistique) avec
différents types de bras contrôle: traitement standard de référence
(molécule unique), traitement prévu par le médecin, ou traitement ciblé
aléatoirement défini.

Ces méthodes et les estimateurs statistiques associés sont d'abord
validés sur des données artificiellement générées afin de mesurer leur
capacité à corriger d'éventuels biais causés par l'hétérogénéité de
l'effet des traitements ou la répartition inégale des traitements
observés par exemple. Une \textbf{application interactive RShiny} est
également fournie pour permettre l'exploration de très nombreux
scénarios de simulations, y compris par un public non familier du
langage R (Figure \ref{fig:Shiny}).

Dans un deuxième temps, une application a été menée avec des données
précliniques issues de \textbf{xénogreffes dérivées de patients} (PDX).
Cela correspond à des tumeurs prélevées chez des patients avant d'être
divisées en échantillons implantés chez des souris immunodéprimées
identiques qui seront traitées avec des molécules différentes (Figure
\ref{fig:PDX-principles}). En conséquence, il est possible d'accéder,
pour chaque tumeur, à sa réponse thérapeutique face à différents
traitements. Ces informations, qui ne sont pas disponibles pour les
vrais patients qui ne peuvent recevoir qu'un traitement à la fois,
permettent d'\textbf{accéder directement dans les données à l'effet de
différentes stratégies thérapeutiques pour chaque tumeur}. Ainsi, il a
été possible de valider, sur des données précliniques réelles, la
supériorité des valeurs issues des nouvelles méthodes d'inférence
causale proposées par rapport aux valeurs obtenues suivant des méthodes
plus directes.

\section{Conclusion}\label{conclusion-1}

Cette thèse trace un chemin, \textbf{de la conception de modèles
mécanistiques du cancer à la quantification de leur impact clinique}. La
faculté de ces modèles à passer de l'outil théorique à une
interprétation clinique repose ici sur leur personnalisation permise par
l'intégration des données omiques dans un canevas tissé à partir des
connaissances biologiques issues de la littérature. Les modèles logiques
personnalisés qui ont été présentés restent essentiellement des supports
qualitatifs à l'interprétation clinique des phénomène tumoraux. La
réflexion menée par la suite sur leur impact clinique a mis en évidence
l'importance d'une évaluation globale qui permette de comprendre
l'origine de leur valeur clinique et la nécessité de développer des
méthodes statistiques adaptées pour évaluer leurs apports en termes de
médecine de précision.

\bibliography{bib/thesis.bib}

\end{document}
