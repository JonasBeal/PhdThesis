[
["index.html", "From the mechanistic modeling of signaling pathways in cancer to the interpretation of models and their contributions: clinical applications and statistical evaluation PhD Thesis", " From the mechanistic modeling of signaling pathways in cancer to the interpretation of models and their contributions: clinical applications and statistical evaluation 23/09/2020 PhD Thesis by Jonas Béal Supervised by Emmanuel Barillot, Laurence Calzone and Aurélien Latouche "],
["summary.html", "Summary", " Summary Beyond its genetic mechanisms, cancer can be understood as a network disease that often results from the interaction between different perturbations in a cellular regulatory network. The dynamics of these networks and associated signaling pathways are complex and require integrated approaches. One approach is to design mechanistic models that translate the biological knowledge of networks in mathematical terms to simulate the molecular features of cancers in a computer-readable form. However, these models only reflect the general mechanisms at work in cancers. This thesis proposes to define personalized mechanistic models of cancer. A generic model is first defined in a logical (or Boolean) formalism, before using omics data (mutations, RNA, proteins) from patients or cell lines in order to make the model specific to each one profile. These personalized models can then be compared with the clinical data of patients in order to validate them. The response to treatment is investigated in particular in this thesis. The explicit representation of the molecular mechanisms by these models allows to simulate the effect of different treatments according to their targets and to verify if the sensitivity of a patient to a drug is well predicted by the corresponding personalized model. An example concerning the response to BRAF inhibitors in melanomas and colorectal cancers is thus presented. The comparison of mechanistic models of cancer, those presented in this thesis and others, with clinical data also encourages a rigorous evaluation of their possible benefits in the context of medical use. The quantification and interpretation of the value of certain prognostic models is briefly presented before focusing on the particular case of models able to recommend the best treatment for each patient according to his molecular profile. A theoretical framework is defined to extend causal inference methods to the evaluation of such precision medicine algorithms. An illustration is provided using simulated data and patient derived xenografts. All the methods and applications put forward a possible path from the design of mechanistic models of cancer to their evaluation using statistical models emulating clinical trials. Keywords: Modeling, Cancer, Mechanistic model, Biostatistics, Causal inference, Precision medicine. "],
["resumé.html", "Resumé", " Resumé Au delà de ses mécanismes génétiques, le cancer peut-être compris comme une maladie de réseaux qui résulte souvent de l’interaction entre différentes perturbations dans un réseau de régulation cellulaire. La dynamique de ces réseaux et des voies de signalisation associées est complexe et requiert des approches intégrées. Une d’entre elles est la conception de modèles dits mécanistiques qui traduisent mathématiquement la connaissance biologique des réseaux afin de pouvoir simuler le fonctionnement moléculaire des cancers informatiquement. Ces modèles ne traduisent cependant que les mécanismes généraux à l’oeuvre dans certains cancers en particulier. Cette thèse propose en premier lieu de définir des modèles mécanistiques personnalisés de cancer. Un modèle générique est d’abord défini dans un formalisme logique (ou Booléen), avant d’utiliser les données omiques (mutations, ARN, protéines) de patients ou de lignées cellulaires afin de rendre le modèle spécifique à chacun. Ces modèles personnalisés peuvent ensuite être confrontés aux données cliniques de patients pour vérifier leur validité. Le cas de la réponse clinique aux traitements est exploré en particulier dans cette thèse. La représentation explicite des mécanismes moléculaires par ces modèles permet en effet de simuler l’effet de différents traitements suivant leur mode d’action et de vérifier si la sensibilité d’un patient à un traitement est bien prédite par le modèle personnalisé correspondant. Un exemple concernant la réponse aux inhibiteurs de BRAF dans les mélanomes et cancers colorectaux est ainsi proposé. La confrontation des modèles mécanistiques de cancer, ceux présentés dans cette thèse et d’autres, aux données cliniques incite par ailleurs à évaluer rigoureusement leurs éventuels bénéfices dans la cadre d’une utilisation médicale. La quantification et l’interprétation de la valeur de certains modèles à visée pronostique est brièvement présentée avant de se focaliser sur le cas particulier des modèles capables de sélectionner le meilleur traitement pour chaque patient en fonction des ses caractéristiques moléculaires. Un cadre théorique est proposé pour étendre les méthodes d’inférence causale à l’évaluation de tels algorithmes de médecine de précision. Une illustration est fournie à l’aide de données simulées et de xénogreffes dérivées de patients. L’ensemble des méthodes et applications décrites tracent donc un chemin, de la conception de modèles mécanistiques de cancer à leur évaluation grâce à des modèles statistiques émulant des essais cliniques. Mots-clés: Modélisation, Cancer, Modèle mécanistique, Biostatistiques, Inférence causale, Médecine de précision. "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements I would like to thank my (insert incredible superlatives) supervisors. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum The jury of course… Other members of the lab Family and friends at last. "],
["preface.html", "Preface", " Preface The present thesis is structured in three parts, each subdivided into three chapters. Since the whole thesis is about cancer modeling, the first part aims at defining the type of model to be referred to, and in particular models that will be called mechanistic, as well as the object of the modeling, i.e. the molecular networks involved in cancer. So the first part answers the question: what is a cancer model and what is its purpose? The second part will be devoted to the methods developed during this thesis to transform qualitative models of molecular networks, known as logic models, into personalized models that can be interpreted clinically. In short, how can a mathematical representation of biological knowledge be transformed into a tool that contributes to the understanding of the clinical manifestations of cancer? Finally, the third and last part will look at how the clinical relevance of all the above-mentioned models can be rigorously evaluated, both in their ability to predict the evolution of the disease and in their ability to recommend the most appropriate treatments for each patient. How to quantify and interpret the value of the clinical information delivered by these models? Moreover, this thesis also exists in an online version that allows to take advantage of the interactivity of some graphs and applications: https://jonasbeal.github.io/thesis/. "],
["scientific-modeling-abstract-the-complexity.html", "Chapter 1 Scientific modeling: abstract the complexity 1.1 What is a model? 1.2 Statistics or mechanistic 1.3 Simplicity is the ultimate sophistication", " Chapter 1 Scientific modeling: abstract the complexity Ce qui est simple est toujours faux. Ce qui ne l’est pas est inutilisable. Paul Valéry (Mauvaises pensées et autres, 1942) The notion of modeling is embedded in science, to the point that it has sometimes been used to define the very nature of scientific research. What is called a model can, however, correspond to very different realities which need to be defined before addressing the object of this thesis which will consist, if one wants to be mischievous, in analyzing models with other models. This semantic elucidation is all the more necessary as this thesis is interdisciplinary, suspended between systems biology and biostatistics. In order to convince the reader of the need for such a preamble, he is invited to ask a statistician and a biologist how they would define what a model is. Figure 1.1: A scientist and his model. Joseph Wright of Derby, A Philosopher Giving a Lecture at the Orrery (in which a lamp is put in place of the sun), c. 1763-65, oil on canvas, Derby Museums and Art Gallery 1.1 What is a model? 1.1.1 In your own words A model is first of all an ambiguous object and a polysemous word. It therefore seems necessary to start with a semantic study. Among the many meanings and synonymous proposed by the dictionary (Figure 1.2), while some definitions are more related to art, several find echoes in scientific practice. It is sometimes a question of the physical representation of an object, often on a reduced scale as in Figure 1.1, and sometimes of a theoretical description intended to facilitate the understanding of the way in which a system works (Collins 2020). It is even sometimes an ideal to be reached and therefore an ambitious prospect for an introduction. Figure 1.2: Network visualization of model thesaurus entries. Generated with the ‘Visual Thesaurus’ ressource The narrower perspective of the scientist does not reduce the completeness of the dictionary’s description to an unambiguous object (Bailer-Jones 2002). In an attempt to approach these multi-faceted objects that are the models, Daniela Bailer-Jones interviewed different scientists and asked them the same question: what is a model? Across the different profiles and fields of study, the answers vary but some patterns begin to emerge (Figure 1.3). A model must capture the essence of the phenomenon being studied. Because it eludes, voluntarily or not, many details or complexity, it is by nature a simplification of the phenomenon. These limitations may restrict its validity to certain cases or suspend it to the fulfilment of some hypotheses. They are not necessarily predictive, but they must be able to generate new hypotheses, be tested and possibly questioned. Finally, and fundamentally, they must provide insights about the object of study and contribute to its understanding. Figure 1.3: Scientists talk about their models: words cloud. Cloud of words summarizing the lexical fields used by scientists to talk about their models in dedicated interviews reported by Bailer-Jones (2002). These definitions circumscribe the model object, its use and its objectives, but they do not in any way describe its nature. And for good reason, because even if we agree on the described contours, the biodiversity of the models remains overwhelming for taxonomists: Probing models, phenomenological models, computational models, developmental models, explanatory models, impoverished models, testing models, idealized models, theoretical models, scale models, heuristic models, caricature models, exploratory models, didactic models, fantasy models, minimal models, toy models, imaginary models, mathematical models, mechanistic models, substitute models, iconic models, formal models, analogue models, and instrumental models are but some of the notions that are used to categorize models. (Frigg and Hartmann 2020) 1.1.2 Physical world and world of ideas Without claiming to be exhaustive, we can make a first simple dichotomy between physical/material and formal/intellectual models (Rosenblueth and Wiener 1945). The former consist in replacing the object of study by another object, just as physical but nevertheless simpler or better known. These may be models involving a change of scale such as the simple miniature replica placed in a wind tunnel, or the metal double helix model used by Watson and Crick to visualize DNA. In all these cases the model allows to visualize the object of study (Figure 1.4 A and B), to manipulate it and play with it to better understand or explain a phenomenon, just like the scientist with his orrery (Figure 1.1). In the case of biology, there are mainly model organisms such as drosophila, zebrafish or mice, for example. We then benefit from the relative simplicity of their genomes, a shorter time scale or ethical differences, usually to elucidate mechanisms of interest in humans. Correspondence between the target system and its model can sometimes be more conceptual, such as that ones relying on mechanical–electrical analogies: a mechanical system (e.g. a spring-mass system) can sometimes be represented by an electric network (e.g. a RLC circuit with a resistor, a capacitor and an inductor). Figure 1.4: Orrery, planets and models. Physical models of planetary motion, either geocentric (Armillary sphere from Plate LXXVII in Encyclopedia Britannica, 1771) or heliocentric in panel B (Bion, 1751, catalogue Bnf) and some geometric representations by Johannes Kepler in panel C (in Astronomia Nova, 1609) The model is then no longer simply a mimetic replica but is based on an intellectual equivalence: we are gradually moving into the realm of formal models (Rosenblueth and Wiener 1945). These are of a more symbolic nature and they represent the original system with a set of logical or mathematical terms, describing the main driving forces or similar structural properties as geometrical models of planetary motions summarized by Kepler in Figure 1.4C. Historically these models have often been expressed by sets of mathematical equations or relationships. Increasingly, these have been implemented by computer. Despite their sometimes less analytical and more numerical nature, many so-called computational models could also belong to this category of formal models. There are then many formalisms, discrete or continuous, deterministic or stochastic, based on differential equations or Boolean algebra (Fowler, Fowler, and Fowler 1997). Despite their more abstract nature, they offer similar scientific services: it is possible to play with their parameters, specifications or boundary conditions in order to better understand the phenomenon. One can also imagine these formal models from a different perspective, which starts from the data in a bottom-up approach instead of starting from the phenomenon in a top-down analysis. These models will then often be called statistical models or models of data (Frigg and Hartmann 2020). This distinction will be further clarified in section 1.2. To summarize and continue a little longer with the astronomical metaphor, the study of a particularly complex system (the solar system) can be broken down into a variety of different models. Physical and mechanical models such as armillary spheres (1.4A and B) make it possible to touch the object of study. In addition, we can observe the evolution of models which, when confronted with data, have progressed from a geocentric to a heliocentric representation to get closer to the current state of knowledge. Sometimes, models with more formal representations are used to give substance to ideas and hypotheses (1.4C). One of the most conceptual forms is then the mathematical language and one can thus consider that the previously mentioned astronomical models find their culmination in Kepler’s equations about orbits, areas and periods that describe the elliptical motion of the planets. We refer to them today as Kepler’s laws. The model has become a law and therefore a paragon of mathematical modeling (Wan 2018). 1.1.3 Preview about cancer models As we get closer to the subject of our study, and in order to illustrate these definitions more concretely, we can take an interest in the meaning of the word model in the context of cancer research. For this, we restrict our corpus to scientific articles found when searching for “cancer model” in the PubMed article database. Among these, we look at the occurrences of the word model and the sentences in which it is included. This cancer-related context of model is represented as a tree in Figure 1.5. Some of the distinctions already mentioned can be found here. The mouse and xenograft models, which will be discussed later in this thesis, represent some of the most common physical models in cancer studies. These are animal models in which the occurrence and mechanisms of cancer, usually induced by the biologist, are studied. On the other hand, prediction, prognostic or risk score models refer to formal models and borrow from statistical language. Figure 1.5: Tree visualization of model semantic context in cancer-related literature Generated with the ‘PubTrees’ tool by Ed Sperr, and based on most relevant PubMed entries for “cancer model” search. Another way to classify cancer models may be to group them into the following categories: in vivo, in vitro and in silico. The first two clearly belong to the physical models but one uses whole living organisms (e.g. a human tumour implanted in an immunodeficient mouse) and the other separates the living from its organism in order to place it in a controlled environment (e.g. tumour cells in growth medium in a Petri dish). In the thesis, data from both in vivo and in vitro models will be used. However, unless otherwise stated, a model will always refer to a representation in silico. This third category, however, contains a very wide variety of models (Deisboeck et al. 2009), to which we will come back in chapter 3. A final ambiguity about the nature of the formal models used in this thesis needs to be clarified beforehand. 1.2 Statistics or mechanistic A rather frequent metaphor is to compare formal models to black boxes that take in input \\(X\\) predictors, or independent variables, and output response variable(s) \\(Y\\), also named dependent variables. The models then split into two categories (Figure 1.6) depending on the answer to the question: are you modeling the inside of the box or not? 1.2.1 The inside of the box The purpose of this section is to present in a schematic, and therefore somewhat caricatural, manner the two competing formal modeling approaches that will be used in this thesis and that we will call mechanistic modeling and statistical modeling. Assuming the unambiguous nature of the predictors and outputs we can imagine that the natural process consists in defining the result Y from the inputs X according to a function of a completely unknown form (Figure 1.6A). Figure 1.6: Different modeling strategies. (A) Data generation from predictors \\(X\\) to response \\(Y\\) in the natural phenomenon. (B) Mechanistic modeling defining mechanisms of data generation inside the box. (C) Statistical modeling finding the function \\(f\\) that gives the best predictions. Aadapted from Breiman (2001b). The first modeling approach, that we will call mechanistic, consists in building the box by imitating what we think is the process of data generation (Figure 1.6B). This integration of a priori knowledge can take different forms. In this thesis it will often come back to presupposing certain relations between entities according to what is known about their behaviour. \\(X_1\\) which acts on \\(X_3\\) may correspond to the action of one biological entity on another, supposedly unidirectional; just as the joint action of \\(X_2\\) and \\(X_3\\) may reflect a known synergy in the expression of genes or the action of proteins. Mathematically this is expressed here with a perfectly deterministic model defined a priori. All in all, in a purely mechanistic approach, the nature of the relations between entities should be linked to biological processes and the parameters in the model all have biological definitions in such a way that it could even be considered to measure them directly. For example, the coefficient \\(2\\) multiplying \\(X_2X_3\\) can correspond to a stoichiometric coefficient or a reaction constant which have a theoretical justification or are accessible by experimentation. In some fields of literature these models are sometimes called mathematical models because they propose a mathematical translation of a phenomenon, which does not start from the data in a bottom-up approach but rather from a top-down theoretical framework. In this thesis we will adhere to the mechanistic model name, which is more transparent and less ambiguous compared to other approaches also based on mathematics, without necessarily the other characteristics described above. The second approach, often called statistical modeling, or sometimes machine learning depending on the precise context and objective, does not necessarily seek to reproduce the natural process of data generation but to find the function allowing the best prediction of \\(Y\\) from \\(X\\) (Figure 1.6C). Pushed to the limit, they are “idealized version of the data we gain from immediate observation” (Frigg and Hartmann 2020), thus providing a phenomenological description. The methods and algorithms used are then intended to be sufficiently flexible and to make the fewest possible assumptions about the relationships between variables or the distribution of data. Without listing them exhaustively, the approaches such as support vector machines (Cortes and Vapnik 1995) or random forests (Breiman 2001a), which will sometimes be mentioned in this thesis, fall into this category which contains many others (Hastie, Tibshirani, and Friedman 2009). Several discrepancies result from this difference in nature between mechanistic and statistical models, some of which are summarized in the Table 1.1. In a somewhat schematic way, we can say that the mechanistic model first asks the question of how and then looks at the result for the output. The notion of causality is intrinsic to the definition of the model. Conversely, the statistical model first tries to approach the Y and then possibly analyses what can be deduced from it, regarding the importance of the variables or their relationships in a post hoc approach (Ishwaran 2007; Manica et al. 2019). The causality is then not a by-product of the algorithm and must be evaluated with dedicated frameworks (Hernán and Robins 2020). The greater flexibility of statistical methods makes it possible to better accept the heterogeneity of the variables, but this is generally done at the cost of a larger number of parameters and therefore requires more data. Moreover, statistical models can be considered as inductive, since they are able to use already generated data to identify patterns in it. Conversely, mechanistic models are more deductive in the sense that they can theoretically allow to extrapolate beyond the original data or knowledge used to build the model (Baker et al. 2018). Finally, the most relevant way of assessing the value or adequacy of these models may be quite different. A statistical model is measured by its ability to predict output in a validation dataset different from the one used to train its parameters. The mechanistic model will also be evaluated on its capacity to approach the data but also to order it, to give a meaning. If its pure predictive performance is generally inferior, how can the value of understanding be assessed? This question will be one of the threads of the dissertation. Table 1.1: Some pros and cons for mechanistic and statistical modeling. Adapted from Baker et al. (2018). Mechanistic modeling Statistical modeling Definition Seeks to establish a mechanistic relationship between inputs and outputs Seeks to establish statistical relationships between inputs and outputs Pros and cons Presupposes and investigates causal links between the variables Looks for patterns and establishes correlations between variables Capable of handling small datasets Requires large datasets Once validated, can be used as a predictive tool in new situations possibly difficult to access through experimentation Can only make predictions that relate to patterns within the data supplied Difficult to accurately incorporate information from multiple space and time scales due to constrained specifications Can tackle problems with multiple space and time scales thanks to flexible specifications Evaluated on closeness to data and ability to make sense of it Evaluated based on predictive performance Mechanistic and statistical models are not perfectly exclusive and rather form the two ends of a spectrum. The definitions and classification of some examples is therefore still partly personal and arbitrary. For instance, the example in 1.6B can be transformed into a model with a more ambiguous status: \\[logit(P[Y=1])=\\beta_1X_1 + \\beta_{23}X_2X_3\\] This model is deliberately ambiguous. As a logistic model, it is therefore naturally defined as a statistical model. But the definition of the interaction between \\(X_2\\) and \\(X_3\\) denotes a mechanistic presupposition. The very choice of a logistic and therefore parametric model could also result from a knowledge of the phenomenon, even if in practice it is often a default choice for a binary output. Finally, the nature of the parameters \\(\\beta_{1}\\) and \\(\\beta_{23}\\) is likely to change the interpretation of the model. If they are deduced from the data and therefore optimized to fit Y as well as possible, one will think of a statistical model whose specification is nevertheless based on knowledge of the phenomenon. On the other hand, one could imagine that these parameters are taken from the biochemistry literature or other data. The model will then be more mechanistic. The boundary between these models is further blurred by the different possibilities of combining these approaches and making them complementary (Baker et al. 2018; Salvucci, Rahman, et al. 2019). 1.2.2 A tale of prey and predators The following is a final general illustration of the concepts and procedures introduced with respect to statistical and mechanistic models through a famous and characteristic example: the Lotka-Volterra model of interactions between prey and predators. This model was, like many students, my first encounter with what could be called mathematical biology. The Italian mathematician Vito Volterra states this system for the first time studying the unexpected characteristics of fish populations in the Adriatic Sea after the First World War. Interestingly, Alfred Lotka, an American physicist deduced the exact same system independantly, starting from very generic process of redistribution of matter among the several components derived from law of mass action (Knuuttila and Loettgers 2017). A detailed description of their works and historical formulation can be found in original articles (Lotka 1925; Volterra 1926) or dedicated reviews (Knuuttila and Loettgers 2017). The general objective is to understand the evolution of the populations of a species of prey and its predator, reasonably isolated from outside intervention. Here we will use Canada lynx (Lynx canadensis) and snowshow hare (Lepus americanus) populations for which an illustrative data set exists (Hewitt 1917). In fact, commercial records listing the quantities of furs sold by trappers to the Canadian Hudson Bay Company may represent a proxy for the populations of these two species as represented in Figure 1.7A. Denoting the population of lynx \\(L(t)\\) and the population of hare \\(H(t)\\) it can be hypothesized that prey, in the absence of predators, would increase in population, while predators on their own would decline in the absence of preys. A prey/predator interaction term can then be added, which will positively impact predators and negatively impact prey. The system can then be formalized with the following differential questions with all coefficients \\(a_1, a_2, b_1, b_2 &gt;0\\): \\[\\dfrac{dH}{dt}=a_1H-a_2HT\\] \\[\\dfrac{dL}{dt}=-b_1L+b_2HL\\] \\(a_1H\\) represents the growth rate of the hare population (prey), i.e., the population grows in proportion to the population itself according to usual birth modeling. The main losses of hares are due to predation by lynx, as represented with a negative coefficient in the \\(-a_2HT\\) term. It is therefore assumed that a fixed percentage of prey-predator encounters will result in the death of the prey. Conversely, it is assumed that the growth of the lynx population depends primarily on the availability of food for all lynxes, summarized in the \\(b_2HL\\) term. In the absence of hares, the lynx population decreases, as denoted by the coefficient \\(-b_1L\\). Important features of mechanistic models are illustrated here: the equations are based on a priori knowledge or assumptions about the structure of the problem and the parameters of the model can be interpreted. \\(a_1\\), for example, could correspond to the frequency of litters among hares and the number of offspring per litter. Figure 1.7: Some analyses around Lotka-Volterra model of a prey-predator system. (A) Evolution of lynx and hares populations based on Hudson Bay Company data about fur pelts. (B) and (C) Linear regression for estimation of parameters. (D) Evolution of lynx and hare populations as predicted by the model based on inferred parameters and initial conditions. This being said, the structure of the model having been defined a priori, it remains to determine its parameters. Two options would theoretically be possible: to propose values based on the interpretation of the parameters and ecological knowledge, or to fit the model to the data in order to find the best parameters. For the sake of simplicity, and because this example has only a pedagogical value in this presentation, we propose to determine them approximately using the following Taylor-based approximation: \\[\\dfrac{1}{y(t)} \\dfrac{dy}{dt} \\simeq \\dfrac{1}{y(t)} \\dfrac{y(t+1)-y(t-1)}{2}\\] By applying this approximation to the two equations of the differential system and plotting the corresponding linear regressions (Figures 1.7B and C), we can obtain an evaluation of the parameters such as \\(a_1=0.82\\), \\(a_2=0.0298\\), \\(b_1=0.509\\), \\(b_2=0.0129\\). By matching the initial conditions to the data, the differential system can then be fully determined and solved numerically (Figures 1.7D). Comparison of data and modeling provides a good illustration of the virtues and weaknesses of a mechanistic model. Firstly, based on explicit and interpretable hypotheses, the model was able to recover the cyclical behaviour and dependencies between the two species: the increase in the lynx population always seems to be preceded by the increase in the hare population. However, the amplitude of the oscillations and their periods are not exactly those observed in the data. This may be related to approximations in the evaluation of parameters, random variation in the data or, of course, simplifications or errors in the structure of the model itself. Besides, if one tries to carry out a statistical modeling of these data, it is very likely that it is possible to approach the curve of populations evolution much closer, especially for the hares. But should it be expressed simply as a function of time or should a joint modeling be proposed? The nature of the causal link between prey and predators will be extremely difficult to establish without strong hypotheses such as those of the mechanistic model. On the other hand, if populations in later years had to be predicted as accurately as possible, it is likely that a sufficiently well-trained statistical model would perform better. Finally, and this is a fundamental difference, the mechanistic model enables to test cases or hypotheses that go beyond the scope of the data. Quite simply, by playing with the variables or parameters of the model, we can predict the exponential decrease of predators in the absence of prey and the exponential growth of prey in the absence of prey. More generally, it is also possible to study analytically or numerically the bifurcation points of the system in order to determine the families of behaviours according to the relative values of the parameters (Flake 1998). It is not possible to infer these new or hypothetical behaviours directly from the data o of the statistical model. This is theoretically possible on the basis of the mechanistic model, provided that it is sufficiently relevant and that its operating hypotheses cover the cases under investigation. Now that the value of mechanistic models has been illustrated in a fairly theoretical example, all that remains is to explore in the next chapters how they can be built and used in the context of cancer. 1.3 Simplicity is the ultimate sophistication Before concluding this modeling introduction, it is important to highlight one of the most important points already introduced in a concise manner by the poet Paul Valéry at the beginning of this chapter. Whatever its nature, a model is always a simplified representation of reality and by extension is always wrong to a certain extent. This is a generally well-accepted fact, but it is crucial to understand the implications for the modeller. This simplification is not a collateral effect but an intrinsic feature of any model: No substantial part of the universe is so simple that it can be grasped and controlled without abstraction. Abstraction consists in replacing the part of the universe under consideration by a model of similar but simpler structure. Models, formal and intellectual on the one hand, or material on the other, are thus a central necessity of scientific procedure. (Rosenblueth and Wiener 1945) Therefore, a model exists only because we are not able to deal directly with the phenomenon and simplification is a necessity to make it more tractable (Potochnik 2017). This simplification appeared many times in the studies of frictionless planes or theoretically isolated systems, in a totally deliberate strategy. However, this idealization can be viewed in several ways [weisberg2007three]. One of them, called Aristotelian or minimal idealization, is to eliminate all the properties of an object that we think are not relevant to the problem in question. This amounts to lying by omission or making assumptions of insignificance by focusing on key causal factors only (Frigg and Hartmann 2020). We therefore refer to the a priori idea that we have of the phenomenon. The other idealization, called Galilean, is to deliberately distort the theory to make it tractable as explicited by Galileo himself: We are trying to investigate what would happen to moveables very diverse in weight, in a medium quite devoid of resistance, so that the whole difference of speed existing between these moveables would have to be referred to inequality of weight alone. Since we lack such a space, let us (instead) observe what happens in the thinnest and least resistant media, comparing this with what happens in others less thin and more resistant. This fairly pragmatic approach should make it possible to evolve iteratively, reducing distortions as and when possible. This could involve the addition of other species or human intervention into the Lotka-Volterra system described above. A three-species Lotka-Volterra model can however become chaotic (Flake 1998), and therefore extremely difficult to use and interpret, thus underlining the importance of simplifying the model. We will have the opportunity to come back to the idealizations made in the course of the cancer models but it is already possible to give some orientations. The biologist who seeks to study cancer using cell lines or animal models is clearly part of Galileo’s lineage. The mathematical or in silico modeler has a more balanced profile. The design of qualitative mechanistic models based on prior knowledge, which is the core of the second part of the thesis, is more akin to minimal idealization, which seeks to highlight the salient features of a system. But the Galilean pragmatism consisting in creating computationnaly-tractable models is also quite widespread, particularly in highly dimensional statistical approaches. Because of the complexity of the phenomena, simplification is therefore a necessity. The objective then should not necessarily be to make the model more complex, but to match its level of simplification with its assumptions and objectives. Faced with the temptation of the author of the model, or his reviewer, to always extend and complicate the model, it could be replied with Lewis Carrol words1: “That’s another thing we’ve learned from your Nation,” said Mein Herr, “map-making. But we’ve carried it much further than you. What do you consider the largest map that would be really useful?” “About six inches to the mile.” “Only six inches!” exclaimed Mein Herr. “We very soon got to six yards to the mile. Then we tried a hundred yards to the mile. And then came the grandest idea of all! We actually made a map of the country, on the scale of a mile to the mile!” “Have you used it much?” I enquired. “It has never been spread out, yet,” said Mein Herr: “the farmers objected: they said it would cover the whole country, and shut out the sunlight! So we now use the country itself, as its own map, and I assure you it does nearly as well.” Lewis Carroll, Sylvie and Bruno (1893) References "],
["cancer-as-deregulation-of-complex-machinery.html", "Chapter 2 Cancer as deregulation of complex machinery 2.1 What is cancer? 2.2 Cancer from a distance: epidemiology and main figures 2.3 Basic molecular biology and cancer 2.4 The new era of genomics 2.5 Data and beyond: from genetic to network disease", " Chapter 2 Cancer as deregulation of complex machinery Does not the entireness of the complex hint at the perfection of the simple? Edgar Allan Poe (Eureka) Armed with all these models, whether statistical or mechanistic, we are going to look at cancer, a particularly complex system that fully justifies their use Since the first chapter recalled how important prior knowledge of the phenomenon under study is for designing models, whatever their nature, this chapter will briefly summarize some of the most important characteristics of this disease before returning to the models themselves in the next chapter. Without aiming for exhaustiveness, and after an epidemiological and statistical description, we will focus on the most useful information for the modeller, i.e., the underlying biological mechanisms and available data. Figure 2.1: Cancer is an old disease. Rembrandt, Bathsheba at Her Bath, c. 1654, oil on canvas, Louvre Museum, Paris 2.1 What is cancer? Cancer can be described as a group of diseases characterized by uncontrolled cell divisions and growth which can spread to surrounding tissues. Descriptions of this disease, especially when associated with solid tumors, have been found as far back as ancient Egyptian documents, at least 1600 BC and we know from the first century A.D. with Aulus Celsus that it is better to remove the tumors and this as soon as possible (Hajdu 2011a). Progress will accelerate during the Renaissance with the renewed interest in medicine, and anatomy in particular, which will advance the knowledge of tumor pathology and surgery (Hajdu 2011b). The progress of anatomical knowledge has also left brilliant testimonies in the field of painting, which make the renown of the Renaissance today. The precision of these artists’ traits has also allowed some retrospective medical analyses, some of them going so far as to identify the signs of a tumor in some of the subjects of these paintings (Bianucci et al. 2018). Such is the bluish stain on the left breast of the Bathsheba painted by Rembrandt (Figure 2.1) which has been subject to controversial interpretations, sometimes described as an example of “skin discolouration, distortion of symmetry with axillary fullness and peau d’orange” (Braithwaite and Shugg 1983) and sometimes spared by photonic and computationnal analyses (Heijblom et al. 2014). The mechanisms of the disease only began to be elucidated with the appearance of the microscope in the 19th century, which revealed its cellular origin (Hajdu 2012a). The classification and description of cancers is then gradually refined and the first non-surgical treatments appear with the discovery of ionising radiation by the Curies (Hajdu 2012b). The 20th century is then the century of understanding the causes of cancer (Hajdu and Darvishian 2013; Hajdu and Vadmal 2013). Some environmental exposures are characterized as asbestos or tobacco. Finally, the biological mechanisms become clearer with the identification of tumor-causing viruses and especially with the discovery of DNA (Watson and Crick 1953). The foundations of our current understanding of cancer date back to this period, which marks the beginning of the molecular biology of cancer. It is this branch of biology that contains the bulk of the knowledge that will be used to build our mechanistic models, and it will be later detailed in Section 2.3. One of the ways to read this brief history of cancer is to see that theoretical and clinical progress has not followed the same timeframes.The medical and clinical management of cancers initially progressed slowly but surely, and this in the absence of an understanding of the mechanisms of cancer. Conversely, the theoretical progress of the last century has not always led to parallel medical progress, except on certain specific points. The interaction between the two is therefore not always obvious. The transformation of fundamental knowledge into medical and clinical impact is therefore of particular importance. This is what is called translational medicine, the aim of which is to go from laboratory bench to bedside (Cohrs et al. 2015). It is in this perspective that we will analyze the mechanistic models studied in this thesis. Their objective is to integrate biological knowledge, or at least a synthesis this knowledge, in order to transform it into a relevant clinical information. 2.2 Cancer from a distance: epidemiology and main figures Before going down to the molecular level, it is important to detail some figures and trends in the epidemiology of cancer today. Following the description in the previous section, cancer is first and foremost defined as a disease. Considered to be a unique disease, it caused 18.1 million new cancer cases and 9.6 million cancer deaths in 2018 according to the Global Cancer Observatory affiliated to World Health Organization (Bray et al. 2018). However, these aggregated data conceal disparities of various kinds. The first one is geographical. Indeed, mortality figures make cancer one of the leading causes of premature death in most countries of the world but its importance relative to other causes of death is even greater in the more developed countries (Figure 2.2). All in all, cancer is the first or second cause of premature death in almost 100 countries worldwide (Bray et al. 2018). These differences call for careful consideration of the impact of population age structures and health-related covariates. Figure 2.2: World map and national rankings of cancer as a cause of premature death. Classification of cancer as a cause of death before the age of 70, based on data for the year 2015. Original Figure, data and methods from Bray et al. (2018). A second disparity lies in the different types of cancer. If we classify tumors solely according to their location, i.e., the organ affected first, we already obtain very wide differences. First of all, the incidence varies considerably (Figure 2.3A)). Cancers do not occur randomly anywhere in the body and certain environments or cell types appear to be more favourable (Tomasetti and Vogelstein 2015). Mortality is also highly variable but is not directly inferred from incidence. Not all types of cancer have the same prognosis (Figure 2.3A and B) and survival rates (Liu et al. 2018). Although breast cancer is much more common than lung cancer, it causes fewer deaths because its prognosis is, on average, much better. The mechanisms at work in the emergence of cancer are therefore not necessarily the same as those that will govern its evolution or its response to treatment. And still on the response to treatment, Figure 2.3B highlights another disparity: not only are the survival prognosis associated with each cancer very different, but the evolution (and generally the improvement) of these prognoses has been very uneven over the last few decades. This means that theoretical and therapeutic advances have not been applied to all types of cancer with the same success. It is one more indication of the diversity of cancer mechanisms in different tissues and biological contexts, which make it impossible to find a panacea, and which, on the contrary, encourage us to carefully consider the particularities of each tumor, both to understand them and to treat them. Under a generic name and in spite of common characteristics, the cancers thus appear as extremely heterogeneous. And to understand the sources of this heterogeneity, it is necessary to consider the disease on a smaller scale. Figure 2.3: Incidence, mortality and survival per cancer types. (A) World incidence and mortality for the 19 most frequent cancer types in 2018, expressed with age-standardized rates (adjusted age structure based on world population); data retrieved from Global Cancer Observatory. (B) Evolution of 5-years relative survival for the same cancer types based on US data from SEER registries in 1975-1977 and 2006-2012; data retrieved from Jemal et al. (2017). 2.3 Basic molecular biology and cancer If it is not possible and desirable to summarize here the state of knowledge about the biology of cancer, we are going to give a very partial vision focused on the main elements used in this thesis, thus aiming to make it a self-sufficient document. The details necessary for a finer and more general understanding can be found in dedicated textbooks such as Alberts et al. (2007) and Weinberg (2013). 2.3.1 Central dogma and core principles Some of the principles that govern biology can be described at the level of one of its simplest element, the cell. Let us consider for the moment a perfectly healthy cell. It must ensure a certain number of functions necessary for its survival and, if necessary, for its division/reproduction. These functions are encoded in its genetic information in the form of DNA, which is stable and shared by the different cells since it is defined at the level of the individual. Most biological functions, however, are not performed by DNA itself which remains in the nucleus of the cell. The DNA is thus transcribed into RNA, another nucleic acid which, in addition to performing some biological functions, becomes the support of the genetic information in the cell. The RNA is then itself translated into new molecules composed of long chains of amino acid residues and called proteins. They are the ones that execute most of the numerous cellular functions: DNA replication, physical structuring of the cell, molecule transport within the cell etc. A rather simplistic but fruitful way to understand this functioning is to consider it as a progressive transfer of biological information from DNA to proteins, which has sometimes been summarized as the central dogma of the molecular biology (2.4), first stated Francis Crick (Crick 1970). Figure 2.4: Central dogma of molecular biology. Schematic representation of the information flow within the cell, from DNA to proteins through RNA, more precisely described in this video (Image credit Genome Research Limited). However, many changes would be necessary to clarify this scheme and the uni-directional nature was questioned early on. Above all, a large number of regulations interact with and disrupt this master plan. The genes are not always all transcribed, or at least not at constant intensities, interrupting or varying the chain upstream. This modulation in the transcription of genes can be induced by proteins, called transcription factors. After a gene transcription, its expression can still be regulated at various stages. RNAs can also be degraded more or less rapidly. RNAs can be reshaped in their structure by a process called splicing, which varies the genetic information they carry. Finally, proteins are subject to all kinds of modifications referred to as post-translational, which can change the chemical nature of certain groups or modify the three-dimensional structure of the whole protein. For instance, some proteins perform their function only if a specific amino acid residue is phosphorylated. In addition, these modifications can be transmitted between proteins, further complicating the flow of information. All these possibilities of regulation play an absolutely essential role in the life of the cell by allowing it to adapt to different contexts and situations. From the same genetic material, a cell of the eye and a cell of the heart can thus perform different functions. Similarly, the same cell subjected to different stimuli at different times can provide different responses because these molecular stimuli trigger a regulation of its programme. But all these regulatory mechanisms can be corrupted. 2.3.2 A rogue machinery With the above knowledge we can now return to the definition of cancer as an uncontrolled division of cells that can lead to the growth of a tumor that eventually spreads to the surrounding tissues. Therefore, this corresponds to normal processes, like cell division and reproduction, that are no longer regulated as they should be and are out of control. Experiments on different model organisms have gradually identified genetic mutations as a major source of these deregulations (Nowell 1976, Reddy et al. (1982)) until cancer was clearly considered as a genetic disease making Renato Dulbecco, Nobel Laureate in Medicine for his work on oncoviruses, say: If we wish to learn more about cancer, we must now concentrate on the cellular genome. (Dulbecco 1986). However, cancer is not a Mendelian disease for which it would be sufficient to identify the one and only gene responsible for deregulation. Indeed, the cell has many protective mechanisms. For example, if a genetic mutation appears in the DNA, it has a very high chance of being repaired by dedicated mechanisms. And if it is not repaired, other mechanisms will take over to trigger the programmed death of the cell, called apoptosis, before it can proliferate wildly. So a cancer cell is probably a cell that has learned to resist this cell death. Similarly, in order to generate excessive growth, a cell will need to be able to replicate itself many times. However, there are pieces of sequences on chromosomes called telomeres that help to limit the number of times each cell can replicate. A cancer cell will therefore have to manage to bypass this protection. Thus we can schematically define the properties that must be acquired by the cancereous cells in order to truly deviate the machinery. In an influential article, these properties were summarized in six hallmarks (Figure 2.5) which are: resisting cell death, enabling replicatve immortality, sustaning proliferative signaling, evading growth suppressors, activating invasion and inducing angiogenesis (Hanahan and Weinberg 2000). Two new ones were subsequently added in the light of advances in knowledge (Hanahan and Weinberg 2011): deregulating cancer energetics and avoiding immne destruction. The acquisition of these capacities generally requires many genetic mutations and is therefore favoured by an underlying genome instability. Figure 2.5: Hallmarks of cancer. The different biological capabilities acquired by cancer cells, as described in Hanahan and Weinberg (2000). Reprinted from Hanahan and Weinberg (2011). Each of these characteristics, or hallmarks, constitutes a research program in its own right. And for each one there are genetic alterations. These are tissue-specific or not, specific to a hallmark or common to several of them (Hanahan and Weinberg 2000). In any case, cancer can only result from the combination of different alterations that invalidate several protective mechanisms at the same time. This is often part of a multi-step process of hallmark acquisition that has been experimentally documented in some specific cases (Hahn et al. 1999) or more recently inferred from genome-wide data for human patients (Tomasetti et al. 2015). In summary, it appears that in order to study the functioning of cancer cells it is necessary to look at several mechanisms and to be able to consider them not separately but together, in as many different patients as possible. This ambitious programme has been made possible by a technological revolution. 2.4 The new era of genomics 2.4.1 From sequencing to multi-omics data In 2001, the first sequencing of the human genome symbolized the beginning of a new era, that of what will become high-throughput genomics (Lander et al. 2001; Venter et al. 2001). From the end of the 20th century, biological data started to accumulate at an ever-increasing rate (Reuter, Spacek, and Snyder 2015), feeding and accelerating cancer research in particular (Stratton, Campbell, and Futreal 2009; Meyerson, Gabriel, and Getz 2010). The ability to sequence the human genome as a whole, for an ever-increasing number of individuals, has enabled less biased and more systematic studies of the causes of cancer (Lander 2011). The number of genes associated with cancer increased drastically and some very important genes such as BRAF of PIK3CA have been identified (Davies et al. 2002; Samuels et al. 2004). Progress also extended to the gene expression data. Gene-expression arrays have made an important contribution by providing access to transcriptomic data (RNA), i.e., what has been transcribed from DNA and is therefore one step further in terms of biological information. This information has made it possible to further explore the differences betwween normal and tumor cells (Perou et al. 1999), or even to refine the classification of cancers, which until now has been done mainly according to the tumor site. Breast cancers are thus divided into subtypes with different combinations of molecular markers that facilitate the understanding of clinical behavior (Perou et al. 2000). One step further, we also note the appearance of prognostic gene signatures such as gene expression patterns correlated with the survival of patients (Van’t Veer et al. 2002). This revolution was then extended to other types of data such as proteins (proteomics), reversible modifications of DNA or DNA-associated proteins (epigenomics), metabolites (metabolomics) and others, each representing a perspective that can complement the others to better understand biological mechanisms, particularly in the case of diseases (Hasin, Seldin, and Lusis 2017). We have thus entered the era of multi-omics data (Vucic et al. 2012). 2.4.2 State-of-the art of cancer data With respect to cancer in particular, this wealth of data is particularly represented by a family of studies conducted by The Cancer Genome Atlas (TCGA) consortium, started in 2008 (Network and others 2008). Cohorts of several hundred patients are thus sequenced over the years for different types of cancer (Network and others 2012), resulting today in a total of 11,000 tumors from 33 of the most prevalent forms of cancer (Ding et al. 2018). Figure 2.6 provides a partial but striking overview of the depth of data available under this program. We can see the frequencies of alterations of certain groups of genes for a list of cancer types, making it possible to visualize the disparities already anticipated in section 2.2 based on patient survival. There are indeed important differences between the organs but also between the different subtypes associated with the same organ. And this representation only corresponds to one layer of data, that of genetic alterations. It could be used for transcriptomic, epigenomic or proteomic data, thus giving rise to an incredibly complex photography. Figure 2.6: Genetic alterations frequencies for cancer types from TCGA data. Frequencies of alteration per pahway and tumor types as summaried in Pan-cancer analyses from TCGA data. Reprinted from Sanchez-Vega et al. (2018). However, the diversity of data available for cancer research extends far beyond this, both in terms of technology and type of data. This may be data from model organisms such as mice or tumors of human origin made more suitable for experimentation. In the latter category, it is crucial to mention the huge amount of data available on cell lines, extracted from human tumors and transformed to be studied in culture. It is then possible to go beyond descriptive data and vary the experimental conditions in order to study the responses of these cells to perturbations and to enrich our knowledge. This provides an opportunity to know the response to more than 100 drugs of about 700 cell lines (Yang et al. 2012). The richness of these data, coupled with the omic profiling of each cell line, enables to study the determinants of response to treatment with unprecedented scope (Iorio et al. 2016). More recently, but following a similar logic, other types of inhibition screenings have been proposed based on a more specific technique called CRISPR-Cas9 (Behan et al. 2019). The simplicity of the cell lines in relation to the original tumors makes all these studies possible but sometimes hinders the clinical application of the knowledge acquired. For this reason, other types of biological models have been developed, including patient-derived xenografts (PDX) which is an implant of human tumors in mice to maintain the existence of a certain tumor microenvironment (Hidalgo et al. 2014), while maintaining drug screening possibilities (Gao et al. 2015). These two types of data, cell lines and PDX, have been used in this thesis, in addition to TCGA patient data, thus justifying the limitation of this presentation, which could otherwise be extended to other types of biological models. Similarly, other technologies are becoming increasingly important in the generation of cancer data, such as single-cell sequencing (Navin 2015), but will not be used in this work. 2.5 Data and beyond: from genetic to network disease All that remains to be done now is to make sense of all these data, to organize it, because cancer understanding does not flow directly from the abundance of data, and the ability to produce it may have been outpaced by the ability to analyze it (Stadler et al. 2014). A striking example is that of the prognostic signatures mentioned above. The many signatures or lists of genes proposed, even for the same cancer type, share relatively few genes, are difficult to interpret and their efficiency is sometimes poorly reproducible (Domany 2014). Even more surprisingly, most signatures composed of randomly selected genes were also found to be associated with patient survival (Venet et al. 2011). One of the main avenues for improving the interpretability of the data is the integration of the prior knowledge we have of the phenomena, especially in the case of cancer (Domany 2014). Figure 2.7: Simplistic representation of cellular circuitry. Normal cellular circuit sand sub-circuits (identified by colours) can be reprogrammed to regulate hallmark capabilities within cancer cells. Reprinted from Hanahan and Weinberg (2011). This a priori knowledge is in fact already present in Figure 2.6 since genetic alterations have been grouped in several categories called pathways. A pathway is group of biological entities, and associated chemical reactions, working together to control a specific cell function like apoptosis or cell division. The interest of these groupings may be understood based on the description of hallmarks. Indeed, if the “aim” of a cancer cell is to inactivate each of the protective functions, then it is more relevant to think not by gene but by function. Inactivating only one of the genes associated with the function may be sufficient and it is no longer necessary to inactivate the others. Numerous alterations in a large number of genes in various patients result often in the same key impaired pathways, like alterations of cell cycle or angiogenesis for instance (Jones et al. 2008). It is therefore possible to improve the stability and interpretability of analyses by moving from the gene scale to the pathway scale (Drier, Sheffer, and Domany 2013). More generally, the integration of biological knowledge often leads to improved performance in various cancer-related prediction tasks, either through the selection of variables or by taking into account the structure of the variables (Bilal et al. 2013; Ferranti, Krane, and Craft 2017). Increasingly, the biological variables are not interpreted separately but in relation to each other (Barabasi and Oltvai 2004). This is reflected in the emergence of more and more resources to summarize and represent signaling pathways and associated networks such as SIGNOR (Perfetto et al. 2016), OmniPath (Türei, Korcsmáros, and Saez-Rodriguez 2016) or the Atlas of Cancer Signaling Network (Kuperstein et al. 2015). Like other diseases, cancer then goes from a genetic disease to a network disease (Del Sol et al. 2010) and one can study how all kinds of genetic alterations affect the wiring of these networks (Pawson and Warner 2007), and modify the cellular functions leading to the previously described cancer hallmarks as depicted schmatically in Figure 2.7. In short, the richness of the data did not make it less necessary to use prior knowledge in order to make the analyses more interpretable and more robust. Figure 2.8: Genetic alterations frequencies from TCGA data mapped on a schematic signaling network. Frequencies of alteration per pathway and tumor types as summarized in Pan-cancer analyses from TCGA data. Reprinted from Sanchez-Vega et al. (2018). The final step, to obtain one of the most complete and integrated visions of cancer biology, is then to integrate omics knowledge with knowledge about the structure of pathways to try to understand in detail how their combinations can lead to so many cancers that are both similar and different. An example of such a representation is given by mapping the TCGA data about genetic alterations, presented in Figure 2.6, on a representation of the different pathways showing not only their internal organization but also their cross-talk (Sanchez-Vega et al. 2018). This representation is proposed in Figure 2.8 and is one the most recent and comprehensive view of the kind of tools and data available to the modeller who wants to dissect more deeply the mechanisms involved in cancer. References "],
["mechanistic-cancer.html", "Chapter 3 Mechanistic modeling of cancer: from complex disease to systems biology 3.1 Introducing the diversity of mechanistic models of cancer 3.2 Cell circuitry and the need for cancer systems biology 3.3 Mechanistic models of molecular signaling 3.4 From mechanistic models to clinical impact?", " Chapter 3 Mechanistic modeling of cancer: from complex disease to systems biology “How remarkable is life? The answer is: very. Those of us who deal in networks of chemical reactions know of nothing like it… How could a chemical sludge become a rose, even with billions of years to try.” George Whitesides The previous chapter identified the need to organize cancer knowledge and data. The integration of biological knowledge, particularly in the form of networks, is a first step in this direction. The deepening of knowledge, however, requires the ability to manipulate objects even more, to experiment, to dissect their behaviour in an infinite number of situations, such as the astronomer with his orrery or physicians with their old anatomical models (Figure 3.1). Is it then possible to create mechanistic models of cancer in the same way? Figure 3.1: Dissecting a biological phenomenon using a non-computational model. Rembrandt, The Anatomy Lesson of Dr Nicolaes Tulp, 1634, oil on canvas, Mauritshuis museum, The Hague 3.1 Introducing the diversity of mechanistic models of cancer Modeling cancer is not a new idea. And the diversity of biological phenomena involved in cancer has given rise to an equally important diversity of models and formalisms, which we seek here to give a brief overview in order to better identify the specific models that we will focus on later. One way to order this diversity is to consider the scales of these models (Figure 3.2). Indeed, cancer can be read at different levels, from the molecular level of DNA and proteins, to the cellular level, to the level of tissues and organisms (Anderson and Quaranta 2008). Models have been proposed at all these scales, using different formalisms (Bellomo, Li, and Maini 2008) and answering different questions. Consistent with the evolution of knowledge and data, the early models were at the macroscopic level. While methods and terminologies may have changed, there are nevertheless traces of these models as early as the 1950s. We then speak rather of mathematical modeling with a meaning that is nevertheless intermediate between what we have defined as mechanistic models and statistical models (Byrne 2010). First, the initiation of tumorigenesis was theorized with biologically-supported mathematical expressions in order to make sense of cancer incidence statistics (Armitage and Doll 1954, Knudson (1971)). These models, however, remained relatively descriptive in that they did not shed any particular light on the biological mechanisms involved and focused on gross characteristics of tumours. The integration of more advanced knowledge as well as the progressive refinement of mathematical formalisms has nevertheless allowed these models to proliferate while gaining in interpretability, with for instance mechanistic models of metastatic relapse (Nicolò et al. 2020). Always on a macroscopic scale, the study of tumor growth has also been the playground of many mathematicians [Araujo and McElwain (2004); byrne2010dissecting], even predicting invasion or response to surgical treatments using spatial modeling (Swanson et al. 2003). This line of research is still quite active today and provides a mathematical basis for comparison with tumour experimental growth (Benzekry et al. 2014). Figure 3.2: The different scales of cancer modeling. Cancer can be approached at different scales, from molecules to organs, using different data (dark blue), but often with the direct or indirect objective of contributing to the study of clinically interpretable phenomena (yellow boxes), in particular by studying the influence of anticancer agents (pale blue). Reprinted from Barbolosi et al. (2016). Taking it down a step further, it is also possible to model cancer at the cellular level, for example by looking at the clonal evolution of cancer (Altrock, Liu, and Michor 2015). The aim is then to understand the impact of the processes of mutation, selection, expansion and cohabitation of different populatons of cells, at specifc rates. The accumulation of a mutation in a population of cells can thus be studied (Bozic et al. 2010). Modeling at the cellular level is well suited to the study of interactions between cells, between cancer cells and their environment or with the immune system. Similar to other kinds of studies of population dynamics, formalisms based on differential equations are quite common (Bellomo, Li, and Maini 2008); but there are many other methods such partial differential equations or agent-based modeling (Letort et al. 2019). Finally, at an even smaller scale, it is possible to model the molecular networks at work in cells (Le Novere 2015). The aim is then to simulate mathematically how the different genes and molecules regulate each other, transmit information and, in the case of cancer, end up being deregulated (Calzone et al. 2010). These models will be the subject of the thesis and will therefore be defined more precisely and used to detail the concepts and tools of systems biology in the following sections. It can already be noted that while these models can integrate the most fundamental biological mechanisms of living organisms, one of the most burning questions is whether it is possible to link them to the larger scales that are clinically more interesting (tissues, organs etc.). Can these models tell us something about the molecular nature of cancer? About patient survival? Their response to treatment? These questions apply to all of the above models, whatever their scales (Figure 3.2), but are more difficult to answer for models defined at molecular scale that are further from the clinical data of interest. The aim of this thesis is to provide potential answers to these questions. One of the ways of approaching these issues has been to propose multi-scale models, which are nevertheless very complex (Anderson and Quaranta 2008; Powathil, Swat, and Chaplain 2015). We will focus here on the use of models defined almost exclusively at the molecular scale, which is assumed to be prominent, to study what can be inferred on the larger scales. 3.2 Cell circuitry and the need for cancer systems biology Most biological systems, and certainly cells, fall into the category of complex systems. These are systems made up of many interacting elements. While these systems can be found in many different scientific fields, the cell as a complex system is characterized by the diversity and multifunctionality of its constituent elements (genes, proteins, small molecules, enzymes), which nevertheless contribute to organized and a priori non-chaotic behaviour (Kitano 2002). Thus, the role of a protein such as the p53 tumour suppressor can only be understood by taking into account the interplay between its relationships with transcription factors and biochemical modifications of the molecule itself (Kitano 2002). In a cell, as in any complex system, the multiplication of components and interactions can make the response or behaviour of the system unexpected or unpredictable. Non-linear responses, such as abrupt changes in the state of a system, called critical transitions, can be observed in response to a moderate change in the signal (Trefois et al. 2015). Generally speaking, it is possible to observe emergent behaviours, i.e., behaviours of the system as a whole that were not trivially deducible from the individual behaviours of its components. This has been documented, through experiments and simulations, in the study of cell signalling pathways and the resulting biological decisions (Bhalla and Iyengar 1999; Helikar et al. 2008). These considerations have thus given rise to system-level or holistic approaches that aim to integrate data and knowledge into more comprehensive representations, often called systems biology. What is true for the cell in general is just as true for cancer in particular. Understanding the intertwining of signaling pathways is necessary to study their contributions to different cancer hallmarks, as shown in Figure 2.7. The concepts described above can thus be transposed to cancer systems biology (Hornberg et al. 2006; Kreeger and Lauffenburger 2010; Barillot et al. 2012). Indeed, it is often a question of understanding or predicting the impact of perturbations on cellular networks. Understanding how a single genetic mutation disrupts and reprograms networks, or even predicting the responses triggered by a drug on a presumably promising molecular target, makes little sense without integrated approaches. In addition, cancers are characterized by the accumulation of numerous mutations and alterations over time that must be considered concomitantly. These points of view of biologists and modellers reinforce the observation already made in the previous chapter of cancer as a network disease, as a system disease (Figure 2.8). Finally, to conclude this general presentation, it is important to understand that while small molecular network modeling is not recent, the rise and multiplication of wide range systems biology approaches is very much related to the production of biological data (De Jong 2002). The last few decades have seen the emergence of high-throughput data that have made it possible to identify and link hundreds of genes or proteins involved in cancer. Exploring the interaction and back and forth between these models and the data they use or predict is therefore of utmost importance. In addition, the now ** massive amount of data has also imposed mathematical or computational approaches as a central element in the management of this profusion** and more and more modeling approaches are focused on data integration or inference (Fröhlich et al. 2018; Bouhaddou et al. 2018). More generally, Figure 3.3 shows that while the number of scientific articles devoted to cancer has increased drastically since the 1950s (panel A), the proportion of these same articles mentioning models, networks or computational approaches has also increased (panel B), illustrating a change in paradigms. Figure 3.3: PubMed trends in cancer studies. (A) PubMed articles with the word Cancer in either title or abstract from 1950 to 2019. (B) Proportion of the Cancer articles with additional keywords expressed as PubMed logical queries. 3.3 Mechanistic models of molecular signaling Once the context has been defined, both biologically and methodologically, it is possible to begin the exploration of the models that will constitute the core of this thesis: the mechanistic models of molecular networks and signaling pathways. Before describing and illustrating some of the existing mathematical formalisms, it is possible to describe the common fundamental elements of this family of approaches. 3.3.1 Networks and data The first step is to identify the relevant biological entities from a question or system of interest (e.g. tumor suppressor genes, signaling cascades of proteins) and then to model their interactions, the regulatory relationships that link them. At this stage the model can generally be represented by a network but this word can cover different realities (Le Novere 2015). The simplest network just represents undirected interactions between entities, which therefore only establishes relationships and not causal mechanisms. But modeling requires more precise definitions, in particular concerning the direction of the interaction (is it A that acts on B or the opposite) and its nature (type of chemical reaction, activation/inhibition etc.). This is usually summarized as activity flows (or influence diagrams) with activation and inhibition arrows as in Figure 2.7 or Figure 3.5A. These arrows emphasize the transormation of static networks into dynamic objects that can be manipulated and interpreted mechanistically. This work can be taken further by writing bipartite graphs, known as process descriptions, which explicitly show the different states of each variable (first type of nodes), depending on their phosphorylation sate for instance, and the reactions that link them (second type of nodes) as in Figure 3.5B. A more precise description of these different representations and their meanings can be found in Le Novere (2015). Once the network structure of the model has been defined, it is possible write the corresponding mathematical formalism and potentially to refine certain parameters. Finally, the model is often confronted with new data to check its consistency with the biological behaviour studied or possibly make new predictions. However, all these steps are not linear and sequential, but rather iterative and cyclical. This modeling cycle, with back and forth to the data, is not specific to molecular network models, but it is possible to specify it in this case (Figure 3.4). The names of the key players involved in the question of interest are thus first extracted from adapted data or from the literature. A first mathematical translation of the relationships between the entities is then proposed before verifying the compatibility of this model with the observations, whether qualitative or quantitative. If the compatibility is not good, we come back to the definition or the parameterization of the model. If compatibility is correct, the model can be used to make new predictions or study phenomena that go beyond the initial data set. Ideally, these predictions will be tested afterwards. This cyclic approach with two successive checks is analogous to the use of validation and test data in the evaluation of most learning algorithms. This analogy can sometimes be masked by the qualitative nature of the predictions or by the lack of explicit fitting of the parameters. Figure 3.4: Modeling a biological network: an iterative and cyclical process. Reprinted from (Béal, Rémy, and Calzone 2020). A different and simpler version of this cycle is described in (Le Novere 2015). 3.3.2 Different formalisms for different applications Beyond these similarities in the construction and representation of models, the precise mathematical formalism that underlies them varies according to the type of question and the data (De Jong 2002). For the sake of simplicity, and without exhaustiveness, we propose to divide into quantitative and qualitative formalisms which will be essentially illustrated respectively by ordinary differential equation (ODE) models and logical (or Boolean) models for which a graphical and schematic comparison is proposed in Figure 3.5. Figure 3.5: Schematic example of logical and ODE modeling around MAPK signaling. (A) Activity flow diagram of a small part of MAPK signaling, each node representing a gene or protein, with an example of logical rule for MEK node for the corresponding logical model. (B) Process description of the same diagram with BRAF and CRAF merged in RAF for the sake of simplicity; each square representing a reaction and the correspondong rate; an example differential equation is provided for the phosphorylated (active) form of MEK. One of the most frequent approaches is the use of chemical kinetics equations to construct ODE systems which are a fairly natural translation of the process descritption networks described in the previous section (Polynikis, Hogan, and Bernardo 2009). Each biological interaction is treated as a reaction governed by the law of mass action and, under certain hypotheses, as a differential equation (Figure 3.5B); the set of reactions in the system then generates a set of differential equations with coupled variables, in an analogous way to the Lotka Volterra system presented in section 1.2.2. Thus the variables generally represent quantities of molecular species, for example concentrations of RNA or proteins, and the stoichiometric coefficients and reaction rates are used to define the system parameters. Approximations are sometimes made to simplify the equations, for example by assuming that they can be written as Michaelis-Menten’s enzymatic reactions, which have a simple and well known behaviour. However, the theoretical accuracy of quantitative models has a cost since each differential equation requires parameters, such as reaction constants or initial conditions, to which the system is very sensitive (Le Novere 2015). The biochemical interpretation of the parameters sometimes allow to find their value in the literature, if the reactions are well characterized, even if possible variations in a given biological or physical context are often unknown. Since knowledge of the values of these parameters is often limited or even non-existent, it may require a very large volume of data (including time series) to fit the many missing parameters which can be difficult if the number of parameters is large (Villaverde and Banga 2014). However, recent work has demonstrated the feasibility and scalability of this type of inference with sufficiently rich data (Fröhlich et al. 2018). At the same time, more qualitative approaches to modeling biological networks have been proposed with discrete variables linked together by rules expressed as logical statements (Abou-Jaoudé et al. 2016). These models are both more abstract since variables do not have a direct biological interpretation (e.g. concentration of a species) but are more versatile since they can unify different biological realities under the same formalism (e.g. activation of a gene or phosphorlation of a protein). The discrete nature of the variables can then be seen as an asymptotic case of the sigmoidal (e.g. Hill function) relationships often found in biology (Le Novere 2015). The step function thus obtained can keep a natural interpretation in the context of biological phenomena: genes activated or not, protein present or absent etc. Similarly, interactions between species are not quantified but are based on a qualitative statements (e.g. A will be active if B and C are active), drastically reducing the number of parameters (Figure 3.5A). If the theoretical interest of this formalism to study biological mechanisms was proposed quite early (Kauffman 1969; Thomas 1973), many concrete applications have also been developed over the years, particularly in cancer research (Saez-Rodriguez, Alexopoulos, et al. 2011a; Remy et al. 2015). This logical formalism will constitute the core of the work presented in Part II, where it will therefore be discussed in greater detail. Table 3.1: Features of quantitative and qualitative modeling applied to biological molecular networks (adapted from Le Novere (2015)) Quantitative modeling Qualitative modeling Example formalism Ordinary differential equation (ODE) models Logical models Type of variables Direct translation of biological quantities, usually continuous Abstract representation of activity levels, usually discrete Objective Quantitatively accurate and temporal simulation of an experimental phenomenon Coarse-grained simulation of qualitative phenotypes Advantages Direct confrontation with experimental data; precise; linear representation of time Faster design; easy translation of literature-based assertions; simulation of perturbations Drawbacks Difficulty determining or fitting parameters More difficult to link to data; lower precision These two formalisms, which are among the most frequent for modelling biological networks, share many similarities, in particular the propensity to be built according to bottom-up strategies based on knowledge of the elementary parts of the model, i.e., biological entities and reactions. However, they differ in their implementation and objectives, one aiming at the most accurate representation possible, the other seeking to capture the essence of the system’s dynamics in a parsimonious way (Table 3.1). The opposition is not irrevocable, as illustrated by the numerous hybrid formalisms that lie within the spectrum delimited by these two extremes such as fuzzy logic or discrete-time differential equations (Le Novere 2015; Calzone, Barillot, and Zinovyev 2018). To conclude, a comparison between the two approaches applied to the same problem is proposed by Calzone, Barillot, and Zinovyev (2018), studying the epithelio-mesenchymal transition (EMT, a biological process involved in cancer), to illustrate in concrete terms their complementarity. 3.3.3 Some examples of complex features With the help of these models, both qualitative and quantitative, many complex behaviours have been identified. Benefiting from the knowledge accumulated in the study of dynamic systems, a whole zoo of patterns with complex and non-intuitive behaviours such as non-linearities have been highlighted (Tyson, Chen, and Novak 2003). The MAPK pathway, coarsely described in Figure 3.5, and often simplified as a rather unidirectional cascade, shows switch or bistability behaviors generated by the complexity of its multiple phosphorylation sites (Markevich, Hoek, and Kholodenko 2004). These models have also been put at the service of understanding cancer and the erroneous decision-making by cells resulting from impaired signaling pathways. Thus, Tyson et al. (2011) summarize superbly well the complexity that can be hidden in the dynamics of smallest molecular networks as soon as they contain more than two entites and crossed regulations or feedback loops. Logical models have also made it possible to better dissect some complex phenomena at play in the cell such as emergent behaviours (Helikar et al. 2008) or mechanisms behind mutation patterns in cancer (Remy et al. 2015). 3.4 From mechanistic models to clinical impact? Mechanistic models have therefore undeniably led to a better understanding of the complex molecular machinery of signalling pathways. But beyond the interest that this understanding represents, do these models also have a clinical utility? In other words, are they of clinical or only scientific value? 3.4.1 A new class of biomarkers Throughout this thesis, the clinical value of mechanical models will often be analyzed by analogy to that of biomarkers. Throughout this thesis, the clinical value of mechanical models will often be analyzed by analogy to that of biomarkers. Biomarkers are usually defined as measurable indicators of patient status or disease progression, such as prostate-specific antigen (PSA) for prostate cancer screening or BRCA1 mutation for breast cancer risk (Henry and Hayes 2012). Biomarkers also encompass multivariate signatures that identify more complex patterns with clinical significance. Taking the logic even further, it was therefore proposed that mechanistic models, which also reveal complex molecular behaviours, could be considered as biomarkers, capturing perhaps even dynamic information (Fey et al. 2015). Like oncology biomarkers, the models will be divided into two categories according to their clinical objectives: prognostic models and predictive models (Oldenhuis et al. 2008). Prognostic biomarkers and models are those that provide information on the evolution of cancer independently of treatment. They are therefore generally confronted with survival or relapse data. The protein Ki-67 for example, encoded by the MKI67 gene, is known to be indicative of the level of proliferation and high levels of expression are thus associated with a poorer prognosis in many cancers (Sawyers 2008). Predictive biomarkers and models, on the other hand, give an indication of the effect of a therapeutic strategy. The simplest example, but not the only one, concerns biomarkers that are themselves the target of treatment: treatments based on monoclonal antibodies directed against HER2 receptors in breast cancer are only effective if the HER2 receptor has been detected in the patient (Sawyers 2008). Without attempting to be exhaustive, some logical and ODE models, with either prognostic or predictive claims, will be described. 3.4.2 Prognostic models One of the first mechanical models of cell signalling to have been explicitly presented as a prognostic biomarker is the one proposed by Fey et al. (2015) and describing c-Jun N-terminal kinase (JNK) pathway in neuroblastoma cells. A summary of the study is provided in Figure 3.6). The model is an ODE translation of the process description network of Figure 3.6)A, further determined and calibrated with molecular biology experimental data obtained using neuroblastoma cell lines. We thus observe the non-linear switch-like dynamics of JNK activation as a function of cellular stress (Figure 3.6)B). The precise characteristics of this sgmoidal response can, however, vary from one individual to another as captured by the network output descriptors \\(A\\), \\(K_{50}\\) and \\(H\\). Fey et al. proposed to perform neuroblastoma patient–specific simulations of the model, using patient gene expressions for ZAK, MKK4, MKK7, JNK and AKT genes to specify the initial conditions of the ODE system. Since JNK activation induces cell death through apoptosis, the patient-specific \\(A\\), \\(K_{50}\\) and \\(H\\) derived from patient-specifc models are then analyzed as prognostic biomarkers (Figure 3.6)C). Readers are invited to refer to the original article for details on model calibration or binarization of network descriptors (Fey et al. 2015). The authors also showed that in the absence of positive feedback from \\(JNK^{**}\\) to \\(^PMKK7\\), an important component of non-linearity, the prognostic value is drastically decreased. All in all, this pipeline from ODE model to survival curves, thus provides a paradigmatic example of the clinical interpretation of mechanistic models of molecular networks that will be reused in later chapters for illustration purposes. Other ODE models following a similar rationale have been proposed by the same group for colorectal cancer (Hector et al. 2012; Salvucci et al. 2017) or glioblastoma [Murphy et al. (2013); salvucci2019system]. Machine learning approaches have also been proposed to ease the clinical implementation of this kind of prognostic models by dealing with the potential lack of patient data needed to personalize them (Salvucci, Rahman, et al. 2019). Figure 3.6: Mechanistic modeling of JNK pathway and survival of neuroblastima patients, as described by Fey et al. (2015). (A) Schematic representation, as a process description, for the ODE model of JNK pathway. (B) Response curve (phosphorylated JNK) as a function of the input stimulus (Stress) and characterization of the corresponding sigmoidal function with maximal amplitude \\(A\\), Hill exponent \\(H\\) and activation threshold \\(K_{50}\\). (C) Survival curves for neuroblastoma patients based on binarized \\(A\\), \\(K_{50}\\) and \\(H\\); binarization thresholds having been defined based on optimization screening on calibration cohort. On the logical modeling side, there are also studies including prognostic value validation. Thus, Khan et al. (2017) proposed two logical models of epitelio-mesenchymal transition (EMT) in bladder and breast cancers. These models are inferred from prior mechanisms knowledge and data analysis with particular attention to potential feedback loops. Using these models, it is possible to study the behaviour of them for all combinations of model inputs (growth factors and receptor proteins) and derive subsequent signatures for good or bad prognosis. These signatures are later validated with cohorts of patients. In this case, the mechanistic model does not seek to capture a dynamic behavior but to facilitate and make understandable the exploration of combinations of input signals that grow exponentially with the number of inputs considered. Other formalisms, called pathway activity analysis and following the same activity flows principles (Figure 3.5A), have been analysed in the light of their prognostic value. Their greater flexibility enables the direct use of networks of several hundred or thousands of genes, such as those present in the KEGG database (Kanehisa et al. 2012). The benefit of mechanistic modeling is then to organize high-dimensional data and to facilitate the a posteriori analysis of the results. 3.4.3 Predictive models But the explicit representation of biological entities in mechanistic models makes them particularly suitable for the study of well-defined perturbations such as drug effects. Indeed, by assuming that the mechanism of action of a drug is at least partially known, it is possible to integrate this mechanism into the model if it contains the target of the drug (Figure 3.7). One can therefore simulate the effect of one drug or even compare several. These strategies have already been implemented in a qualitative way with logical models used to explain resistance to certain treatments of breast cancer (Zañudo, Scaltriti, and Albert 2017) or even highlight the synergy of certain combinations of treatments in gastric cancer (Flobak et al. 2015). The value of these models, however, is more scientific than clinical in that they focus on a single cell line or a restricted group of cell lines. The possibility to personalize the predictions or recommendations for different molecular profiles of cell lines or patients is therefore not obvious. Still within the context of logical formalism, Knijnenburg et al. (2016) proposed a broader approach: if their model needs to be trained, it can nevertheless provide an analytical framework for several hundred cell lines, while remaining within the scope of the training data to ensure the validity of predictions. Figure 3.7: Network model of oncogenic signal transduction in ER+ breast cancer, including some drugs and their targets. Reprinted from Zañudo, Scaltriti, and Albert (2017). Conceptually comparable strategies can be found on the side of differential equations where large mechanical models of cell signalling are also trained to predict the response to different treatments (Bouhaddou et al. 2018; Fröhlich et al. 2018). A calibrated model can then predict the response to a combination of treatments not tested in the training data, thereby proving the ability of mechanistic models to extend their predictive value beyond the data (Fröhlich et al. 2018). As with prognostic models, mechanical approaches other than logical formalisms and ODEs have been proposed and validated (Jastrzebski et al. 2018). What can be learned from these predictive models is that they require significant training data to be able to go beyond qualitative predictions and dissect treatment response mechanisms of many cell lines simultaneously. For obvious practical and ethical reasons, the validation of these models is for the moment limited to preclinical data since they require data for many uncertain therapeutic interventions. This first bridge between mechanistic models of cell signalling and clinical applications concludes this introductory part. The next part will be devoted to the definition of new methods to establish this connection based on logical formalism, before the third part proposes a more statistical evaluation of the prognostic and predictive values of the models presented in the previous parts. References "],
["logical-modeling-principles-and-data-integration.html", "Chapter 4 Logical modeling principles and data integration 4.1 Logical modeling paradigms for qualitative description 4.2 The MaBoSS framework for logical modeling 4.3 Data integration and semi-quantitative logical modeling", " Chapter 4 Logical modeling principles and data integration Je suis l’halluciné de la forêt des Nombres. Ils me fixent, avec leurs yeux de leurs problèmes ; Ils sont, pour éternellement rester : les mêmes. Primordiaux et définis, Ils tiennent le monde entre leurs infinis ; Ils expliquent le fond et l’essence des choses, Puisqu’à travers les temps planent leurs causes. Émile Verhaeren (Les nombres) Another way of ordering the diversity of mechanistic models presented above is to consider their relationship to biological data. Those that make little use of these data are essentially theoretical scope models that describe the general functioning of signaling pathways and associated systems (Calzone et al. 2010). Other models propose more quantitative models but require much more data, either from databases or experimental data generated for this purpose in order to fit the parameters. In the latter case, the necessary data is usually perturbation data: how does my system react to this or that inhibition or activation? For a single cell line this already corresponds to a large amount of data (Razzaq et al. 2018). And if we want to extend these approaches to many cell lines, the amount of data becomes massive (Fröhlich et al. 2018). For patient-specific models, access to this perturbation data is even more difficult. Between theoretical models that are not very demanding in terms of data but not very applicable clinically and models with a clinical focus but very demanding in terms of data, an intermediate alternative is missing. Can patient-specific mechanistic models be developed that would provide qualitative clinical interpretation with a small amount of data, accessible even in patients? In this part, composed of three chapters, a middle way will be described to answer positively to this question. This methodology will be based on a historically qualitative mathematical formalism already presented in the previous chapter under the name of logical modeling. Logical modeling in general will be detailed in this chapter before describing an original personalized approach in the next two chapters. Scientific content This chapter presents the theoretical bases of logical modeling and the tools used thereafter. It does not present any original work but refers to the synthesis and analyses of logical modeling as described in Béal et al. (2019) and Béal, Rémy, and Calzone (2020). 4.1 Logical modeling paradigms for qualitative description Mathematical models serve as tools to answer a biological question in a formal way, to detect blind spots and thus better understand a system, to organize, into a consensual and compact manner, information dispersed in different articles. In the light of this definition, logical formalism (also called Boolean) may seem one of the closest to natural language in that it can translate quite directly the statements present in the literature such as “protein A activates protein B” or “the expression of gene C requires the joint presence of factors D and E”. Indeed, shortly after the first descriptions of control circuits by Jacob and Monod (1961), the interest of logical models to describe biological systems was put forward by Kauffman (1969) and Thomas (1973). Since then, studies have multiplied (Thomas and d’Ari 1990), varying the fields of biological applications and also the mathematical and computational implementations (Naldi, Hernandez, Levy, et al. 2018). The two subsections below summarize the characteristics common to most of the logical formalisms, before detailing the implementation chosen in this thesis in section 4.2. A review of the use of data in logic models will finally be proposed in section 4.3. 4.1.1 Regulatory graph and logical rules A logical model is based on a network called regulatory graph (Figure 4.1), where each node represents a component (e.g. genes, proteins, complexes, phenotypes or processes), and is associated with discrete levels of activity (\\(0\\), \\(1\\), or more when justified). The use of a discrete formalism in molecular network modeling relies on the highly non-linear nature of regulation, and thus on the existence of a regulatory threshold. Assuming that each variable represents a level of expression, it will take the value \\(0\\) if the level of expression of the entity is below the regulation threshold, i.e., insufficient to carry out the regulation; and the value \\(1\\) if it is above the threshold and regulation is possible. In other words, the control threshold discretizes the state space, here the expression levels. It is therefore possible to distinguish several thresholds for the same variable, corresponding to distinct controls that do not take place at the same expression levels. The variable is then multivalued. This extension greatly enriches the formalism, because it allows to distinguish situations that are qualitatively different and that would be confused with Boolean variables. In the continuation of this thesis, we will consider by default that the activity levels are binary, \\(0\\) corresponding to an inactive entity and \\(1\\) to an active entity. The edges of this regulatory graph correspond to influences, either positive or negative, which illustrate the possible interactions between two entities (Figure 4.1). Positive edges can represent the formation of active complexes, mediation of synthesis, catalysis, etc. and they will be later depicted as green arrows (\\(\\leftarrow\\)). Negative edges on the other hand can represent inhibition of synthesis, degradation, inhibiting (de)phosphorylation, etc. and they will be depicted as red turnstile (\\(\\vdash\\)). Figure 4.1: A simple example of a logical model. Regulatory graph on the left with positive (green) and negative regulations (red); a set of possible corresponding logical rules on the right. Then, each node of the regulatory graph has a corresponding Boolean variable associated to it. The variables can take two values: \\(0\\) for absent or inactive (OFF), and \\(1\\) for present or active (ON). These variables change their value according to a logical rule assigned to them. The state of a variable will thus depend on its logical rule, which is based on logical statements, i.e., on a function of the node regulators linked with logical connectors AND (\\(\\&amp;\\)), OR (\\(|\\)) and NOT (\\(!\\)). These operators can account for what is known about the biology behind these edges. If two input nodes are needed for the activation of the target node, they will be linked by an AND gate; to list different means of activation of a node, an OR gate will be used; and negative influences will rely on NOT gates. The rules corresponding to the toy model in Figure 4.1 could be interpreted literally like this: A is acivated to 1 if B is active; B is updated to 1 in the absence of A and the presence/activity of C; C is an input of the model and therefore not regulated. It can be noted that the logical rules cannot be deduced only from the regulatory graph, which can be less precise and ambiguous. One could thus imagine that B is activated if C is, OR if A is not, thus changing the behavior of the model. 4.1.2 State transition graph and updates In a Boolean framework, the variables associated to each node can take two values, either \\(0\\) or \\(1\\). We define a model state as a vector of all node states. All the possible transitions from any model state to another are dependent on the set of logical rules that define the model. These transitions can be viewed into a graph called a state transition graph (STG), where nodes are model states and edges are the transitions from one model state to another. STG nodes will be later depicted with rounded squares instead of circles in order to emphasize the difference with regulatory graphs. That way, trajectories from an initial condition to all the final states can be determined. In a model with \\(n\\) nodes, the STG can contain up to \\(2^n\\) model state nodes; thus, if \\(n\\) is too big, the construction and the visualization of the graph becomes difficult. Figure 4.2: State transition graph and synchronous updates. Stable state (A) and limit cycle (B) attractors obtained for the example logical model with synchronous updates (all possible updates simultaneously). Figures above/below STG edges correspond to the number of nodes updated in each transition. Based the simple logical model of Figure 4.1 it is nevertheless possible to represent the STG comprehensively. The idea for this is to start from a state of the system and track the successive states defined by the logical rules and the corresponding updates. The first strategy to construct this STG is to change simultaneously at each time step all the variables that can be changed (Figure 4.2). This method is referred to as a synchronous updating strategy. In the second method, referred to as a asynchronous updating strategy, variables are changed one at a time (Figure 4.3) and therefore each state has as many successors as there are components whose state must be changed according to logical rules (Figure 4.3A). The latter asynchronous method will be used exclusively in the work presented thereafter. Figure 4.3: State transition graph and asynchronous updates. Stable state (A) and limit cycle (B) attractors obtained for the example logical model with asynchronous updates (one update at a time). Figures above/below STG edges correspond to the number of nodes updated in each transition. We then define attractors of the model as long-term asymptotic behaviors of the system. Two types of attractors are identified: stable states, when the system has reached a model state whose successor in the transition graph is the model state itself; and cyclic attractors, when trajectories in the transition graph lead to a group of model states that are cycling. For both synchronous and asynchronous updating strategies, the toy model shows the existence of two types of attractors: a stable steady state and a limit cycle, depending on the initial value of \\(C\\). There are two disconnected components of the STG for this example that correspond to the two possible values for the input \\(C\\). If \\(C\\) is initially equal to 0 (inactive), then there exists only one stable state: \\(A=B=C=0\\). All the trajectories in the state transition graph lead to a single final model state. If \\(C\\) is initially equal to 1, then the attractor is a limit cycle. The path in the STG cycles for any initial model state of this connected component. Note that for the asynchronous and synchronous graphs, the precise paths or limit cycles may differ. To conclude, it is important to emphasize and illustrate the characteristics of asynchronous updates in this toy example. In Figure 4.3A, the transition from the initial state (\\(A=C=0;B=1\\)) suggests two distinct possibilities, so it is necessary to define additional rules or heuristics to choose between possible transitions. We will come back to this by specifying the logical modeling implementation chosen in this thesis in section 4.2. 4.1.3 Tools for logical modeling Numerous tools have been developed to build logical models and study the dynamics of the systems under investigation, each with its own specificity. They allow, for example, to represent regulation networks; to edit, modify or infer logical rules; to identify stable states; to reduce models; to visualize graphs of synchronous or asynchronous transitions. Some also allow to integrate temporal data; to discretize expression data; to simulate the model stochastically or to integrate delays; to identify existing models, etc. Among them, we can cite GINsim (Naldi, Hernandez, Abou-Jaoudé, et al. 2018), BoolNet (Müssel, Hopfensitz, and Kestler 2010), pyBoolNet (Klarner, Streck, and Siebert 2016), BooleanNet (Albert et al. 2008), CellCollective (Helikar et al. 2012), bioLQM (Naldi 2018), MaBoSS (Stoll et al. 2012; Stoll et al. 2017), PINT (Paulevé 2017), CaspoTS (Ostrowski et al. 2016), or CellNOptR (Terfve et al. 2012). The interaction between all these tools, their interoperability and complementarity are highlighted in the form of a notebook jupyter (Naldi, Hernandez, Levy, et al. 2018), and some of them are described in more details in section 4.3. 4.2 The MaBoSS framework for logical modeling In the present study, all simulations have been performed with MaBoSS, a Markovian Boolean Stochastic Simulator whose design is summarized in Figure 4.4 and precisely described by Stoll et al. (2012) and Stoll et al. (2017). This framework is based on an asynchronous update scheme combined with a continuous time feature obtained with Gillespie algorithm (Gillespie 1976), allowing simulations to be continuous in time despite the discrete nature of logical modeling. Figure 4.4: Main principles of MaBoSS simulation framework and Gillespie algorithm. (A) A logical model with regulatory graph, logical rules and transition rates. (B) A corresponding state transition graph with two possible transitions in asynchronous update for a given initial state; each transition has an associated probability. (C) Random selection of a specific transition and time by the Gillespie algorithm from two uniform random variables. (D) Schematic representation of a logical model simulation with MaBoSS: average trajectory obtained from the mean of many individual stochastic trajectories. 4.2.1 Gillespie algorithm Gillespie algorithm provides a stochastic way to choose a specific transition among several possible ones and to infer a corresponding time for this transition. Thus, MaBoSS computation results in one stochastic trajectory as a function of time. To achieve this, transition rates seen as qualitative activation or inactivation rates, must be specified for each node (Figure 4.4A). They can be set either all to the same value by default, in the absence of any indication, or in various levels reflecting different orders of magnitude: post-translational modifications are quicker than transcriptions for instance. These transition rates are translated as transition probabilities in order to determine the actual transition (Figure 4.4B). Indeed, the probability for each possible transition to be chosen for the next update is the ratio of its transition rate to the sum of rates of all possible transitions. Higher rates correspond to transitions that will take place with greater probability, or in other words more quickly. Thus at each update, the Gillespie algorithm performs the procedure described in Figure 4.4C. Two uniform random variables \\(u\\) and \\(u&#39;\\) are drawn and used respectively to select the transition among the different possibilities (with \\(u\\)) and to infer the corresponding time (with \\(u&#39;\\)). Based on the described formula, time \\(\\delta t\\) follows an exponential law whose average is equal to the inverse of the sum of all possible transition rates (Figure 4.4C). In present work, except otherwise stated, all transition states will be initially assigned to 1. 4.2.2 A stochastic exploration of model behaviours Since MaBoSS computes stochastic trajectories, it is relevant to compute several trajectories in order to get an insight of the average behavior by generating a population of stochastic trajectories over the asynchronous state transition graph (Figure 4.4D). The aggregation of stochastic trajectories can also be interpreted as a description of an heterogeneous population. In fact, in all the examples in next chapters, all simulations have consisted thousands of computed trajectories. The larger the model, the larger the space of possibilities and the more trajectories are required to explore it. Since several trajectories are simulated, initial values of each node can be defined with a continuous value between 0 and 1 representing the probability for the node to be defined to 1 for each new trajectory. For instance, a node with a \\(0.6\\) initial condition will be set to \\(1\\) in \\(60\\%\\) of simulated trajectories and to \\(0\\) in \\(40\\%\\) of the cases. In the present work, we will focus on the asymptotic state of these simulations instead of transient dynamics and we will call node scores the asymptotic agregated score obtained by averaging all trajectories at a given final time point. Indeed, asymptotic states are more closely related to logical model attractors than transient dynamics and are therefore less dependent on updating stochasticity and more biologically meaningful (Huang, Ernberg, and Kauffman 2009). Note that the simulation time should be chosen carefully to ensure that the asymptotic state is achieved, and the term final state may be considered as safer. All in all this modeling framework is at the intersection of logical modeling and continuous dynamic modeling. If the definition of time remains rather abstract and difficult to interpret experimentally, the stochastic exploration of trajectories makes it possible to refine the purely binary interpretation of the variables. 4.2.3 From theoretical models to data models? To sum up, logical formalism makes it possible to design fairly quickly and easily models that reflect a priori knowledge of the phenomena being studied. Thus, they allow answering questions for which there is little information on the precise mechanisms involved in a disease or when there is a lack of data related to the expression of genes or the quantity of key proteins, or on the speed of certain processes. Logical models can confirm that a network is a good illustration of the underlying biological question. However, in order to propose a patient-specific mechanistic approach, it seems crucial to use the biological data available. How is this possible in a formalism that is by definition quite abstract? 4.3 Data integration and semi-quantitative logical modeling The higher level of abstraction of the logical formalism sometimes makes the necessary back and forth between theoretical modeling and experimental or clinical data less easy. However, many theoretical approaches have been developed over the years to enable this dialogue at all stages, from the construction to the validation of a logical model, as summarized in Figure 4.5. This section summarizes some of these approaches to show how the use of biological data enriches logical models and brings them closer to clinical applications in precision medicine. The purpose of this presentation is also to better contextualize the original approach presented in the following chapter. It should be noted that the methods presented below are all applicable to logical models, and illustrated with such examples where possible. However, some methods are not specific to this formalism and can be applied to other modeling frameworks. Figure 4.5: Data integration in logical modeling. The main types of data used are shown on the left; the essential steps of the logical modeling are shown linearly on the right. 4.3.1 Build the regulatory graph Faced with a biological question (Figure 4.5, first step), it is crucial to identify the main actors in the process in order to define the outline of the model (Figure 4.5, second step). A first approach relies on the existing scientific literature on the topic: which biological species and which interactions have been identified as relevant to my problem? In a more automatic way, it is possible to extract information from different databases in order to establish an initial list of biological entities and interactions associated with a biological phenomenon or even a gene of interest (Kanehisa et al. 2012; Perfetto et al. 2016). As an example, starting from the study of E2F1 gene as the hub of many regulatory mechanisms, Khan et al. (2017) have reconstructed a dense network of interactions in the vicinity of E2F1, which will be used for the construction of their subsequent model. The main difficulty here is to choose and select the relevant biological information adapted to the context of the model to be created, depending for example on the type of cancer studied or the desired level of precision. But if the literature can be considered as processed data, it is also possible to use directly experimental data related to the problem under study. Key actors of biological processes identified by statistical analysis, such as differentially expressed genes or the most frequently mutated genes in a patient cohort, are selected and used as a starting point for the construction of the model (Remy et al. 2015). More comprehensive approaches can use differential analysis tools on signaling pathways, rather than individual genes, to choose the relevant processes to include by contrasting different groups of patients based on their grades, metstatic status, resistance to treatments etc. (Martignetti et al. 2016; Montagud et al. 2017). Similarly, the study of regulatory networks involving transcription factors may justify the use of ChIP-seq data to identify possible new transcriptional regulations not previously listed (Collombet et al. 2017). Once the main actors have been identified, it is necessary to infer the links between them (Figure 4.5, third and fourth step). However, starting from a list of genes and proteins of interest, how can we ensure that the regulatory relationships are complete and relevant? While a careful reading of the literature can provide locally interesting information, the use of omics data is also a resource that can be declined to different levels of precision. The major interest of these methods, assuming that the data are adequate and sufficiently massive, is to be able to extract information as large as the dataset, potentially on hundreds of entities, and above all specific to the object of study: a cancer subtype or a particular cell line can thus generate their own interaction network (Lefebvre et al. 2010). Inference methods extract biological knowledge hidden in large databases, summarize it and represent it via networks. Many methods construct coexpression networks, which are non-oriented graphs, with different metrics and methods (Margolin et al. 2006; Vert, Qiu, and Noble 2007). Other approaches seek to infer causal relations between components, allowing the reconstruction of directed graphs where the links between entities are oriented, and sometimes even signed as activating (positive) or inhibiting (negative) regulations. These methods often make use of time series (Hill et al. 2016) or perturbation data (Meinshausen et al. 2016), but also more recently from observational data (Verny et al. 2017). The information extracted from the data is then directly readable in the form of activity flows as described in the SBGN standards (Novère et al. 2009), thus providing a representation adapted to the construction of qualitative models and a fortiori of logical models (Le Novere 2015). Closer to the objective of defining logical models, certain methods allow the study and inference of co-regulation expressed with logical operators (Elati et al. 2007), thus facilitating the passage from the definition of an interaction network to the construction of a true logical model. 4.3.2 Define the logical rules Precision must then be taken further by defining the logical rules that complete the network (Figure 4.5, fifth step). The first source of aggregated data to define logical rules is the scientific literature. The modeler looks for the state of knowledge on a given regulatory mechanism and translates it into a local logical rule, according to the desired level of precision. For example, it has been observed that the protein kinase AKT can stabilize the oncogene MDM2 by phosphorylation, which leads to the degradation of p53 by forming a complex with it: this example can be translated by a simple inhibition relationship of AKT on p53 if this level of precision is considered sufficient or else intermediate species such as MDM2 can be used (Cohen et al. 2015). Then, the effect of inhibition must be defined: can MDM2 alone inhibit p53 or does the presence of other activators outweigh this effect? This kind of considerations allows to define the logical combinations between the different inputs of a network node. In some cases, experimental data can be used to answer such questions: is a single activator sufficient or is the presence of all activators necessary? Which of the activator or inhibitor prevails in the case of simultaneous presence? While this information is often found in the literature, one should generate one’s own experimental data to ensure an answer tailored to the study context, using a variety of experimental molecular biology techniques. For example, in order to elucidate the relationship between Foxo1 and Cebpa in a model of differentiation of myeloid and lymphoid cells, Collombet et al. (2017) first established the physical relationship between these species by ChIP-seq before determining the nature of this relationship using an ectopic expression experiment of Foxo1 in macrophage cells. Other, more global approaches have been developed in recent years, driven by the influx of data from high-throughput sequencing techniques. Based on this rich and complex data, it has become possible to infer entire logical models, with precisely defined rules and interactions (Ostrowski et al. 2016). The algorithms CellNOpt (Terfve et al. 2012) and caspo (Videla et al. 2017) provide two examples of these approaches, and more recently the SCNS tool described a graphical interface to infer logical models from single cell data (Woodhouse et al. 2018). This model-inference goes beyond simpler structure-inference by defining the logical rules, but it is generally based on a predefined topological structure to which time series or perturbation data are added. These data provide access to the response dynamics of a system. By questioning the way the system reacts, these data are therefore richer than a snapshot and thus facilitate the transition from correlation to causality, and thus the inference of logical rules. In practice, the use of proteomic or phospho-proteomic data is often recommended because these data account for the activity of the protein and are in fact the closest to the cellular response (Ostrowski et al. 2016; Terfve et al. 2012; Terfve et al. 2015). In spite of the richness of this type of data, model inference is sometimes still an under-determined problem that can lead to a large number of models with different logical rules equally compatible with the data. In such situations, it is then a matter of choosing the model on the basis of biological relevance criteria or of accepting to use families of models instead of limiting oneself to a single model (Videla et al. 2017). In all cases, constructing logical rules directly from data specific to the problem can make it possible to obtain logical rules that are also specific to the context or the system under study (Saez-Rodriguez, Alexopoulos, et al. 2011b). For example, the inference of logical models specific to one or some cancer cell lines is a powerful tool to study their particularities (Razzaq et al. 2018). 4.3.3 Validate the model Finally, the data can be used to validate the biological or clinical relevance of the models (Figure 4.5, sixth step). Compared to a system of differential equations, logical modelling has the particularity of being more abstract and therefore less directly reliable to an experimental reality for its validation. A system of differential equations can be compared to the chemical kinetics of the biological system under study. Compared to continuous formalisms, the dynamics of logic model simulation is more difficult to take into account but it is possible to verify it qualitatively, for example by validating the cyclic nature of activation trajectories for a model simulating the cell cycle (Fauré et al. 2006) or cellular decisions as a function of the activation signal (Calzone et al. 2010). A second, more frequent approach consists in looking at the model’s steady states and associating them with physiological conditions (Weinstein et al. 2017, Cohen et al. (2015)). Finally, a third strategy focuses on the asymptotic state reached during the stochastic simulation of the model(s), a state representing a mixture of the different steady states according to the probability that the model has of reaching them. In many models, to facilitate the analysis, nodes representing phenotypes have been added as “read-out” of the activity of certain entities. Thus, if a model includes a node named Proliferation, it will then be simpler to draw interpretations from the simulations performed with the model that will be linked to experimental observations of tumor growth or cell proliferation (Grieco et al. 2013; Steinway et al. 2015). To validate these models, the activity of phenotypes when forcing some node activity to \\(0\\) or \\(1\\) is compared with the results of gene mutations reported in experiments carried out on mice or cell lines (Fauré et al. 2006; Cohen et al. 2015). Another similar method for validating the relevance of a logical model is based on the analysis of the effects of different therapeutic molecules. The mechanistic nature of logical modeling makes it relatively easy to simulate the effect of these molecules. It is possible to simulate the effect of an inhibitory molecule by forcing the activity of its target to 0 and to compare with data (Zañudo, Scaltriti, and Albert 2017; Iorio et al. 2016; Knijnenburg et al. 2016). Beyond validation, some studies have predicted new therapeutic targets based on logical models, for instance by pointing out weaknesses in the topology of a regulatory system (Sahin et al. 2009). Taking advantage of the ease of modeling and multiplying combinations of therapeutic molecules, logical modeling has also proved fruitful in predicting the best therapeutic combinations and their synergies, in the context of gastric cancers for example (Flobak et al. 2015). Experimental confirmation of the predictions resulting from the modeling is then the ultimate stage in the validation of a logical model, completing the fruitful round trip between models and data. References "],
["personalization-of-logical-models-method-and-prognostic-validation.html", "Chapter 5 Personalization of logical models: method and prognostic validation 5.1 From one generic model to data-specific models with PROFILE method 5.2 An integration tool for high-dimensional data?", " Chapter 5 Personalization of logical models: method and prognostic validation All happy families are alike; each unhappy family is unhappy in its own way. Leo Tolstoy (Anna Karenina, 1877) Now that logical modeling has been introduced, it is possible to come back to the question that structures this part and to refine it. Is it possible to use routine omics data to obtain logical models that provide qualitative clinical interpretation?. We thus propose a sequential approach, separating the model construction process from the integration of biological data. A generic logical model is first built, based on the literature knowledge, and the data are then used to specify the model. Indeed, the model as defined from the literature is often generic in the sense that it summarizes the state of knowledge on a probably heterogeneous pathology or population. Assuming that this general regulatory scheme provides a relevant framework for the system, it may then be relevant to use more precise omics data to impose biologically sourced constraints on the model: inactivation of a gene in a patient, activation of a protein or a signalling pathway by overexpression or phosphorylation, etc. This approach, called PROFILE (PeRsonalization OF logIcaL ModEls), allows the integration of both discrete (mutations) and continuous data (RNA expression levels, proteins) based on the MaBoSS software, and leads to specific models of a cell line or a patient. Scientific content This chapter presents the method developed during the thesis to personalize logical models, i.e., generate patient-specific models from a single generic one. The principles of the method and some analyses on patient data have been comprehensively described in Béal et al. (2019) and briefly summarized in Béal et al. (2020). Analyses on cell lines are unpublished. 5.1 From one generic model to data-specific models with PROFILE method The PROFILE method is summarized in Figure 5.1 and the different steps are successively described in the following subsections. Figure 5.1: Graphical abstract of PROFILE method to personalize logical models with omics data. On the one hand (upper left), a generic logical model, in a MaBoSS format is derived from literature knowledge to serve as the starting-point. On the other hand (upper right), omics data are gathered (e.g., genome and transcriptome) as data frames, and processed through functional inference methods (for already discrete genome data) or binarization/normalization (for continuous expression data). The resulting patient profiles are used to perform model personalization, i.e., adapt the generic model with patient data. The merging of the generic model with the patient profiles creates a personalized MaBoSS model per patient. Then, biological or clinical relevance of these patient-specific models can be assessed. 5.1.1 Gathering knowledge and data The first steps are therefore to build a logical model adapted to the biological question (Figure 5.1, upper left) and to collect omics data that will be used to personalize the model (Figure 5.1, upper right). The construction of the model can be based on literature or data (see previous chapter). In the latter case, the data used to build the model will preferably be distinct from those used to personalize the model. 5.1.1.1 A generic logical model of cancer pathways In this chapter, which is essentially methodological in nature, we will use a published logical model of cancer pathways to illustrate our PROFILE methodology. It is based on a regulatory network summarizing several key players and pathways involved in cancer mechanisms: RTKs, PI3K/AKT, WNT/\\(\\beta\\)-catenin, TGF-\\(\\beta\\)/Smads, Rb, HIF-1, p53 and ATM/ATR (Fumia and Martins 2013). The later analyses will be mainly focused on two read-out nodes, Proliferation and Apoptosis. Based on the model’s logical rules Proliferation node is activated by any of the cyclins (CyclinA, CyclinB, CyclinD, and CyclinE) and is, thus, an indicator of cyclin activity as an abstraction and simplification of the cell cycle behavior. Apoptosis node is regulated by Caspase8 and Caspase9. This generic model contains 98 nodes and 254 edges. Further details and visual representation are provided in section B.1 and Figure B.1. Model files are available in MaBoSS format in a dedicated GitHub repository. 5.1.1.2 Cancer data to feed the models In order to showcase the method, breast-cancer patient data are gathered from METABRIC studies (Curtis et al. 2012; Pereira et al. 2016). 1904 patients have data for both mutations, copy number alterations, RNA expression and clinical status (e.g. survival). This number rises to 2504 patients if we only look at the mutations. Additional analyses were also performed based on the smaller and clinically less complete TCGA breast cancer data (Network and others 2012). These are detailed in Béal et al. (2019) but not included in this thesis. A more comprehensive description of these two databases can be found in section A.3. In addition to these examples proposed in the original article, an application to cell line data is proposed in section 5.2.1 to link to the next chapters. A cohort of 663 cell lines from different types of cancer will be used. The data are from Cell Models Passpors (@ Meer et al. 2019) and are described in more detail in the appendix A.1. In all cases, samples and cell lines will sometimes be referred to as patients for the sake of simplicity. 5.1.2 Adapting patient profiles to a logical model Before describing precisely the methodologies for using the data to generate patent-specific models, it is important to understand that these data will need to be transformed. This is the transformation of raw omics data into processed profiles that can be used directly in logical modeling. 5.1.2.1 Functional inference of discrete data Since the logical formalism is itself discrete, the integration of discrete data is more straightforward. The most natural idea, used in many previous works, is to interpret the functional effect of these alterations and to encode it directly in the model. For instance, a deleterious mutation is integrated into the model by setting the corresponding node to \\(0\\) and ignoring the logical rule associated to it. For activating mutation, the node is set to \\(1\\). The main obstacle is therefore to estimate the functional impact of the alterations in order to translate them as well as possible in the model. For mutations, based on the variant classification provided by the data, inactivating mutations (nonsense, frame-shift insertions or deletions and mutation in splice or translation start sites) are assumed to correspond to loss of function mutations and therefore the corresponding nodes of the model are forced to \\(0\\). Then, missense mutations are matched with OncoKB database (Chakravarty et al. 2017): for each mutation present in the database, an effect is assessed (gain or loss of function assigned to \\(1\\) and \\(0\\), respectively) with a corresponding confidence based on expert and literature knowledge. Then, mutations targeting oncogenes (resp. tumor-suppressor genes), as defined in the 2020+ driver gene prediction method (Tokheim et al. 2016), are assumed to be gain of function mutations (resp. loss of function) and therefore assigned to \\(1\\) (resp. \\(0\\)). To rule potential passenger mutations out, each automatic assignment of a oncogene/tumor-suppressor gene muations requires that the effect of the mutation has been identified as significant by predictive software based on protein structure such as SIFT (Kumar, Henikoff, and Ng 2009) or PolyPhen (Adzhubei et al. 2010). For integration of copy number alterations, we use the discrete estimation of gain and loss of copies from GISTIC algorithm processing (Mermel et al. 2011). The loss of both alleles of a gene (labelled -2) can thus be interpreted as a 0. Conversely, a significant gain of copies (labelled +2) denotes a gene that tends to be more highly expressed although the interpretation is more uncertain. 5.1.2.2 Normalization of continuous data The integration of continuous data, such as RNA expression levels, in logical modeling is more difficult. The stochastic framework of MaBoSS provides however some possibilities. The main continuous mechanistic parameters of MaBoSS are the initial conditions of each node (its initial probability of being activated among the set of simulated stochastic trajectories) and the transition rates associated with the nodes (its probability to have its transition performed in an asynchronous update). In order to facilitate the use of continuous data through one of these two possibilities, we propose to transform them so that the values are continuous between 0 and 1, what we will refer to hereafter as normalized data. It is assumed that these continuous data can be good proxies of biological activity, 0 corresponding to a very low level of activity of the biological entity and 1 to a very high level. This assumption will have to be explained and justified each time: high level of expression of an RNA or significant phosphorylation of a protein interpreted as continuous markers of an important biological activity for example. Figure 5.2: Bimodal distribution of ERG gene in TCGA prostate cancer cohort. This bimodality is largely explained by the fusion status of ERG gene. Patients for whom the gene has fused with TMPRSS2 have a much higher level of RNA expression for ERG. One of the assumptions of our analysis is that the interpretation of continuous data can only be relative and not absolute. It is indeed difficult to define an absolute threshold of RNA level at which a gene will be considered as activated. This may depend on contexts, technologies or even the way in which the data have been processed. On the other hand, it is possible to estimate that a gene is over-expressed for a patient compared to a cohort of interest. In contrast, the effect of a mutation can be estimated more independently. Thus, the continuous data will be normalized for the whole cohort studied, for each gene individually. In order to retain biological information as much as possible, distribution patterns are identified and normalized in different ways (Figure 5.4). We will illustrate the process by taking the example of the expression data expressed with continuous RNA levels. Beforehand, genes with no variation in expression level or too many missing values are discarded from the analysis. Then, we seek to identify first the genes that have a bimodal distribution. Indeed, these naturally fit into a binary formalism and this bimodality often has an underlying biological explanation. As an example, in the TCGA prostate cancer cohort (used in section 6.3), a gene called ERG has a bimodal distribution when looking at RNA levels in all patients. This distribution is almost entirely explained by an underlying genetic alteration that is the fusion of the ERG gene with the TMPRSS2 gene promoter (Figure 5.2), which is very common in this cancer (Tomlins et al. 2005). In the data we identify bimodal patterns based on three distinct criteria: Hartigan’s dip test of unimodality, Bimodality Index (BI) and kurtosis. The dip test measures multi-modality in a sample using the maximum difference between empirical distribution and the best unimodal distribution, i.e., the one that minimizes this maximum difference (Hartigan and Hartigan 1985). Values below \\(0.05\\) indicate a significant multi-modality. In PROFILE, this dip statistic is computed using the R package diptest. The Bimodality Index (BI) evaluates the ability to fit two distinct Gaussian components with equal variance (Wang et al. 2009). Once the best 2-Gaussian fit is determined, along with the respective means \\(\\mu_1\\) and \\(\\mu_2\\) and common variance \\(\\sigma\\), the standardized distance \\(\\delta\\) between the two populations is given by \\[\\delta = \\dfrac{|\\mu_1-\\mu_2|}{\\sigma}\\] and the BI is defined by \\[BI=[p(1-p)]^{1/2}\\delta\\] where \\(p\\) is the proportion of observations in the first component. In PROFILE, BI is computed using the R package mclust. Finally, the kurtosis method corresponds to a descriptor of the shape of the distribution, of its tailedness, or non-Gaussianity. A negative kurtosis distribution, especially, defines platykurtic (flattened) distributions, and potentially bimodal distributions. It has been proposed as a tool to identify small outliers subgroups or major subdivisions (Teschendorff et al. 2006). In our case, we focus on negative kurtosis distributions to rule out non-relevant bimodal distributions composed of a major mode and a very small outliers’ group or a single outlier. Although Dip test, BI and negative kurtosis criteria emerge as similar tools in the sense that they select genes whose values can be clustered in two distinct groups of comparable size, we choose to combine them in order to correct their respective limits and increase the robustness of our method. For that, we consider that all three conditions (Dip test, Bimodality Index and kurtosis) must be fulfilled in order for a gene to be considered as bimodal. The thresholds of each test are inspired by those advocated in the papers presenting the tools individually. Dip test is a statistical test to which the classical \\(0.05\\) threshold has been chosen. In the article describing BI, authors explored a cut-off range between 1.1 and 1.5 and we chose \\(1.5\\) for the present work. Regarding kurtosis, the usual cut-off is \\(0\\), but since this criterion does not directly target bimodality, this criterion has been relaxed to \\(K &lt; 1\\). Several examples of the relative differences and complementarities between these criteria can be seen in Figure 5.3. Figure 5.3: Bimodality criteria and their combinations. Examples of gene expression distributions for the different combinations of bimodality criteria: Dip test, Bimodality Index (BI) and kurtosis (K). Plots are organized in a Venn diagram. Non-bimodal genes are further classified as unimodal or zero-inflated distributions, looking at the position of the distribution density peak (Figure 5.4A). Then, based on this three category classification of genes, a pattern-preserving normalization can be performed, as summarized in Figure 5.4B. For a bimodal gene \\(i\\), a 2-component Gaussian mixture model is fitted using mclust R package resulting in a lower mode \\(M_{i,0}\\) and an upper mode \\(M_{i,1}\\). Denoting \\(X_{i,j}\\) the expression value for gene \\(i\\) and sample \\(j\\), \\(X_{i,j}\\) has a probability to belong to \\(M_{i,0}\\) or \\(M_{i,1}\\) such as \\(P[X_{i,j} \\in M_{i,0}]+P[X_{i,j} \\in M_{i,1}]=1\\). For these bimodal genes, the normalization processing is defined as: \\[X_{i,j}^{norm}=P[X_{i,j}] \\in M_{i,1}\\] For unimodal distributions, we transform data through a sigmoid function in order to maintain the most common pattern which is unimodal and nearly-symmetric: \\[X_{i, j}^{norm}=\\dfrac{1}{1+e^{-\\lambda(X_{i, j}-median(X_{i}))}}\\] Since the slope of the function depends on \\(\\lambda\\), we adapt it to the dispersion of initial data in order to maintain a significant dispersion in \\([0, 1]\\) interval: more dispersed unimodal distributions are mapped with a gentle slope, peaked distributions with a steep one. We map the median absolute deviation \\(MAD(X_{i})=median(|X_{i}-median(X_i)|)\\) on both sides of the median respectively to \\(0.25\\) and \\(0.75\\) to ensure a minimal dispersion of the mapping. Thus, the proposed mapping results in: \\[\\lambda=\\dfrac{log(3)}{MAD(X_i)}\\] Last, zero-inflated distributions are transformed by linear normalization of the initial distribution: \\[X_{i, j}^{norm}=\\dfrac{X_{i, j}-min(X_{i})}{max(X_{i}-min(X_{i}))}\\] The transformation is applied to data between 1st and 99th quantiles to be more robust to outliers. Values outside this range are respectively assigned to \\(0\\) and \\(1\\). All the categoriation of distributions and the subsequent normalizations are summarized in Figure 5.4. With the help of the categories described here, it is also possible to binarize the continuous data quite simply. This binarization is required for some methods of network inference or logical modeling but will not be used in the examples presented beloww. Readers may refer to Béal et al. (2019) for more details. Figure 5.4: Normalization of continuous data for logical modeling. (A) Combinations of tests and criteria to classify distributions of continuous data (such as gene expression for one gene and all patients) as bimodal, unimodal or zero-inflated. (B) Normalization methods for each kind of distribution. 5.1.3 Personalizing logical models with patient data It is now possible to redefine more precisely the ways of integrating data into a logical model defined with MaBoSS, as sketched at the beginning of the previous section. Personalization is defined here as the specification of a logical model with data from a given patient: each patient has a personalized model tailored to his/her data, so that all personalized models are different specifications of the same logical model, using data from different patients (Figure 5.1). Based on MaBoSS formalism and the processed patient data, there are several possibilities to personalize a generic logical model with patient data. One possibility to have patient-specific models is to force the value of the variables corresponding to the altered genes in a given patient, i.e., constraining some model nodes to an inactive (\\(0\\)) or active (\\(1\\)) state (Figure 5.5A). In order to constrain a node to \\(0\\) (resp. \\(1\\)), the initial value of the node is set to \\(0\\) (resp. \\(1\\)) and \\(k_{0\\rightarrow 1}\\) (resp. \\(k_{1\\rightarrow 0}\\)) to \\(0\\) to force the node to maintain its initially defined state. For instance, the effect of a TP53 inactivating mutation can be modeled by setting the node p53 in the model and its initial condition to \\(0\\) and ignoring the logical rule of p53 variable. Because of the type of data used, this personalization method is referred to as discrete personalization. It has also been called strict node variants in Béal et al. (2019) because this data integration overwrites the logical rules. Figure 5.5: Methods for personalization of logical models. (A) Personalization with discrete data, such as mutations, with some nodes forced to \\(0\\) based on loss of function alteration (left) or \\(1\\) based on gain of function/constitutive activation (right). (B) Personalization with continuous data used to define the initial conditions of nodes and to influence the transitions rates and the subsequent probabilities of transition in asynchronous updates. Another possible strategy is to modify the initial conditions of the variables of the altered genes according to the results of the normalization (i.e., the probability of initial activation for one node among the thousands of stochastic trajectories). These initial conditions can capture different environmental and genetic conditions. Nevertheless, in the course of the simulation, these variables will be prone to be updated depending on their logical rules. Finally, as MaBoSS uses Gillespie algorithm to explore the STG, data can be mapped to the transition rates of this algorithm. In the simplest case, all transition rates of the model are set to \\(1\\), meaning that all possible transitions are equally probable. Alternatively, it is possible to separate the speed of processes by setting the transition rates to different values to account for what is known about the reactions: more probable reactions will have a larger transition rate than less probable reactions (Stoll et al. 2012). For this, different orders of magnitude for these values can be used. They are set according to the activation status of the node (derived from normalized values) and an amplification factor \\(F\\), designed to generate a higher relative difference in the transition rates, and are therefore defined for each node \\(i\\) and sample \\(j\\): \\[k^{0\\rightarrow1}_{i,j}=F^{2(X^{norm}_{i,j}-0.5)}\\] \\[k^{1\\rightarrow0}_{i,j}=\\dfrac{1}{k^{0\\rightarrow1}_{i,j}}\\] Thus, if a gene has a value of \\(1\\) based on its RNA profile, \\(k_{0\\rightarrow1}\\) (resp. \\(k_{1\\rightarrow0}\\)) will be \\(10^2\\) (resp. \\(10^{-2}\\)) with an amplification factor of \\(100\\). This amplification factor is therefore a hyper-parameter of the method. Very low values of \\(F\\) will have no impact while higher values will make some transitions almost impossible and the method will then approach the discrete personalization described above. Some quantitative illustrations of the influence of \\(F\\) are provided in Béal et al. (2019). The integration of continuous data through the initial conditions of the nodes and the transition rates are combined to form a second personalization method called continuous personalization and described in Figure 5.5B. This method has also been called soft node variants to emphasize its difference with discrete/strict personalization: it may influence the trajectories in the solution state space leading to a change in probabilities of the resulting stable state but it does not overwrite the logical rules. To illustrate a little more explicitly the impact of continuous personalization, if a given node has a normalized value of \\(0.8\\) after data processing (based on proteins levels for instance), it will be initialized as \\(1\\) in 80% of the stochastic trajectories, its transition rate \\(k_{0\\rightarrow1}\\) will be increased (favoring its activation) and its transition rate \\(k_{1\\rightarrow0}\\) will be decreased (hampering its inactivation). These changes increase the probability that this node will remain in an activated state close to the one inferred from the patient’s data, while maintaining the validity of its logical rule. Thus, continuous personalization appears as a smoother way to shape logical models’ simulations based on patient data. In summary, different types of data can be used, with different integration methods. Note that it is quite natural to use genetic alterations (mutations, CNA) to specify definitive changes in models (such as those of discrete personalization) since this corresponds to biological reality: a mutation cannot be undone or reversed. Conversely, continuous alterations in expression or phosphorylation are subject to modification and regulation, thus justifying their interpretation in a less strong and definitive way (such as continuous personalization). Finally, it follows from these definitions that there are different strategies for personalizing a logical model since discrete and continuous personalizations can each use different types of data; and moreover, these two strategies can be combined. Except otherwise stated, mutations (resp. RNA or protein) will always be integrated using discrete (resp. continuous) personalization and the joint integration of both types of data will therefore combine both methods. The relative merits of the different personalization strategies will be discussed below. 5.2 An integration tool for high-dimensional data? Once the method has been defined, it is imperative to study its validity and possible limitations. This comes down to answering the question: do personalized models capture a biological reality, and in our case do they discriminate between different types of cancer? 5.2.1 Biological relevance in cell lines These questions can be addressed using cell line data. Using the logical model of cancer pathways from Fumia and Martins (2013), it is possible to study the 663 cell lines from different types of tumors by integrating their processed omics profiles to the generic logical model to obtain as many personalized models. If we focus on the read-out of Proliferation, one of the easiest to interpret, there are several ways to study its relevance. For each cell line and each personalization strategy (and corresponding data type) we can define a personalized model and derive the asymptotic value the Proliferation node, called Proliferation score. This score is therefore a priori different for all cell lines that present a different molecular profile. For the whole population of cell lines, this score can be confronted with other markers of proliferation such as the levels of Ki67 (Miller et al. 2018), here replaced as an example by the RNA levels of the corresponding MKI67 gene. It can then be observed that the simulated Proliferation indicator, derived from the personalized models, correlates positively with the biomarker, but only when RNA has been used in the personalization (Figure 5.6A). The correlation makes qualitative sense, but the heterogeneity appears to be very large and most of the variability is not captured by the models. This heterogeneity is also visible by focusing on some types of cancer (Figure 5.6B). Thus this kind of comparison only validates the models’ ability to retrieve a RNA biomarker (not used in personalization) when they themselves integrate other RNA data. It is also consistent that scores from models personalized with mutations only have less uniform distributions due to the discrete nature of the data and the many identical profiles: many cell lines are not distinguishable by mutations only. Figure 5.6: Validation of personalized Proliferation scores in cell lines. (A) Comparison with MKI67 proliferation biomarker for all cancer cell lines. (B) Same with breast (BRCA) and lung (LUAD) cancer only. (C) Comparison with doubling times in a subset of 60 cell lines. It is possible to go one step further by comparing these personalized Proliferation scores with the doubling time of the cell lines, i.e., the time it takes for the cell line population to double. A cell line described as proliferative (high Proliferation score) should thus have a low doubling time. This can be observed qualitatively by using a subgroup of cell lines for which this information is available (Figure 5.6C). These correlations are not significant and once again summarize a large heterogeneity. Predicting doubling times is, however, a rather difficult task with poor accuracies, even with the help of more flexible machine learning low (Kurilov, Haibe-Kains, and Brors 2020). 5.2.2 Validation with patient data Figure 5.7: Comparaison of personalized scores with tumor grades for breast cancer patients in METABRIC cohort. Comparisons are provided for different personalization strategies (with mutations and/or RNA) and two different model nodes (Proliferation and Apoptosis). Patient data can as well be used to reproduce analyses of the same type as those previously performed with the MKI67 biomarker, as was done in Béal et al. (2019), but we focus here on the more clinical applications of the personalized mechanistic models. By analogy with the validations proposed for other mechanistic models (Fey et al. 2015), it is also possible to evaluate the prognostic value of personalized logical models on patient data. For example, when studying breast cancer patients in the METABRIC cohort, Proliferation and Apoptosis scores differ according to tumor grade. The more advanced tumors (grade 3) are associated with higher Proliferation scores and lower Apoptosis scores (Figure 5.7). This is in line with the natural interpretation that could be given since proliferation is by definition a sign of cancer progression while apoptosis, a programmed death of defective cells, is on the contrary a protective mechanism. While these trends are monotonous and clearly significant for the third strategy using both mutations and RNA (\\(p&lt;10^{-12}\\) with Jonckheere–Terpstra test for ordered differences among classes, for both nodes), this is not the case when the two types of data are used separately: mutations (resp. RNA) are not sufficient to personalize Proliferation (resp. Apoptosis) scores in a meaningful way. The personalisation method therefore seems to be able to combine discrete and continuous data in such a way that some of the biological information is preserved. Figure 5.8: Hazard ratios for Proliferation and Apoptosis in a survival Cox model in METABRIC cohort. Higher Proliferation (resp. Apoptosis) scores correspond to higher (resp. lower) probabilities of death. This comparison to clinical data can be extended to patient survival data in the same cohort. If we focus on the strategy integrating both mutations and RNA, we observe that in a Cox model of survival, Proliferation is significantly associated with a higher risk of event while Apoptosis is associated with a lower risk, which is again consistent (Figure 5.8). In a more schematic and visual way, it is possible to transform these continuous Proliferation and Apoptosis scores into binary indicators (using medians) and observe their impact on survival, as has been done in previously mentioned studies (Fey et al. 2015; Salvucci, Zakaria, et al. 2019). We then observe the same behaviour for the two personalized scores (Figure 5.9A and B). Interestingly, if we combine the indicators to create groups that are expected to be of very bad prognosis (high Proliferation, low Apoptosis) or of very good prognosis (low Proliferation, high Apoptosis), we further discriminate patients and confirm the qualitatively meaningful interpretation of the personalized scores. It should be noted that the clinical validations presented here remain voluntarily simple and quite close to those proposed in similar articles. Discussions and statistical developments will be proposed in Part III. Figure 5.9: Prognostic value of Proliferation scores for breast cancer patients in METABRIC cohort. (A) Survival curve for overall survival stratified with Proliferation scores from personalized models integrating mutations and RNA; scores have binarized based on median and survival censored at 120 months. (B) Same with Apoptosis scores. (C) Survival curve stratified with combinations of Proliferation and Apoptosis scores, based on the same thresholds, and the corresponding number of patients at risk (D). 5.2.3 Perspectives In summary, this kind of application of personalized models allows the integration of quite heterogeneous and moderately dimensional biological data in a constrained framework that orders the relationships between variables and guides interpretations. Comparison with external biological or clinical data then makes it possible to verify the absence of major contradictions in the definition of the model. However, the interest of these mechanistic approaches in this type of task appears as quite moderate compared to statistical models. The qualitative aspect is not necessarily compensated here by the integration of knowledge into the structure of the model, especially in examples that use an extremely broad logical model, which has not been specifically designed for the problems to which it is applied. It is then necessary to study the application of these personalized models to more suitable problems, in which the explicitly mechanistic nature of the models can be exploited. References "],
["personalized-logical-models-to-study-an-interpret-drug-response.html", "Chapter 6 Personalized logical models to study an interpret drug response 6.1 One step further with drugs 6.2 Case study on BRAF in melanoma and colorectal cancers 6.3 Application on prostate cancer study and challenges 6.4 Limitations and perspectives", " Chapter 6 Personalized logical models to study an interpret drug response Il serait excellent que tout médecin ait la possibilité d’expérimenter un grand nombre de médicaments sur lui-même. Sa compréhension de leurs effets en serait tout autre. Mikhail Bulgakov (Morphine, 1927) Historically, all mechanistic models of molecular networks, and logical models in particular, have been widely used to study response to treatments (Flobak et al. 2015; Jastrzebski et al. 2018). Indeed, biological entities, many of which are prospective therapeutic targets, are explicitly represented in the model, making it possible to simulate their inhibition. This is what will be presented in this chapter using the personalized logical models described above. Can they be used to study the response of biological systems to perturbations, in this case the response of cell lines to gene or protein inhibitions? Compared to the numerous statistical models designed to predict the sensitivity of cell lines to treatements, what information do these personalized mechanistic models provide? Scientific content This chapter extends the method presented in the previous chapter to investigate drug response with personalized logical models. The first application to cell lines of all cancer types was presented orally at ISMB2020 in Basel but is not published. The example about BRAF in melanomas and colorectal cancers is under review and the corresponding pre-print is available as Béal et al. (2020). In this joint work, only the construction of the generic logical model was mostly carried out by collaborators and will therefore be described more succinctly. Finally, the work on prostate cancer presented in a third section will be submitted soon. It is also a joint work, in which my participation focused on the application of the PROFILE method. 6.1 One step further with drugs One of the main clinical consequences of the underlying molecular complexity of cancers is the divergent response to treatment, even for a priori similar tumors. In the light of high-throughput sequencing data, the mechanisms governing these responses are somewhat better understood, for patients and especially for model organisms such as cell lines (Heiser et al. 2012; Garnett et al. 2012). But beyond a few simple cases, the diversity of response biomarkers once again calls for holistic approaches to unravel the underlying mechanisms. 6.1.1 Modeling response to cancer treatments To study these observed differences in drug response in various cancers, some approaches based on mathematical modeling were developed to explore the complexity of differential drug sensitivities. A number of machine learning-based methods for predicting sensitivities have been proposed (Costello et al. 2014), either without particular constraints or with varying degrees of prior knowledge; but they do not necessarily provide a mechanistic understanding of the response. Some other approaches focused on the description of the processes that might influence the response by integrating knowledge of the signaling pathways and their mechanisms and translated it into a mathematical model (Eduati et al. 2017; Jastrzebski et al. 2018; Fröhlich et al. 2018). The first step of this approach implies the construction of a network recapitulating knowledge of the interactions between selected biological entities (driver genes but also key genes of signaling pathways), extracted from the literature or from public pathway databases, or directly inferred from data (Verny et al. 2017). This static representation of the mechanisms is then translated into a dynamical mathematical model with the goal to not only understand the observed differences (Jastrzebski et al. 2018) but also to predict means to revert unexpected behaviours. One way to address issues related to patient response to treatments is to fit these mechanistic models to the available data, and to train them on high-throughput cell-line specific perturbation data (Eduati et al. 2017; Jastrzebski et al. 2018; Klinger et al. 2013). These mechanistic models are then easier to interpret with regard to the main drivers of drug response. They also enable the in silico simulations of new designs such as combinations of drugs not present in the initial training data (Fröhlich et al. 2018). However, these mechanistic models contain many parameters that need to be fitted or extracted from the literature. Some parsimonious mathematical formalisms have been developed to make up for the absence of either rich perturbation data to train the models or fully quantified kinetic or molecular data to derive the parameters directly from literature. One of these approaches is the logical modeling, which uses discrete variables governed by logical rules. Its explicit syntax facilitates the interpretation of mechanisms and drug response (Zañudo, Scaltriti, and Albert 2017; Iorio et al. 2016) and despite its simplicity, semi-quantitative analyses have already been performed on complex systems including drug response studies (Knijnenburg et al. 2016; Eduati et al. 2020). 6.1.2 An application of personalized logical models But logical formalism has also shown its relevance regarding drug response in cases where the model is not automatically trained on data but simply constructed from literature or pathway databases and where biological experiments focus on a particular cell line (Flobak et al. 2015). The study is then restricted to one cell line only from which some data and parameters have been experimentally inferred. Using the PROFILE method, it is possible to generate personalized logical models associated with different cell lines and then use them to study the response to treatment. Since the models are not trained with perturbation data but simply specified/constrained by interpreting the molecular profiles, it is possible to do this with a rather limited amount of data. Figure 6.1: Schematic extension of PROFILE-personalized logical models to drug investigation. (A) Schematic representation of a logical model of cancer molecular networks, in particular the one described in appendix B.2 and used in the nexte subsection. (B) Sequential pipeline for drug response investigation with PROFILE, starting from a generic logical model, then transformed into several personalized models with different molecular profiles (correspondong to several cell lines); these models are finally simulated with a defined drug inhibition. (C) A possible analysis of the predictions of personalized models obtained from the generic model described in (A); the position of the personalized models in the PCA phenotype (or read-out) space of the model is studied as well as the impact of the treatment on this position. The principles are summarized in Figure 6.1. A generic model (Figure 6.1A) is first transformed into as many personalized models as there are cell lines with an omics profile. These persnalized models are then simulated by adding the effect of a given treatment (Figure 6.1B). The treatments that can be studied are generally targeted inhibitors. Generally speaking, one must be able to translate the mechanism of action of the treatment into the logical model. The impact of more systemic treatments such as chemotherapy or radiotherapy is more difficult to study with these methods, in any case with most of the logical models published to date, even if in theory, precise modelling of the pathways associated with these treatments (such as DNA repair) could contribute to this. It is then possible to analyze the personalized scores for each cell line (asyptotic values of the phenotypic read-outs of the model) with or without the effect of treatment. If the model includes more than two phenotypes of interest, such as the one in Figure 6.1A, one can visualize these behaviors in the PCA space of the personalized scores, as shown schematically in Figure 6.1C. In this case the directions of the original phenotype features (Proliferation, Apoptosis, Quiescence) have been added in the PCA-transformed space in order to facilitate the interpretation of positions and drug-induced displacements. In this mock example, based on personalized models, treatment would promote a shift from proliferative to more apoptotic or quiescent behaviors, in particular in the red and green cell lines, which are a priori more sensitive to the treatment. 6.1.3 A pan-cancer attempt This versatile analysis framework was first applied during this thesis to a large pan-drug and pan-cancer analysis. On the basis of generic logical models such as those previously presented (cf appendix B.1 and B.2), and in view of the abundance of available data (across cancer tissues and drugs such as in GDSC cell line dataset, cf appendix B.1), there were no theoretical obstacles to such an analysis. Although the simulations were carried out without any problems, the analysis nevertheless proved extremely difficult to interpret. We will highlight the various problems encountered, propose an illustration and some perspectives that led to the work presented in the following section. Based on the PROFILE methodology and GDSC data, hundreds of personalized models can be obtained, each corresponding to a cell line. For each of these personalized models, several dozen of potential drugs have a mechanism of action that can be mechanistically translated into the logical model. We thus obtain tens of thousands of “personalized model/drug” pairs that correspond to experimentally evaluated drug sensitivies (cf appendix @ref(#appendix-GDSC) for details). Firstly, the comparison of simulated and experimental data is not straightforward. As the models are qualitative, it is necessary to carry out the validation in this spirit. The idea is not to predict sensitivity quantitatively, rather to verify their relative relevance. In the first place, do we recover the cell lines that are most sensitive to a given drug? With several hundred cell lines, it is difficult to make this reflection graphically as in Figure 6.1C. More quantitative approaches, such as correlation, would require the definition of a precise sensitivity proxy in personalized models. Should we choose the personalised Proliferation score obtained with drug? Or the drug-induced displacement in the mechanistic model (the drug arrows in Figure 6.1C)? Or is a combination of phenotypes used, if so which one? As for experimental metrics, which ones to choose, and what interpretations do they allow? Whatever the choice, dose-response AUC or IC50 (see details in the appendix A.1.2), a problem arises: can the sensitivities of a cell line to different drugs be compared? Such a comparison would allow the most clinically interesting questions of precision medicine to be asked: for a given molecular profile, can the model predict the best treatment to administer? However, AUCs are comparable for different drugs only if the concentration ranges tested are similar; and IC50s are extrapolated, sometimes well beyond the concentrations tested. Qualitative comparisons for a given drug therefore seem the most meaningful, as long as a relevant proxy in personalized models can be justified. Aware of these difficulties, if one decides to do a correlation analysis, for each drug, of the personalized correlation scores with experimental sensitivities, one realizes that responses to certain drugs are correctly predicted when others are not correlated at all. But it is difficult to decide between two different interpretations: does this mean that correctly predicted drugs are well modelled and others are not? Or does it mean that some correlations appear to be better by chance because so many drugs have been modeled? A case study can be illustrated more precisely with the example shown in Figure 6.2. In order to simplify the analysis presented schematically in Figure 6.1C, the 663 cell lines were averaged by cancer type (according to TCGA denominations) and the drug-induced shifts are all represented from the origin in the PCA space. There is evidence that the effect of the drug on personalized models (using only mutations) tends to make them less proliferative and more apoptotic/quiescent (Figure 6.2A). This shift is strongest for those types of cancer that are actually most sensitive to this inhibitor experimentally (i.e., low AUC), such as skin cutaneous melanomas (SKCM) in particular, and colorectal (COAD/READ) or pancreatic (PAAD) cancers to a lesser extent. The ability of personalized models to explain this difference can be understood by a known underlying biological reality: the prevalence of BRAF or RAS mutations in these cancers. The three aforementioned cancers are thus very frequently mutated for one of the two genes (Figure 6.2B). Then, the model translates the fact that these two genes are located just upstream of MAP2K1. It is therefore natural that an inhibition just downstream of these important mutations is particularly effective (Figure 6.2C). In a case such as this, the relevance of the model can be explained and justified a posteriori. This analysis is much more difficult in the vast majority of cases, whether the correlations are apparently good or not. Figure 6.2: PROFILE-generated models and sensitivites to MAP2K1 inhibitors avergaed per cancer type. (A) Effects of MAP2K1 inhibitors on personalized logical models averaged per cancer types and represented in a normalized PCA space with super-imposed original phenotypes. (B) Proportion of BRAF- and RAS-mutated cell lines in some cancer types. (C) Zoom on the MAPK pathway of the logical model used. This example highlights a problem of scope. The fact that the method enables to study hundreds of cell lines and dozens of drugs does not mean that it is relevant in each case. The description of pathways in the model is more or less accurate. For example, a node at the model boundaries probably has many regulators missing. Is it then relevant to investigate the response of personalized models to its inhibition? It is therefore necessary to restrict the drugs studied. Similarly, even if the logical model summarizes many important pathways, it is probably unsuitable for certain cell lines or certain types of cancer with different etiologies. However, it is difficult to restrict the scope of the analysis in an unbiased way without having designed a model de novo for a specific purpose. For all these reasons, it was decided to leave aside this naive, broad-spectrum approach in favour of starting from a more specific biological question and constructing the appropriate logical model. 6.2 Case study on BRAF in melanoma and colorectal cancers In order to address the limitations outlined in this exploratory analysis, we propose here a pipeline based on logical modeling and able to go from the formulation of a specific biological question to the validation of a mathematical model on pre-clinical data, in this case a set of cell lines, and the subsequent interpretation of potential resistance mechanisms (Figure 6.3). As before,** one of the main points of differentiation with existing mechanistic approaches, is that this framework does not rely on any training of parameters but only on the automatic integration and interpretation of omics features**. Figure 6.3: BRAF modelling flowchart: from a biological question to validated personalized logical models. 6.2.1 Biological and clinical context The construction of a mathematical model must be based first and foremost on a precise and specific biological problem, at the origin of the design of the model. Here, we choose to explore the different responses to treatments in diverse cancers that bear the same mutation. A well-studied example of these variations is the BRAF mutation and especially its V600E substitution. BRAF is mutated in 40 to 70% of melanoma tumors and in 10% of colorectal tumors, each time composed almost entirely of V600E mutations (Cantwell-Dorris, O’Leary, and Sheils 2011). In spite of these similarities, BRAF inhibition treatments have experienced opposite results with improved survival in patients with melanoma (Chapman et al. 2011) and significant resistance in colorectal cancers (Kopetz et al. 2010), suggesting drastic mechanistic differences. Some subsequent studies have proposed context-based molecular explanations, often highlighting surrounding genes or signalling pathways, such as a feedback activation of EGFR (Prahallad et al. 2012) or other mechanisms [Poulikakos et al. (2011); sun2014reversible]. These various findings support the need for an integrative mechanistic model able to formalize and explain more systematically the differences in drug sensitivities depending on the molecular context. The purpose of the study we propose here is not to provide a comprehensive molecular description of the response but to verify that the existence and functionality of the suggested feedback loops around the signalling pathway in which BRAF is involved (Prahallad et al. 2012) may be a first hint towards these differences. For a more thorough study of these cancers, we refer to other works (Eduati et al. 2017; Baur et al. 2020; Cho et al. 2016). 6.2.2 A logical model centred on BRAF A logical model summarizing the main molecular interactions at work in colorectal cancers and melanomas is thus built from the literature and completed with databases. As previously mentioned, the objective is to understand whether it is possible to model and explain differences in responses to BRAF inhibition in melanoma and colorectal cancer patients using the same regulatory network. The fact that the two cancers share the same network but differ from the alterations and expression of their genes constitutes our prior hypothesis. The focus of this model is put on two important signaling pathways involved in the mechanisms of resistance to BRAF inhibition which are the ERK1/2 MAPK and PI3K/AKT pathways (Ursem, Atreya, and Van Loon 2018; Rossi et al. 2019). The generic network presented in Figure 6.4 recapitulates the known interactions between the biological entities of the network that was first built from the literature, and then verified and completed with potential missing connections using SIGNOR database (Perfetto et al. 2016). More details and references about the model can be found in appendix B.3. All in all, the logical model formalizes the knowledge compiled from different sources and highlights the role of SOX10, FOXD3, CRAF, PI3K, PTEN and of EGFR in resistance to anti-BRAF treatments. Figure 6.4: Logical model of signaling pathways around BRAF in colorectal andmelanoma cancers. Grey nodes represent input nodes, which may correspond to the environmental conditions. Square nodes represent multi-valued variable (MEK, ERK, p70 and Proliferation). Dark blue nodes accounts for families (several genes/entities for one node). Light blue node represents the phenotypic read-out of the model, i.e., Proliferation. Once the structure of the model was defined, and before moving on to its personalization, its consistency with the literature was checked using a model-checking procedure. Indeed, due to the complexity of the system, properly taking into account the interactions between entities does not automatically guarantee that the model will reproduce certain dynamic behaviours. An example of a biological assertion may be the reactivation of the MAPK (mitogen-activated protein kinase) pathway through EGFR signal after BRAF inhibition in colorectal cancer (Prahallad et al. 2012): it is possible to check whether a simulation of this situation with the model gives the same result or not. Because there are many such assertions and because it is useful to verify them as the model is built, automatic model-checking tools have been defined, based on the MaBoSS syntax and inspired by the python unittest library. More details are provided in Béal et al. (2020) and in a corresponding GitHub repository. The list of biological assertions used to validate the model is detailed in the appendix B.3. 6.2.3 Cell lines data The omics profiles of colorectal and melanoma cell lines are downloaded from Cell Model Passports portal (Meer et al. 2019). 64 colorectal cancer (CRC) cell lines and 65 cutaneaous melanoma (CM) cell lines are listed in the database, with at least mutation or RNA-seq data (59 CM and 53 CRC with both mutations and RNA-seq data). These omics profiles are used to generate cell-line-specific logical models as described in PROFILE method (Figure 5.1). The prevalence of mutations and their combination for the two types of cancer can be seen in Figure 6.5A and is consistent with the clinical situation described above with melanomas more frequently BRAF-mutated and colorectal cancers more frequently RAS-mutated. Figure 6.5: Descriptive analysis of cell lines for melanomas and colorectal cancers. (A) Number of cell lines for the four most frequently mutated genes and their combinations (plot from UpSetR package (Conway, Lex, and Gehlenborg 2017)). (B) Differential sensitivities to BRAF inhibition by the drug PLX-4720 (lower panel) or by CRISPR inhibition (upper panel), depending on BRAF mutational status and cancer type. Numbers of cell lines in eache category are indicated. Note that high sensitivities correspond to low AUC and high scaled Bayesian factors. In order to validate the relevance of personalized models to explain differential sensitivities to drugs, some experimental screening datasets are used. Drug screening data are downloaded from the Genomics of Drug Sensitivity in Cancer (GDSC) dataset (Yang et al. 2012) which includes two BRAF inhibitors: PLX-4720 and Dabrafenib. The cell lines are treated with increasing concentration of drugs and the viability of the cell line relative to untreated control is measured. The dose-response relative viability curve is fitted and then used to compute the area under the dose-response curve (AUC) (Vis et al. 2016). AUC is a value between \\(0\\) and \\(1\\): values close to \\(1\\) mean that the relative viability has not been decreased, and lower values correspond to increased sensitivity to inhibitions (details in appendix A.1.2). The results obtained with the two drugs are very strongly correlated (Pearson correlation of \\(0.91\\)) and the analyses presented here will therefore focus on only one of them, PLX-4720. In a complementary way, some results of CRISPR/Cas9 screening are also downloaded from Cell Model Passports. This technology, which is described in more detail in the appendix A.1.3, allows targeted inhibitions of certain genes. Two different datasets from Sanger Institute (Behan et al. 2019) and Broad Institute (Meyers et al. 2017) are available. We use scaled Bayesian factors to assess the effect of CRISPR targeting of genes. These scores are computed based on the fold change distribution of sgRNA (Hart and Moffat 2016). The highest values indicate that the targeted gene is essential to the cell fitness. The agreement between the two databases is good (Dempster et al. 2019) but we choose to focus on the Broad database, which is more balanced in terms of the relative proportions of melanomas and colorectal cancers. Figure 6.5B illustrates both the relative quantities of cell lines for which drug or CRISPR screening data are available (depending on their BRAF status) as well as differences in sensitivity to BRAF inhibition. The greater sensitivity of BRAF-mutated melanomas compared to BRAF-mutated colorectal cancers is well observed for PLX-4720. However, the overlap in the distributions requires a deeper look into the data and a search for more precise explanations of the differences in sensitivity, including within each type of cancer. The finding appears to be similar for CRISPR despite a sample size that is too small; the higher average sensitivity of melanomas even extends to non-mutated BRAF. 6.2.4 Validation of personalized models using CRISPR/Cas9 and drug screening The validation of personalized logical models using these screening data is done with the following rationale. First, the models are personalized using omics data from the cell lines. Then, two separate simulations are performed for each personalized model: one without the inhibition, the other by creating and activating a BRAF inhibitor to mimic the drug or CRISPR inhibition. The ratio of the Proliferation phenotype obtained with inhibition and without inhibition is the proxy used to be compared with the different screening metrics each of which is also standardized (AUC calculated on relative viability for drugs and Bayes factor computed from fold-changes and then scaled). 6.2.4.1 Differential sensitivities to BRAF targeting explained by personalized logical models Once the logical model consistency has been validated, personalized models are generated for each cell line by integrating their interpreted genomic features directly as model constraints or parameters. Sensitivities to BRAF inhibition inferred from models are then compared to experimentally observed sensitivities (Figure 6.6). In all the following analyses, we focus on three different personalization strategies using: only mutations as discrete personalization (Figure 6.6A, upper row), only RNA as continuous personalization (Figure 6.6A, middle row) or mutations combined with RNA (Figure 6.6A, lower row). These choices reflect first of all the following a priori: mutations are much more drastic and permanent changes than RNA, whose expression levels are more subject to fluctuation and regulation. The objective is also to answer the following questions: What type of data is most likely to explain the differences in responses? Is it relevant to combine them? Figure 6.6 shows an example of the type of analyses possible with personalized models, zooming in more and more on the details from panel A to panel C. Figure 6.6: Validation of personalized models of BRAF inhibition with cell lines data. (A) Pearson correlations between normalized Proliferation scores from personalized models and experimental sensitivities to BRAF inhibition by drug or CRISPR targeting; each row corresponds to a different personalization strategy; only the values for the significant correlations are displayed. (B) Scatter plots with non-overlapping points corresponding to correlations of panel A, with the three personalization strategies, focusing on one drug (PLX-4720) and one CRISPR dataset (Broad). (C) Enlargement of one scatter plot in B (left) with the table describing the omics profiles used for each cell line to explore the response mechanisms; interactive version in Figure 6.7 or GitHub files. The first approach consists in using only mutations as discrete personalization (Figure 6.6, A, upper row): the mutations identified in the dataset and that are present in the regulatory network are set to 1 for activating mutations and set to 0 for inactivating mutations. In this case, the Proliferation scores from personalized models significantly correlate with both BRAF drug inhibitors (PLX-4720 and Dabrafenib) and both CRISPR datasets (using Pearson correlations). Note that the opposite directions of the correlations for the drug and CRISPR datasets are due to the fact that cell lines sensitive to BRAF inhibition result in low AUCs, and high scaled Bayesian factors, respectively, and, if the models are relevant, to low standardized Proliferation scores. Looking more closely at the corresponding scatter plot for PLX-4720 (Figure 6.6B, upper left), it can be seen that this correlation results from the model’s ability to use mutations’ information to recover the highest sensitivity of the BRAF-mutated cell lines that form an undifferentiated cluster on the left side. These cell lines are indeed relatively more sensitive than non-mutated BRAF cell lines. However, the integration of mutations alone does not explain the significant differences within this subgroup (AUC between 0.55 and 0.9). A very similar behaviour can be observed when comparing model simulations with CRISPR data (Figure 6.6B, upper right). Using only RNA data as continuous personalization (Figure 6.6A and B, middle rows) is both less informative and more difficult to interpret. For continuous data such as RNAseq data, we normalize the expression values and set both the initial conditions and the transition rates of the model variables to the corresponding values. Correlations with experimental BRAF inhibitions appear weaker and more uncertain. The key point, however, is that the combination of mutations and RNA, as depicted in Figure 6.6 A and B lower rows, seems to be more relevant. This is partially true in quantitative terms but it is even easier to interpret in the corresponding scatter plots (Figure 6.7). Comparing first the Broad CRISPR scatter plots using mutations only (Figure 6.6B, upper right) and using both mutations and RNA (Figure 6.6B, lower right), we can observe that non-responsive cell lines (scaled Bayesian factor below 0), grouped in the lower right corner and correctly predicted using only mutations stayed in the same area: these strong mutational phenotypes have not been displaced by the addition of RNA data. Other cell lines previously considered to be of intermediate sensitivity by the model (e.g., COLO-678 or SK-MEL-2) were shifted to the right, consistent with the lack of sensitivity observed experimentally. Finally, BRAF-mutated cell lines, previously clustered in one single column on the left using only mutations (with normalized Proliferation scores around 0.5), have been moved in different directions. Many of the most sensitive cell lines (scaled Bayesian factor above 2) have been pushed to the left in accordance with the high sensitivities observed experimentally (e.g., HT-29 or SK-MEL-24). It is even observed that the model corrected the position of the two BRAF mutated cell lines, but whose sensitivity is experimentally low (melanoma cell line HT-144 and colorectal cell line HT-55). Only one cell line (SK-MEL-30) has seen its positioning evolve counter-intuitively as a result of the addition of RNA in the personalization strategy: relatively sensitive to the inhibition of BRAF, it has, however, seen its standardised Proliferation score approach 1. All in all, this contribution of RNA data results in significant correlations even when restricted to BRAF-mutated cell lines only (\\(R=0.69\\), \\(p.value=0.006\\)). Figure 6.7: Multi-omics integration and enhanced value of RNA in addition to mutations. The successive observation of the two personalization strategies allows to visualize the impact of the RNA addition and to look at the corresponding omics sprofiles for the different points. A similar analysis can be made of the impact of adding RNA data to personalization when comparing with the experimental response to PLX-4720 (Figure 6.6B, upper and lower left). Most of the non-sensitive cell lines (upper right corner) have not seen the behaviour of the personalized models change with RNA addition. However, the numerous BRAF-mutated cell lines previously grouped around standardized Proliferation scores of 0.5, are now better differentiated and their sensitivity predicted by personalized models has generally been revised towards lower scores (i.e., higher sensitivity). Similar to the CRISPR data analysis, three sensitive cell lines have been shifted to the right and are misinterpreted by the model. As a result, the correlation restricted to BRAF-mutated cell lines is no longer significant (R=0.26, p.value=0.1). 6.2.4.2 An investigative tool These personalized models are not primarily intended to be predictive tools but rather used to reason and explore the possible mechanisms and specificities of each cell line, for example by studying the molecular alterations at the origin of the observed behaviour (Figure 6.7). To continue on the previous examples, the two melanoma cell lines, HT-144 and SK-MEL-24, share the same mutational profiles but have very different sensitivities to BRAF targeting (Figure 6.6C). This inconsistency is partially corrected by the addition of the RNA data, which allows the model to take into account the difference in CRAF expression between the two cell lines. In fact, CRAF is a crucial node for the network since it is necessary for the reactivation of the MAPK pathway after BRAF inhibition. Therefore, the high sensitivity of SK-MEL-24 may be explained by its low CRAF expression level, which makes the reactivation of the MAPK pathway more difficult for this cell line. Conversely, in HT-144, the high level of CRAF expression allows the signal to flow properly through this pathway even after BRAF inhibition, thus making this cell line more resistant. The importance of CRAF expression is also evident in HT-29, a CRC BRAF mutated cell line with other important mutations (PI3K activation and p53 deletion). However, it remains sensitive to treatment, due to its very low level of CRAF expression. Another interesting contribution of RNA appears in the melanoma cell line UACC-62, which is particularly sensitive to treatment. The model is able to correctly predict its response once RNA levels are integrated. In this case, the reason for sensitivity seems to be due to the low level of PDPK1, which makes it difficult to activate p70 and thus trigger the resistance linked to PI3K/AKT pathway activation. Similarly, the CRC resistant cell line, HT55, which carries only the BRAF mutation, expresses high levels of PDPK1, in addition to high levels of CRAF, supporting the idea that the presence of both MAPK and PI3K/AKT pathways may confer resistance to BRAF inhibition treatments. We can also mention a cluster of RAS mutated cell lines, usually NRAS mutated for melanomas (e.g., SK-MEL-2) and KRAS for colorectal cancers (e.g., COLO-678), which are classified by the model as resistant. Interestingly, in these cell lines, a low level of CRAF is not enough to block the signal of the MAPK pathway, which is stronger in the model because of the simulation of the RAS mutation (RAS is set to \\(1\\)). Only SK-MEL-30 appears to be incorrectly classified and is observed to be more sensitive than the other cell lines with a similar mutation profile. This could be due to the fact that our network is incomplete and not able to account for some alterations responsible for this cell line sensitivity. The exploration of the mutational profile for this cell line might be a hint of The problem may also come from the fact that this cell line contains a frameshift mutation of RPS6KB2 (p70 node) not referenced in OncoKB and therefore not included in the simulation. Figure 6.8: Application of personalized models to other CRISPR targets. (A) Personalization strategies using either mutations only (as discrete data) or combined with RNA (as continuous data) with their corresponding scatter plots in panels B and C. (B) Scatter plot comparing normalized Proliferation scores of p53 inhibition in the models with experiment sensitivity of cell lines to TP53 CRISPR inhibition, indicating p53 mutational status as interpreted in the model. Pearson correlations and the corresponding p-values are shown. (C) Similar analysis as in panel B with PI3K model node and PIK3CA CRISPR inhibition. The versatility of the logical formalism makes it possible to test other node inhibitions as in Figure 6.8, but remains limited by the scope of the model. Since the present model has been designed around BRAF, its regulators have been carefully selected and implemented, which is not necessarily the case for other nodes of the model. Therefore, these personalized models can be used to study how comprehensive the descriptions of the regulation of other nodes or parts of the model are. Thus, model simulations show that response trends to TP53 inhibition are consistently recovered by the model (Figure 6.8B) but the simple regulation of p53 in the model results in coarse-grained patterns, although slightly improved by addition of RNA data. Similar analyses regarding the targeting of PIK3CA (in CRISPR data) simulated, in the model, by the inhibition of PI3K node, can be performed (Figure 6.8C). Low correlations are an indication highlighting the insufficient regulation of the node, probably confirming the scope issues raised in the pan-cancer-preliminary analysis. 6.2.5 Comparison of the mechanistic approach with machine learning methods Figure 6.9: Random forests to predict and explain sensitivity to BRAF inhibition. (A) Performances of random forests for BRAF sensitivity prediction measured with percentage of explained variance; different learning task with unprocessed original data (thousands of genes), unprocessed original data for model-related genes only (tens of genes), and processed profiles of cell lines (tens of genes); \\(n\\) samples and \\(p\\) variables per learning task. (B) Variable importance for drug prediction only, with the 10 best variables with positive importance for each case. In order to provide comparison elements unbiased by prior knowledge or by the construction of the model, we performed some simple machine learning algorithms. Random forests are used as an example of a machine learning approach to compare with mechanistic models and are implemented with randomForestSRC R package (Breiman 2001a). Random forests can be seen as an aggregation of decision trees, each trained on a different training set formed by uniform sampling with replacement of the original cohort. Prediction performances are computed using out-of-the bag estimates for each individual (i.e, average estimate from trees that did not contain the individual in their bootstrap training sample) and summarized as percentage of variance explained by the random forest. In this case, random forests have been fitted with inputs (mutations and/or RNA data) and outputs (sensitivities to drug or CRISPR BRAF inhibition) similar to those of logical models and the corresponding predictive accuracies are reported in Figure 6.9A. The first insight concerns data processing. The percentages of variance explained by the models are similar (around 70% of explained variance for drug sensitivity prediction) in the following three cases: unprocessed original data (thousands of genes), unprocessed original data for model-related genes only (tens of genes), and processed profiles of cell lines (tens of genes). This supports the choice of a model with a small number of relevant genes, which appear to contain most of the information needed for prediction. Second, the absolute level of performance appears much lower for CRISPR (between 30 and 50%) probably suffering from the lower number of samples, especially in cases where the number of variables is the highest. This tends to reinforce the interest of mechanistic approaches that do not use any training on the data for smaller datasets, less suitable for learning. Finally, while mutations and RNA data seem to provide the same predictive power (especially for drugs), using the two together does not necessarily result in a better performance in this case. It is also possible to compute the variable importance that assesses the contribution of variables to the overall performance. The solution adopted in this paper to measure it, and called VIMP in the package, consists in introducing random permutations between individuals for the values of a variable and quantifying the variation in performance resulting from this addition of noise. In the case of key variables for prediction, this perturbation will decrease the performance and will result in a high variable importance (Ishwaran 2007). Variable importance in these different random forests are reported in Figure 6.9B and are consistent with the analysis of mechanistic models. The mutational status of BRAF is definitely the most important variable followed by mutations in RAS or TP53. Concerning RNA levels, the most explanatory variables seem to be FOXD3 or PTEN, in line with the definition of the logical model. 6.3 Application on prostate cancer study and challenges Before summarizing the potential and limitations of the PROFILE approaches described in this and the previous chapter, a final example may be mentioned. Indeed, another application of the PROFILE method, quite similar to the examples presented in the previous and this chapter, has been carried out on prostate cancer. Chronologically, this project was one of the first applications of the method. However, as this project was more collaborative than personal, the previous chapters have been illustrated by more exclusively personal work when they were equivalent. We will therefore only briefly mention here the differences and insights specific to this study. First, a logical model specific to prostate cancer was developed by some collaborators (Pauline Traynard and Arnau Montagud) over a long period of time, resulting in a large and comprehensive model of 146 nodes, which is described in more detail in the appendix B.4 and Figure B.3. Using the TCGA prostate cancer dataset (A.3.3) prognostic validation of the model was first carried out, similarly to Figure 5.7, by comparing individualized scores of some phenotypes in the model (i.e., Proliferation) with clinical markers, in this case Gleason score, a grading system specific to prostate cancer. The qualitative evolution of the personalized Proliferation scores is also qualitatively validated (predicted proliferating tumors are on average of higher grade) but, despite the specificity and magnitude of the model, much of the variability is not explained. The use of cell line data was also explored using Cell Model Passports data, restricted to the 7 prostate cell lines. The size of the model then allows qualitative predictions to be made on the proliferative, apoptotic and metastatic qualities of the different lines. Except for proliferation, however, experimental validation of the relevance of these predictions is difficult using public data or the literature. But again, after these preliminary validations, the focus of the study was on treatment response with a slightly different rationale than in the previous example. Focusing on a particular cell line (LnCaP) and its corresponding personalized logical model, the idea is to simulate with the models all possible inhibitions or combinations of inhibitions in order to identify possible vulnerabilities or relevant treatment synergies. Experimental validation on the cell line was then carried out for certain genes that could be targeted depending on the existence of the treatments. The efficacy of certain inhibitions highlighted by the simulations, such as that of HSP90, was confirmed experimentally on this particular cell line. Despite the limitations of the approach in this application to prostate cancer, the study demonstrates the feasibility of the method for investigating the complexity of therapeutic responses and guiding experimental validation. 6.4 Limitations and perspectives The emergence of high-throughput data has made it easier to generate models for prognostic or diagnostic predictions in the field of cancer. The numerous lists of genes, biomarkers and predictors proposed have, however, often been difficult to interpret because of their sometimes uncertain clinical impact and little overlap between competing approaches (Domany 2014). Methods that can be interpreted by design, which integrate a priori biological knowledge, therefore appear to be an interesting complement able to reconcile the omics data generated and the knowledge accumulated in the literature. These benefits come at the cost of having accurate expert description of the problem to provide a relevant basis to the mechanistic models. This is particularly true in this work since the personalized models all derive from the same structure (the initial generic logical model) of which they are partially constrained versions. It is therefore necessary to have a generic model that is both sufficiently accurate and broad enough so that the data integration allows the expression of the singularities of each cell line. If this is not the case, the learning of logical rules or the use of ensemble modeling could be favoured, usually including perturbation time-series data (Razzaq et al. 2018). It should also be noted that, in the logical models presented here, the translation of biological knowledge into a logical rule is not necessarily deterministic and unambiguous. The choices here have been made based on the interpretation of the literature only. And the presence of certain outliers, i.e., cell lines whose behaviour is not explained by the models, may indeed result from the limitations of the model, either in its scope (important genes not integrated), or in its definition (incorrect logical rules). More global or data-driven approaches to define the model would be possible but would require different training/validation steps and different sets of data. The second key point is the omics data used. For practical reasons, we have focused on mutation and RNA data. The legitimacy of the former is not in doubt, but their interpretation is, on the other hand, a crucial point whose relevance must be systematically verified. The omission or over-interpretation of certain mutations can severely affect the behaviour of personalized models. Validation using sensitivity data provides a good indicator in this respect. However, the question is broader for RNA data: are they relevant data to be used to personalize models, i.e., can they be considered as good proxies for node activity? The protein nature of many nodes in the model would encourage the use of protein level data instead, or even phosphorylation levels if they were available for these data. One perspective could even be to push personalization to the point of defining different types of data or even different personalization strategies for each node according to the knowledge of the mechanisms at work in the corresponding biological entity. A balance should then be found to allow a certain degree of automation in the code and to avoid overfitting. Despite these limitations, the results described above support the importance of combining the integration of different types of data to better explain differences in drug sensitivities. There was no doubt about this position of principle in general (Azuaje 2017), and in particular in machine learning methods (Costello et al. 2014; Aben et al. 2016). The technical implementation of these multi-omic integrations is nevertheless more difficult in mechanistic models where the relationships between the different types of data need to be more explicitly formulated (Klinger et al. 2013). The present work therefore reinforces the possibility and value of integrating different types of data in a mechanistic framework to improve relevance and interpretation and illustrates this by highlighting the value of RNA data in addition to mutation data in predicting the response of cell lines to BRAF inhibition. In addition, one piece of data that could be further exploited is that of the specific behaviour of the drugs or inhibitors studied, since for instance some BRAF inhibitors have affinities that vary according to mutations in the BRAF gene itself. The integration of truly precise data on the nature of the drug is nevertheless limited by logical formalism and is more often found in more flexible approaches, e.g. in deep learning (Manica et al. 2019). To conclude, we provide a comprehensive pipeline from clinical question to a validated mechanistic model which uses different types of omics data and adapts to dozens of different cell lines. This work, which is based only on the interpretation of data and not on the training of the model, continues some previous work that has already demonstrated the value of mechanistic approaches to answer questions about response to treatment, especially using dynamic data (Saez-Rodriguez and Blüthgen 2020), and sometimes about similar pathways (Klinger et al. 2013). In this context, our approach proves the interest of logical formalism to make use of scarce and static data facilitating application to a wide range of issues and datasets in a way that is sometimes complementary to learning-based approaches. References "],
["information-flows-in-mechanistic-models-of-cancer.html", "Chapter 7 Information flows in mechanistic models of cancer 7.1 Processing of biological information 7.2 Reanalysis of mechanistic models of cancer", " Chapter 7 Information flows in mechanistic models of cancer Et l’effet qui s’en va nous decouvre les causes. Alfred de Musset (Poésies nouvelles, 1843) The mechanistic models of cancer presented in the previous section have allowed us to integrate the omics data, to make them speak in order to better understand the clinical characteristics of cell lines or patients. But beyond their undeniable intellectual and scientific interest, do they have a direct clinical value? Given the abundance and complexity of patient data available to physicians, the use of computer tools and mathematical models is inevitable and increasingly frequent. Because of their explicit representation of phenomena, mechanistic models can provide a more easily understood alternative for physicians or patients. Is it therefore desirable and relevant to use these models in support of medical decision making? And how can their clinical validity and impact be rigorously measured? First of all, the purpose of this chapter is to outline some of the limitations of the previously presented evaluations of mechanistic models. These evaluations answered the question: are the models of clinical value? We will show that an additional question could be: do mechanistic models have an added clinical value? Scientific content This chapter is relying on unpublished content. The exploratory analyses presented below have helped to clarify considerations expressed qualitatively in previous chapters and formed the starting point for subsequent chapters on the clinical value of cancer models. 7.1 Processing of biological information Mechanistic models, and their outputs in particular, have so far been considered and evaluated as biomarkers. A comprehensive appreciation requires that they be seen as information processing tools. 7.1.1 Information in, information out Indeed, the mechanistic models presented in this thesis (Figures 3.6, 5.9 and 6.6) can be schematically represented by Figure 7.1: inputs \\(X\\) (often omics data) are processed through a mechanistic model (here the grey box) to result in an output \\(Y\\). These models can thus be assimilated to a mathematical transformation, often non-linear, of \\(X\\) in \\(Y\\). Thus, when validating the biological or clinical relevance of \\(Y\\), either by calculating a correlation with the ground truth or by using it to stratify survival curves, only the absolute value of \\(Y\\) is checked. This is an important step and a prerequisite for a well-constructed model. On the other hand, it is not sufficient information to understand how the model works. Indeed, the inputs \\(X\\) probably also have a predictive or clinical value. In short, measuring only the output value of the model does not necessarily reveal the model’s ability to make sense of the data it uses. At the extreme, it is conceivable that a random function of some valuable inputs could be correlated to the biological reality. Figure 7.1: Evaluation of a mechanistic model. Adapted from Figure 1.6. Therefore, the question of the incremental value of the model can be explained as follows: what does the output of the model represent in relation to the inputs? If we restrict ourselves to cases where the absolute biological/clinical value of \\(Y\\) is positive, we can then identify two families of situations. First we can imagine a situation where the model has improved the value of the inputs. The output would then have a higher value than the inputs (better biological validation etc.), or in any case a complementary value, a value not present in the inputs. This would correspond to the capture by the model of emerging or non-linar effects. In the second situation, the output does not capture emergent properties but summarizes, totally or partially, the information present in the inputs. This would correspond to a knowledge-informed dimensionality-reduction. Even in the latter case, the scientific value of the model as a tool for understanding is not necessarily questioned. The analyses presented below are simply intended to supplement the understanding of models and how they process information. 7.1.2 Emergence of information in artifical examples These questions can be illustrated using a very simple artificial model represented in Figure 7.2. On the one hand there are two latent biological variables called Proliferation (P) and Apoptosis (A) resulting in our biological groud truth, Growth. On the other hand, the modeler has access to three different random variables \\(N_1\\), \\(N_2\\) and \\(N_3\\) respectively associated with the sign of P, the absolute value of P and the value of A. Two mechanistic models are defined, one linear (with its output \\(O_{linear}\\)) and one non-linear (with its output \\(O_{non-linear}\\)). We note that the two outputs are sufficiently well defined to be correlated with Growth but only the non-linear model makes use of \\(N_2\\) by multiplying it with \\(N_1\\). Figure 7.2: Evaluation of a mechanistic model. Adapted from Figure 1.6. The ability of models to use inputs to create or summarize information through outputs will be studied using the explained variation metric \\(R^2\\). If a linear model is defined as \\(y_i=\\beta_0+\\beta_1x_i+e_i\\), linear coefficients \\(\\beta\\) are estimated by minimizing the sum of squared differences between predicted and real values of \\(y\\). The fitted model is written \\(\\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1}x_i\\) and \\(R^2\\) also called coefficient of determination is defined as: \\[R^2=\\dfrac{\\sum_{i=1}^{n} (\\hat{y_i}-\\bar{y_i})^2}{\\sum_{i=1}^{n} (y_i-\\bar{y_i})^2}\\] Therefore \\(R^2\\) measures the proportion of variation in \\(y\\) that is explained by the regressors. In order to avoid overfitting, it is possible to calculate the adjusted \\(R^2\\) that corrects with the number of regressors or to fit the model on training data and calculate the \\(R^2\\) on validation data. The latter option was chosen using cross validation and averaging over the \\(R^2\\) obtained in the different folds. Metrics with an interpretation similar to \\(R^2\\) have been defined for logistic regressions or survival analysis (Choodari-Oskooei, Royston, and Parmar 2012). In the case of regressions with several variables \\(x_i\\), it is possible to decompose \\(R^2\\) into different components associated with each of the variables. This decomposition is carried out here by averaging over orderings according to the method proposed by Lindeman (1980) and applied in R code by Grömping and others (2006). The precise formulas are detailed in appendix C.1. Here is an example of schematic reasoning that can be carried out with \\(R^2\\) about the two models in Figure 7.2. Using only the outputs of the models to predict Growth, explained variations are \\(R^2_{O_{non-linear}}=0.455\\) and \\(R^2_{O_{linear}}=0.379\\). The models are thus correctly defined and partly recover the biological read-out. However, the inputs of the model also have an important predictive value since \\(R^2_{N_1+N_2+N_3}=0.514\\). How can we understand the relationship between these values? First, the model including the \\(N_i\\) inputs and the output \\(O\\) as regressors show different performances with \\[R^2_{N_1+N_2+N_3+O_{linear}}=0.514=R^2_{N_1+N_2+N_3},\\] \\[R^2_{N_1+N_2+N_3+O_{non-linear}}=0.586&gt;R^2_{N_1+N_2+N_3}.\\] This means that \\(O_{linear}\\) has no added value compared to a linear combination of the inputs. This was expected given its definition. On the other hand, \\(O_{non-linear}\\) has allowed to extract an emergent information which improves the global prediction when combined linearly with the inputs. We can go further in understanding by breaking down the \\(R^2\\). In Figure 7.3A and B (left columns), \\(R^2\\) of the inputs’ models are decomposed to show that \\(N_1\\) and \\(N_3\\) contribute most to the prediction in a linear model. By using the same strategies for decomposing the \\(R_2\\) and calculating the incremental \\(R^2\\), it is also possible to decompose the \\(R^2\\) of \\(O\\) according to its origin: its component \\(N_1\\) (\\(0.22\\) in Figure 7.3A) is the proportion of \\(R^2\\) that is also explained by \\(N_1\\), so it can be interpreted as being the part of the value of \\(N_1\\) captured by \\(O\\). In the non-linear case, we can see in the decomposition that \\(O_{non-linear}\\) has an additional created component (\\(0.07\\)), it is the non-linear component that is not shared with any of the inputs. Figure 7.3: Decomposition of \\(R^2\\) for inputs and output of example models. (A) Results for the non-linear model inputs and output \\(O_{non-linear}\\) as defined in Figure 7.2. (B) Same with the linear model. Colors represent the origine of \\(R^2\\) contribution. In particular, for right colums (model\\(Y\\sim O\\)), the red share represent the proportion of the \\(R^2\\) of the output \\(O\\) that does not come linearly from the inputs, and therefore its emerging part. In conclusion, if these two models generate meaningful outputs that are correlated with the biological read-out Growth, the analysis of their information processing classifies them into two different categories outlined in the previous sub-section. The linear model summarizes some of the information present in the inputs, without creating any. It can be likened to a relevant dimensionality reduction. The output of the non-linear model also fails to avoid some information losses, but at the same time it extracts new non-linear information. Thus, in combination with the inputs, it provides added value measured by the increase in total \\(R^2\\). Note that \\(R^2\\) is used here as one tool among others to illustrate the reflection on personalized mechanistic models as information processing tools. The point to remember is not technical but rather methodological: these mechanistic models based on omics data cannot be evaluated for themselves but must be evaluated in comparison with the data they use in order to better explain the way they process information. In this logic of model selection, other tools such as the Akaike Information Criterion (AIC) have been proposed and could allow to quantify if the reduction of dimension carried out by the models (from many omics inputs to one mechanistic output) allows a more parsimonious description of biology than the direct use of inputs (Kirk, Thorne, and Stumpf 2013). 7.2 Reanalysis of mechanistic models of cancer Using the tools presented above, it is possible to deepen the analysis of some mechanistic models already presented in this thesis. 7.2.1 ODE model of JNK pathway by Fey et al. (2015) One of the first applications of personalized mechanistic models to cancer is the one proposed by Fey et al. (2015) regarding JNK pathways in patients with neuroblastomas. This work has been described in section 3.4.2 and is recalled in Figure 7.4. The evaluation of the mechanistic models in the original paper was performed by assessing the clinical value of the inputs (RNA levels of ZAK, MKK4, MKK7, JNK and AKT genes) and outputs (\\(H\\), \\(A\\) and \\(K_{50}\\)) separately by comparing them with survival data. The outputs were binarized to optimize the separation between the curves in a log-rank test. In this section we propose to quantify the value of the output in relation to those of the inputs, leaving the output continuous, using the tools described in the previous section. In the context of survival data, the \\(R^2\\) described above is replaced by the \\(R^2\\) defined by Royston and Sauerbrei (2004), whose properties have been validated using simulated data (Choodari-Oskooei, Royston, and Parmar 2012). Figure 7.4: Mechanistic modeling of JNK pathway and survival of neuroblastima patients, as described by Fey et al. (2015). (A) Schematic representation (as a process description (Le Novere 2015)) for the ODE model of JNK pathway. (B) Response curve (phosphorylated JNK) as a function of the input stimulus (Stress) and characterization of the corresponding sigmoidal function with maximal amplitude \\(A\\), Hill exponent \\(H\\) and activation threshold \\(K_{50}\\). (C) Survival curves for neuroblastoma patients based on binarized \\(A\\), \\(K_{50}\\) and \\(H\\); binarization thresholds having been defined based on optimization screening on calibration cohort. Thus, the \\(R^2\\) of the output \\(H\\) is \\(0.39\\) while that of the combined inputs is \\(0.60\\). We can see from the decompositions that \\(H\\) derives most of its the value from ZAK, MKK4 and AKT (Figure 7.5A, right column), which were already the largest contributors in the combined evaluation of the inputs (Figure 7.5A, left column). However, \\(H\\) also includes an emerging non-linear share (\\(R^2=0.08\\)) that was not explained by the linear combination of inputs. Thus, combining \\(H\\) with the inputs in a survival prediction model does indeed allow to observe an added value with a global \\(R^2\\) of \\(0.68\\). In addition, the authors in the original study stressed the importance of positive feedback from JNK to MKK7 (Figure 7.4A). In its absence, we find that the value of \\(H\\) is almost reduced to zero, since not only its non-linear part (Figure 7.5, red share), but also its parts derived from inputs, disappear. Analyzing the other outputs of the model (\\(A\\) and \\(K_{50}\\)) reveals similar but less dramatic trends underlining the importance of this feedback which allows the model to capture a clinically relevant behaviour, assimilated by the authors to the capacity of cells to trigger apoptosis in case of stress. In the case of this model, the analyses provide a better understanding of how the model works with respect to survival prediction: the outputs partly summarize clinical information already present in the inputs but also reveal relevant emerging information. Figure 7.5: Decomposition of \\(R^2\\) for inputs and output for ODE model in Fey et al. (2015). (A) Results for the Fey model inputs and output \\(H\\) as defined in Figure 7.4A and B. (B) Same using the model without positive feedback between JNK and MKK7. Colors represent the origine of \\(R^2\\) contribution. In particular, for right colums (model\\(Y\\sim H\\)), the red share represent the proportion of the \\(R^2\\) of the output \\(H\\) that does not come linearly from the inputs, and therefore its emerging part. 7.2.2 Personalized logical models: BRAF inhibition in melanoma and colorectal cancers Similarly, it is appropriate to assess the relevance of the personalized logical models presented so far. Unlike the models of the previous sub-section, however, they integrate a much larger number of variables and the decomposition of \\(R^2\\) is no longer accessible, because of its computational cost, which increases exponentially with the number of variables. If we focus on the example best suited to these models, that of BRAF inhibition sensistivity, we can however reformulate the question more simply. Given that the most important predictor of the answer is the status of the BRAF mutation itself, do the personalized models allow us to do better or provide additional information? In the case of CRISPR data, the \\(R^2\\) of BRAF alone is \\(0.75\\), the \\(R^2\\) of the personalized scores from the models is \\(0.73\\), while the combination of the two increases the \\(R^2\\) to \\(0.83\\). In the absence of a precise decomposition, this gain can come either from the contribution of the other variables used in the model (the RNA levels of CRAF for example) or from the emergence of non-linear effects. In both cases, these figures are another way of expressing the remarks in section 6.2.4.1: thanks to the integration of other data and their organization in a framework based on literature knowledge, the model provides a more precise and complete vision of the response mechanisms. As positive as it is, this increase in \\(R^2\\) remains modest, illustrating that the main interest of these models is not necessarily a pure gain in predictive performance. Rather, it lies in their explanatory capacity and in their ability to support the investigation of mechanisms such as in section 6.2.4.1. In a complementary way, one could imagine extending these analyses to other nodes of the model and not only to its output in order to dissect even more precisely the information processing within the model. References "],
["clinical-evidence-generation-and-causal-inference.html", "Chapter 8 Clinical evidence generation and causal inference 8.1 Clinical trials and beyond 8.2 Causal inference methods to leverage data", " Chapter 8 Clinical evidence generation and causal inference Maudit soit le père de l’épouse du forgeron qui forgea le fer de la cognée avec laquelle le bûcheron abattit le chêne dans lequel on sculpta le lit où fut engendré l’arrière-grand-père de l’homme qui conduisit la voiture Wdans laquelle ta mère rencontra ton père!. Robert Desnos (La colombe de l’arche, 1923) The previous chapter proposed some tools to evaluate and quantify the value of mechanistic models, and in particular their outputs, with simple statistical tools. The latter, such as \\(R^2\\), are by no means specific to medical applications. One of the particularities of mechanistic cancer models, on the other hand, is the possibility of simulating treatments that imitate therapeutic interventions. Before tackling more precise questions, this chapter will therefore introduce certain clinical or statistical methods used to evaluate the effect of different types of treatments on patients. A more specific issue related to the evaluation of mechanistic models will be explored in the next chapter using these methods. Scientific content This short chapter introduces the framework of causal inference based on the literature and the description of causal inference in the preprint Béal and Latouche (2020). 8.1 Clinical trials and beyond 8.1.1 Randomized clinical trials as gold standards When it comes to evaluating the effect of a therapeutic intervention, the reference method in most cases in modern medicine is the randomized clinical trial, which will be described now in its simplest version. Without loss of generality, the rationale for this approach can be detailed for one drug, which will be referred to as \\(A\\) in the remainder of the chapter (Figure 8.1). The patients who can benefit from this drug, and therefore those eligible for the clinical trial, are first of all defined (specific disease, characteristics etc.). Then, they are randomly separated into two distinct groups, one receiving the new treatment to be evaluated (\\(A=1\\)) and the other generally receiving the treatment considered as standard of care, or a placebo if no validated treatment is available (\\(A=0\\)). A predefined treatment response criterion \\(Y\\) (viral load, tumor size, etc.) is then compared for the two groups to quantify the average treatment effect (ATE): \\[ATE= E[Y|A=1]-E[Y|A=0]\\] Thus it will be possible to say, for example, that “compared to patients who received the standard treatment, those treated with the new drug have a 20% lower tumor volume”. In this example, randomly choosing how the two groups of patients, treated and untreated, are constituted ensures a priori that the two groups are comparable. Indeed, it should be verified that the untreated patients were not on average suffering from more advanced cancers that are more likely to proliferate and grow. In this case, the difference in outcome between the groups could simply come from a difference in initial composition and not from a difference derived from therapeutic interventions. Random assignment of treatments therefore offers minimum guarantees concerning the characteristics of the two subgroups. Figure 8.1: Principles of randomized clinical trials.. This trial evaluates the impact of treatment \\(A\\). 8.1.2 Observational data and confounding factors The problem of comparability between the two groups is reinforced when the data used does not come from a clinical trial. In the remainder of this thesis these data will be called observational data. This means that in the available data, some patients were treated with the new drug (\\(A=1\\)) and others received the reference treatment (\\(A=0\\)). However, the assignment of treatment was not decided by the observer. This assignment was therefore made according to a protocol unknown to the observer which has no guarantee that the two groups are in fact comparable. The situation can be illustrated with a simple simulated example involving a confounding variable \\(C\\) in addition to the treatment variable \\(A\\) and the outcome variable \\(Y\\). If \\(Y\\) represents tumor volume and \\(A\\) the treatment to be evaluated, \\(C\\) could be a biomarker of cancer agressiveness. 1000 patients have been simulated for all variables in two different settings represented in Figures 8.2 and 8.3. In the first case (Figure 8.2), the outcome \\(Y\\) is positively correlated to \\(C\\) (more agressive tumors have bigger volume) and decreased when \\(A=1\\) (treatment decreases tumor volume). \\(C\\) has no influence on \\(A\\). The causal relationships between the variables and the associated coefficients used to simulate data are summarized in the directed acyclic graphs (DAG) in Figure 8.2A. The observed relations between variables in siimulated data are shown in Figure 8.2B, C and D. In particular, the theoretical influence of \\(A\\) on \\(Y\\) is recovered in the observed data since \\(E[Y|A=1]-E[Y|A=0]=-5.05\\) Figure 8.2: Analysis on observed data without confounder. (A) Directed acyclic graphs with causal relations between variables and parameters used to simulate data. (B) Influence of \\(C\\) on \\(A\\) in observed simulated data. (C) Same with \\(C\\) and \\(Y\\). (D) Same with \\(A\\) and \\(Y\\). In the second case (Figure 8.3), \\(C\\) has an influence on \\(Y\\): the more aggressive the tumor, the more likely the patient is to be treated with the new drug. In this case the simultaneous influence of \\(C\\) on \\(A\\) and \\(Y\\) makes it a real confounder. The direct observation of the differences in outcomes between treated and untreated patients reveals only a small benefit of the new treatment which does not correspond to the underlying reality used in these simulations since the theoretical causal influence of \\(A\\) on \\(Y\\) remained the same as in the previous case. The confounding factor prevents the nature of the causal link between A and Y from being simply inferred. Figure 8.3: Analysis on observed data with confounder. (A) Directed acyclic graphs with causal relations between variables and parameters used to simulate data. (B) Influence of \\(C\\) on \\(A\\) in observed simulated data. (C) Same with \\(C\\) and \\(Y\\). (D) Same with \\(A\\) and \\(Y\\). 8.2 Causal inference methods to leverage data Despite these difficulties, some statistical methods have been developed to derive estimates with a causal interpretation from observational data, under precise assumptions. This work will focus the potential outcomes framework (Rubin 1974). We will first describe briefly the fundamentals of this framework and different methods that are part of it. 8.2.1 Notations in potential outcomes framework First of all, the notations used in this and the next chapter are defined as follows. We will use \\(j=1,...,N\\) to index the individuals in the population. \\(A_j\\) and \\(Y_j\\) correspond respectively to the actual treatment received by individual \\(j\\) and the outcome. In the most simple case, treatment takes values in \\(\\mathcal{A}=\\{0, 1\\}\\), \\(1\\) denoting the treated patients and \\(0\\) the control ones. \\(Y_j\\) corresponds to the patient’s response to treatment. In the case of cancer it may be a continuous value (e.g size of tumor), a binary value (e.g status or event indicator), or even a time-to-event (e.g time to relapse or death). Only the first two cases will be discussed later. Finally, it is necessary to take into account the possible presence of confounders influencing both \\(A\\) and \\(Y\\) and denoted \\(C_j\\) for individual \\(j\\). The potential outcomes framework is also described as counterfactual because it defines variables like \\(Y_j(a)\\) to denote the potential outcome of individual \\(j\\) in case he has been treated by \\(A=a\\) which may be different from what we observe if \\(A_j\\neq a\\). These counterfactual variables make it possible to write the causal estimands. For instance, in this context, we can easily compute the difference in outcome between treated patients and control patients (Figure 8.4, left part): \\(E[Y | A=1] - E[Y | A=0].\\) However, this difference has no causal interpretation as it does not offer any guarantees as to the confounding factor, as an unbalanced distribution of \\(C\\) can induce biases. Thus we define another estimate: \\(E[Y(1)] - E[Y(0)].\\) In this case, we compare between two ideal cohorts (Figure 8.4, right part), one in which all patients have been treated (possibly contrary to the fact) and one in which all patients have been left in the control arm (once again, possibly contrary to the fact). Figure 8.4: Association, causation and their associated cohorts. Assocation analyses are based on observed cohorts and conditional probabilities. Causation analyses are based on counterfactual variables and cohorts. 8.2.2 Identification of causal effects The next question is whether it is possible to estimate the counterfactual variables \\(Y(A)\\) and under what conditions. The potential outcomes framework explicits assumptions of consistency, positivity and conditional exchangeability to estimate these counterfactual variables and therefore infer causal estimates from observational (non-randomized) data (Rubin 1974, Hernán and Robins (2020)). Consistency means that values of treatment under comparison represent well-defined interventions which themselves correspond to the treatments in the data: \\(\\textrm{if} \\: A_j=a, \\textrm{then} \\: Y_j(a)=Y_j.\\) Exchangeability means that treated and control patients are exchangeable, i.e if the treated patients had not been treated they would have had the same outcomes as the controls, and conversely. Since we usually observe some confounders we define conditional exchangeability to hold if cohorts are exchangeable for same values of confounding \\(C\\). Therefore conditional exchangeability will hold if there are no unmeasured confounding: \\(Y(a) {\\perp \\!\\!\\! \\perp}A | C.\\) Positivity assumption states that the probability of being administered a certain version of treatment conditional on \\(C\\) is greater than zero: \\(\\textrm{if} \\: P[C=c] \\neq 0, P[A=a | C=c] &gt;0.\\) Intuitively, this positivity condition is required to ensure that the defined counterfactual variables make sense and do not represent something that cannot exist. Under these three assumptions, there are different methods and estimators available to evaluate causal effects from observational data. Two of them will be described and applied to the same example as above: the description of the example and the failure of the direct methods are recalled in Figure 8.5A and B and two causal inference methods are illustrated in Figure 8.5C, D and E. 8.2.2.1 Standardization or parametric g-formula The first method is called standardization or parametric g-formula and it is the one that will be described in more detail in this chapter and the following one. It is based on the following equations: \\[\\begin{equation*} \\begin{aligned} E[Y(a)] &amp; = \\sum_{c} E[Y(a)|c] \\times P[c] \\\\ &amp; = \\sum_{c} E[Y(a)|a,c] \\times P[c,w] &amp;&amp;\\text{ (exchangeability } Y(a) \\perp \\!\\!\\! \\perp A | C \\text{ )} \\\\ &amp; = \\sum_{c} E[Y|a,c] \\times P[c] &amp;&amp;\\text{ (consistency)} \\end{aligned} \\end{equation*}\\] Thus the average effect of treatment on the entire cohort can be written with standardized means: \\[\\begin{equation} \\begin{aligned} E[Y(A=1)] - &amp; E[Y(A=0)] = \\\\ \\sum_{c} \\Big( &amp; E[Y | A=1, C=c]-E[Y | A=0, C=c]\\Big) \\times P[C=c] \\end{aligned} \\end{equation}\\] Computationally, non-parametric estimation of \\(E[Y | A=a, C=c]\\) is usually out of reach. Thus, on real-world dataset, \\(E[Y | A=a, C=c]\\) is estimated through outcome modeling and explicit computation \\(P[C=c]\\) is replaced by its empirical estimate. In the simple example depicted in Figure 8.5A, a linear model of the outcome (\\(Y\\sim C+A\\)) is fitted on observed data. This model is then used to infer \\(E[Y | A=1, C=c]\\) and \\(E[Y | A=0, C=c]\\) for each patient with covariate \\(C=c\\) (Figure 8.5C). By averaging these values over the whole cohort the confounding effect is corrected and the estimator is much closer to the true value \\(-5\\) than the naive estimates (Figure 8.5D and B). Figure 8.5: Causal inference methods on a simple example. (A) Directed acyclic graphs with causal relations between variables and parameters used to simulate data. (B) Association between \\(A\\) and \\(Y\\) from observed data. (C) Some simulated samples/patients with their original variables (\\(C\\), \\(A\\) and \\(Y\\)), variables from outcome model (\\(E[Y|A=0,c)]\\), \\(E[Y|A=1,c)]\\)) and weights from treatment model (\\(W^A\\)). (D) Standardized causal effect of \\(A\\) on \\(Y\\) based on and outcome modeling. (E) IPW causal effect of \\(A\\) on \\(Y\\) based on weights derived from treatment modeling; in this panel weights are taken into account in boxplots and estimations. 8.2.2.2 Inverse probability weighting (IPW) and propensity scores Based on the same counterfactual framework, it is possible to build another class of models, called marginal structural models (Robins, Hernan, and Brumback 2000), from which we derive estimators different from the standardized estimators called inverse-probability-of-treatment weighted (IPW) estimators (Cole and Hernán 2008). IP weighting is equivalent to creating a pseudo-population where the link between covariates and treatment is cancelled. In the case of binary treatment \\(A \\in {0 ,1}\\), weights are defined for each patient as the inverse of the probability to have received the version of treatment he or she actually received, knowing his or her covariates: \\[W^A=\\dfrac{1}{f[A|C]} \\text{ with } f[a|c]=P[A=a|C=c],\\] \\(f[a|c]\\) being called the propensity score, i.e., the probability to have received the treatment \\(A=a\\), given the covariates \\(C=c\\). Again, propensity scores will be estimated in later examples using a parametric model. In this case with a binary treatment \\(A\\), a logistic treatment model is used (\\(A\\sim C\\)) to derive the weights \\(W^A\\) (Figure 8.5C). Note that propensity scores are also useful for positivity investigations since values very close to \\(0\\) or \\(1\\) may indicate (quasi-)violations of positivity. Under the same hypothesis of exchangeability, positivity and consistency we can derive the modified Horvitz-Thompson estimator (Horvitz and Thompson 1952; Hernán and Robins 2020): \\[\\begin{equation} E[Y(a)]=\\dfrac{\\hat{E}[I(A=a)W^{A}Y]}{\\hat{E}[I(A=a)W^A]}, \\tag{8.1} \\end{equation}\\] \\(I\\) being the indicator function. Once again, this method brings estimates closer to the true causal effect by correcting for the influence of the confounder (Figure 8.5E). In summary, evaluating the effect of a treatment requires isolating its impact from that of all confounding factors. This can be done in a randomized clinical trial designed for this purpose. However, there is a great amount of other data available that may not have been generated in this rigorous framework. It is nevertheless possible to draw causal interpretations from them, under certain hypotheses, thus offering insights for a posteriori statistical evaluation of specific therapeutic strategies. References "],
["chapter-precision.html", "Chapter 9 Causal inference for precision medicine 9.1 Precision medicine in oncology 9.2 Emulating clinical trials to evaluate precision medicine algorithms 9.3 Causal inference methods and precision medicine 9.4 Application to simulated data 9.5 Application to PDX 9.6 Limitations and perspective", " Chapter 9 Causal inference for precision medicine Felix qui potuit rerum cognoscere causas. Virgil (Georgics, 29 BC) Throughout this manuscript, we first described the complexity of cancer mechanisms, through the diversity of genetic alterations or non-linear signaling pathways. This complexity naturally led to the choice of systemic modeling approaches and in particular mechanistic models whose explicit nature facilitates the study of the effects of new molecular perturbations such as treatments. The simple study of the response to BRAF inhibitors has thus required the consideration of many other genes and pathways. This final chapter proposes to take the complexity a step further by considering different treatments. The diversity of patients’ molecular profiles suggests that the best treatment is not necessarily the same for all patients: this is what is known as precision medicine. This is already a clinical reality in oncology that could be reinforced in the future by the emergence of new computational models of cancer, whether mechanistic or not. How then can we assess the relevance of these models in their ability to guide patient treatment? Scientific content This chapter presents an extension of the causal inference framework to quantify the value of precision medicine strategies. This work is currently under revision and is available as a preprint in Béal and Latouche (2020). All code is available in the dedicated GitHub repository 9.1 Precision medicine in oncology It is first important to understand what is meant by the concept of precision medicine in the treatment of cancer patients in order to place subsequent questions in a plausible clinical framework. 9.1.1 An illustration with patient-derived xenografts Precision medicine is based on the diversity of treatment responses observed in different tumors. It has already been observed in previous chapters about BRAF inhibition that different cell lines respond differently to a particular treatment. A broader analysis of pre-clinical data shows that the same is true for the vast majority of treatments. It would be possible to illustrate this using the same data from cell lines extended to other drugs. However, because of the more directly clinical impact of the issues discussed in this chapter, the analyses presented below will focus on another type of data that is closer to patient data: patient-derived xenografts (PDX). A PDX is a tumor tissue that has been removed from a patient and implanted into immunodeficient mice (Hidalgo et al. 2014). Unlike cell lines, which are in vitro models, PDXs are in vivo models that allow cancer cells to evolve in a more realistic microenvironment. In the same way as for cell lines, PDX can be used for drug screening. The data used in this chapter come from a study by Gao et al. (2015) which contains several hundred tumors and more than fifty drugs. Not all drugs having been tested for all tumors and details of the drugs and types of cancer tested are available in the appendix A.2. This dataset was generated following the “one animal per model per treatment” approach (\\(1 \\times 1 \\times 1\\)), the principles of which are summarized in Figure 9.1A. It should be noted that different drug response metrics are computed in the source data, two of which will be used in the analyses. The first one is continuous and called Best Average Response in the data, it is based on the variation of the tumor volume after treatment, the lower values (and especially negative) corresponding to better responses. The second one is originally categorical and based on a modified Response Evaluation Criteria In Solid Tumors (RECIST) criteria. It was binarized for this study so that the responders have a score of \\(1\\) and non-responders \\(0\\). The details of the definition and distribution of these metrics are given in appendix A.2. Figure 9.1: Principles of PDX screening. (A) Schematic pipeline for PDX screening with tumor biopsies from one patient divided in several pieces later implanted in similar immunodeficient mice. Each mouse is then treated with a different drug; the collection of mice that have received tumor samples from the same patient but have been treated with different drugs therefore gives access to several outcomes for the same tumor of origin. (B) Corresponding counterfactual variables. In order to illustrate the diversity of response to treatment, the database is momentarily restricted to the 4 most widely tested drugs and the 180 tumors (or PDX models) that were evaluated for all four drugs. The four chosen drugs target different pathways: binimetinib (MAPK inhibitor), BKM120 (PIK inhibitor), HDM201 (MDM2 inhibitor) and LEE011 (CDK inhibitor). In Figure 9.2A the 4 treatments show a high variability of response, with a slight advantage for BKM120 and binimetinib on average over all tumors. However, each of the treatments was found to be the most effective of the 4 for a significant proportion of tumors (Figure 9.2B), with binimetinib and BKM being the best treatment for one-third of tumors each and LEE011/HDM201 sharing the remaining one-third of tumors. It thus appears that in view of the molecular diversity of tumors and the increasing number of treatments available, it does not seem advisable, according to these preclinical data, to treat all tumors with the same gold-standard treatment. Furthermore, the tissue of origin of the tumors in this example does not appear to be the main determinant of tumor preference for certain treatments. Figure 9.2: Differences in drug response for 4 drugs and 180 tumors a call for precision medicine. (A) Distribution of treatment response for the 4 different drugs, each with all 180 tumors. (B) Number of times each of the 4 drugs is the most effective for a given tumor, distribution by tissue of origin. 9.1.2 Clinical trials and treatment algorithms These remarks can be extended to patients. Thus, precision medicine (PM) consists in assigning the most appropriate treatment to each patient according to his or her characteristics, usually genomic alterations for cancer patients (Friedman et al. 2015; De Gramont et al. 2015). At the individual level, targeted treatments have provided relevant solutions for patients with specific mutations (Abou-Jawde et al. 2003). Putting together these various treatments, some precision medicine strategies can be defined. Based on the genomic profile of the patient, the treatment most likely to be successful is chosen. If the information available is reliable, precision medicine can thus be reduced to a treatment algorithm that takes as input the molecular characteristics of the patient’s tumor and outputs a recommendation of treatment. An example of such a treatment algorithm from the SHIVA clinical trial by Le Tourneau et al. (2015) is shown in Figure 9.3 where different treatments are associated with different alterations. In this case, the treatment algorithm can be considered as an aggregation of the medical knowledge accumulated on the individual biomarkers. Figure 9.3: An example of a precision medicine treatment algorithm: the SHIVA clinical trial. Specific molecular alterations and their associated treatments, as proposed in the SHIVA clinical trial (Le Tourneau et al. 2015). 9.1.3 Computational models to assign cancer treatments But the treatment algorithm example in Figure 9.3 could be more complex. Indeed, previous chapters have stressed, for example, that being mutated for the BRAF gene is not the only predictor of response to an inhibitor of BRAF (here Vemurafenib). The same is true for most treatments that could benefit from more global and systemic analyses, taking into account more variables and their interactions. This complexity would require the use of computational methods. It is on this point that this chapter links to the previous ones. Some of the cancer models studied throughout this thesis, or their future developments, could be interpreted as treatment algorithms. Indeed, a model capable of predicting the response to a single treatment does not necessarily allow the inference of precision medicine strategies. On the other hand, a model capable of predicting a patient’s response to different treatments is also capable of indicating which one is the best. Such models would then move from systems biology to systems therapeutics (Hansen and Iyengar 2013), taking patients’ geneomic features as inputs and outputting a treatment recommendation. In theory, mechanistic models seem to be suitable for this purpose since their explicit representation of genes and proteins makes it possible to simulate the effect of different therapeutic interventions. However, the feasibility of designing and calibrating such a model has yet to be demonstrated. Other types of models are being studied that could achieve these goals. For example, some recent approaches propose the use of deep learning to provide a computational tools for predicting the growth of cells (Ma et al. 2018), or even the sensitivity of cell lines to different treatments (Manica et al. 2019). In short, if no computational model is sufficiently developed to date to replace the clinician, the emergence of this type of tool is likely in the medium term. This raises the question of how to assess the clinical value of the precision medicine strategies (and corresponding treatment algorithms) derived from these models. For the sake of generality, this question will be addressed more broadly in the following without reference to models as a possible source of the treatment algorithm: how to evaluate the clinical impact of a precision medicine strategy and the treatment algorithm? The methods presented will indeed be the same, whether the algorithm evaluated comes from a model or from the knowledge of clinicians as in Figure 9.3. In the spirit of this thesis, the question nevertheless finds its origin in the first hypothesis related to models. 9.2 Emulating clinical trials to evaluate precision medicine algorithms 9.2.1 Objectives and applications The question then arises of how to quantify the clinical benefit provided by these treatment algorithms. Some precision medicine clinical trials have been proposed, demonstrating both the feasibility of collecting information about mutations (Le Tourneau et al. 2015) or RNA (Rodon et al. 2019) in real-time and the clinical benefit that can be expected from these approaches for some patients (Coyne, Takebe, and Chen 2017). However, the increasing abundance of genomic data and biological knowledge make it progressively easier to establish new algorithms for precision medicine, either directly based on physician knowledge or provided by computational models (Hansen and Iyengar 2013). For practical reasons it is not possible to propose a real clinical trial for each new precision medicine algorithm or for any variants, comparing standard of care with new algorithm-based therapeutic strategies. Therefore, this work provides a method to assess the clinical impact of proposed PM treatment algorithm based on already generated data, emulating precision medicine clinical trials and analyzing them in the causal inference framework (Hernán and Robins 2016). First we will define the causal estimates of the precision medicine effects (later referred to as causal estimates) we want to assess, and the corresponding ideal clinical trials one would like to perform. Next, we will define the notations and the causal framework we use to infer the causal effects from observational data with multiple versions of treatment, based on the previous work by VanderWeele and Hernan (2013). The main principles of the potential outcome framework having been introduced in the previous chapter, an extension to the case of precision medicine will be described, focusing on the multiplicity of treatment versions, i.e., targeted drugs. Then we will apply these methods to simulated data in order to investigate the different biases of the candidate methods. An example scenario will be presented and a RShiny interactive application has been developed to further explore other user-defined settings. Finally, the analysis of data from patient-derived xenografts (PDX) makes it possible both to apply the methods to pre-clinical situation and to have data approximating the counterfactual responses, thus enabling further validation of the proposed estimation methods. 9.2.2 Target trials for precision medicine: definition of causal estimates We first specify the precision medicine effects that are to be estimated. These effects will finally be estimated based on observational data through the causal framework and target trial emulation (Hernán and Robins 2016). In this context the notion of target trial refers to the real clinical trial whose estimates are sought to be reproduced through causal inference. Thus, if we think in terms of clinical trials, we are not trying to prove or quantify the superiority of one treatment over another but rather to evaluate the clinical utility of a precision medicine strategy assigning treatments based on genomic features of patients. This is therefore closer to the well-studied biomarker-based designs for clinical trials (Freidlin, McShane, and Korn 2010). In a way, it is a matter of extending these unidimensional biomarker-based designs to multidimensional strategies that allow a choice between quite a number of different treatments. The potentially large number of treatments thus prompts us to draw more inspiration from scalable biomarker-strategy designs than biomarker-stratified designs (Freidlin, McShane, and Korn 2010). We can draw a methodological parallel with some trials like the Tumor Chemosensitivity Assay Ovarian Cancer study in which a biochemical assay guides the choice of preferred chemotherapy for patients in a panel of twelve different treatments (Cree et al. 2007). More recently, some clinical trials have been proposed that include precision medicine strategies, particularly in oncology (Le Tourneau et al. 2015; Flaherty et al. 2020). On the basis of these clinical examples, we propose three different target trials and their corresponding causal estimates, the clinical relevance of which may vary according to medical contexts. Each target trial contains a precision-medicine directed arm in which patients are treated in accordance with the precision medicine algorithm recommendations but they are differentiated from each other by alternative control arms (Figure 9.4). Causal effects will be estimated solely on patients eligible for the assignment of a personalized treatment, i.e., those for whom the treatment algorithm is able to recommend a drug. Figure 9.4: Target trials to estimate causal effect of precision medicine (PM) algorithm versus different controls. Patients are first screened according to their eligibility for the algorithm: based on their genomic characteristics patients are recommended a specific treatment (eligible) or not (no eligible). Then eligible patients are randomized and assigned either to PM-directed arm or to one of the alternative control arms (\\(\\text{CE}_1\\), \\(\\text{CE}_2\\) or \\(\\text{CE}_3\\)) 9.2.2.1 First causal effect (\\(\\text{CE}_1\\)): comparison with a single standard The first possible target trial is to compare the precision medicine arm with a control arm in which all patients have been treated with the same single treatment. This could classically be the current standard of care applied to all patients (e.g chemotherapy cancer treatment). 9.2.2.2 Second causal effect (\\(\\text{CE}_2\\)): comparison with physician’s assignment of drugs Then, in order to propose a more comprehensive clinical assessment, we propose a second causal effect, comparing the PM arm with the current clinical practice, i.e the assignment of the same targeted treatments by physicians in the absence of the algorithm. This implicitly means comparing two PM strategies: the one derived from the algorithm and the one that corresponds to current physician’s knowledge. Unlike the former, the latter may not be perfectly deterministic depending on the heterogeneity of medical knowledge or practices. This way of defining \\(\\text{CE}_2\\) by focusing on the doctor’s assignment of the same treatments stems from our question of interest: to quantify the relevance of the algorithm itself. Another possibility would have been to compare the precision medicine arm with the doctor’s treatments, allowing him to use treatments other than those of the PM arm, such as the gold-standard one described in \\(\\text{CE}_1\\). But the differences between the arms could then be biased by the use of treatments with different overall efficacy, changing the focus of the question. We will therefore stick to the first definition, which is more focused on the relevance of the algorithm. 9.2.2.3 Third causal effect (\\(\\text{CE}_3\\)): comparison with random assignment of drugs Finally, we define the \\(\\text{CE}_3\\) effect comparing the PM arm with a control arm using exactly the same pool of treatments assigned randomly. In this case, we measure the ability of the PM algorithm to assign treatments effectively based on genomic features of patients. This comparison has already been considered in the context of biomarker-based clinical trials (Sargent et al. 2005). Although this comparison with random assignment is methodologically relevant, it may not make sense from a clinical point of view if the common clinical practice already contains strong indications (or contraindications) for some patient-treatment associations. 9.3 Causal inference methods and precision medicine 9.3.1 A treatment with multiple versions The statement of the potential outcomes framework implicitly implies the uniqueness of the versions of the treatment (Rubin 1980) or at least the treatment variation irrelevance (VanderWeele 2009). In the precision medicine case, the multiplicity of treatment versions is inherent: a given treatment status may encompass several drugs since a patient may be associated with several molecular agents based on his or her genomic characteristics. \\(A\\) can be seen as a compound treatment (Hernán and VanderWeele 2011) or a treatment with multiple versions (VanderWeele and Hernan 2013). Therefore, we define a variable \\(K_j\\) denoting the version of treatment administered to individual \\(j\\). If \\(A_j=a\\) is the arm to which the patient is assigned, \\(K^a_j\\) is the molecule received, the version of treatment \\(A=a\\) (e.g a specific anti-cancer drug) and \\(K^a_j \\in \\mathcal{K}^a\\), the set of versions of treatment \\(A=a\\). In our precision medicine problem, \\(A=0\\) will denote control patients and \\(A=1\\) the patients treated with an anti-cancer drug of the precision medicine pool. \\(\\mathcal{K}^1=\\{k^1_1, ..., k^1_P\\}\\) is the set of \\(P\\) possible targeted treatments for \\(A=1\\) patients. For the sake of simplicity we will assume that there is only one treatment version for \\(A=0\\) controls, \\(\\mathcal{K}^0=\\{k^0\\}\\). We also need to define other counterfactual variables like \\(K^a_j(a)\\), the counterfactual version of treatment \\(A=a\\) if the subject had been given the treatment level \\(a\\). Thus, we finally write the counterfactual outcome as \\(Y_j(a,k^a)\\) for individual \\(j\\) when treatment \\(A\\) has been set to \\(a\\), using \\(k^a\\) as the version of treatment \\(a\\), with \\(k^a \\in \\mathcal{K}^a\\). Causal relations between variables \\(C\\), \\(A\\), \\(K\\) and \\(Y\\) are depicted in the causal diagram in Figure 9.5. It should be noted that \\(A\\) has no direct influence on \\(Y\\), its only effect is entirely mediated by \\(K\\), which is the real treatment in the pharmacological sense. Figure 9.5: Causal diagram illustrating relations between variables under multiple versions of treatment. Treatment \\(A\\), version of treatment \\(K\\), outcome \\(Y\\), and confounding variables \\(C\\) and \\(W\\) are placed in a causal digram, along with their interpretation in the precision medicine application. In this context, we can also define the assignment of a version of treatment for patients eligible for precision medicine algorithm. It is important to note that not all patients are necessarily eligible for the precision medicine strategy. Indeed, the treatment assignment algorithm relies on targetable alterations to establish its recommendations. In the absence of these, no recommendation can be offered to the patient. We denote \\(\\mathcal{C}^{PM}\\) the set of eligible patient profiles and consequently define the drug assignment algorithm as the function \\(r\\) which associates to each \\(C\\) a precision medicine treatment version \\(K\\) such as: \\[\\forall j \\in [[ 1, N ]], \\: \\textrm{if} \\: C_j \\in \\mathcal{C}^{PM}, r(C_j) \\in \\mathcal{K}^{1}\\] 9.3.2 Causal inference with multiple versions Consequently, the multiplicity of versions prevents direct application of the framework as described in section 8.2.2. The theoretical framework has however been extended to causal inference under multiple versions of treatment and some identifiability conditions and properties have been studied, especially in the seminal article by VanderWeele and Hernan (2013). One of the first required adaptation to identify some causal effects is to distinguish between confounders \\(C\\) and \\(W\\) (Figure 9.5). \\(W\\) indicates a collection of covariates that may be causes of treatment \\(A\\) or version of treatment \\(K\\) but are not direct causes of \\(Y\\). These covariates are of special interest for causal effects identification under multiple versions of treatment. \\(C\\) indicates all other covariates. In our precision medicine settings, the genomic features of patients may define the eligibility to precision medicine and therefore affect \\(A\\). They may also be used to define the version of treatment \\(K\\). And finally they can influence the response to treatment \\(Y\\). Thus, the genomic features of patients are a typical example of type \\(C\\) confounders. All causal relationships are summarized in Figure 9.5. Please note that no \\(W\\) variable is present in the applications provided later because all the covariates considered in this situation were likely to influence \\(A\\), \\(K\\) and \\(Y\\) and therefore belonged rather to the covariates of type \\(C\\). However all subsequent formulas and definitions have been derived taking into account \\(W\\). We summarize here some general observations from VanderWeele and Hernan (2013) regarding the extension of the framework to multiple versions before discussing specific estimates of interest of our precision medicine settings in the next section. These two sections will be based exclusively on the method called standardization or parametric g-formula described in section 8.2.2.1. The adaptation of other methods to precision medicine will be discussed more briefly in section 9.3.4. First of all, the identifiability conditions have to be adapted. The consistency assumption for instance is extended to \\(K\\): \\[\\textrm{if} \\: A_j=a, \\textrm{then} \\: K_j^a(a)=K_j^a\\]. Then, the conditional exchangeability or no-unmeasured confounding assumptions, may be stated in two different ways, either without or with versions of treatment: \\[\\begin{equation} Y(a) \\perp \\!\\!\\! \\perp A | (C,W) \\tag{9.1} \\end{equation}\\] \\[\\begin{equation} Y(a, k^a) \\perp \\!\\!\\! \\perp \\{A, K\\} | C \\tag{9.2} \\end{equation}\\] If equation (9.1) holds, we can derive a new version of the standardised estimator with multiple versions of treatment (VanderWeele and Hernan 2013): \\[\\begin{equation} E[Y(a)] = E[Y(a, K^a(a))] = \\sum_{c,w} E[Y | A=a, C=c, W=w] \\times P[c,w] \\tag{9.3} \\end{equation}\\] Specifically, it should be noted that we need to add \\(W\\) in the set of covariates that must be taken into account in standardization, and we need positivity to hold for \\(C\\) and \\(W\\), i.e., \\(0&lt;P[A=a|C=c,W=w]&lt;1\\). Detailed proof of equation (9.3) is provided in appendix C.2.1. Equation (9.3) paves the way to overall treatment effect assessment since \\(E[Y(1, K^1(1))]-E[Y(0, K^0(0))]\\) would estimate the effect of treatment \\(A=1\\) compared to \\(A=0\\) with current versions of treatment. Conversely, estimating a treatment effect for a given unique version of treatment \\(E[Y(a,k^a)]\\) would require to check the exchangeability with regard to versions \\(K\\) and therefore to hold equation (9.2) true (VanderWeele and Hernan 2013): \\[\\begin{equation} E[Y(a, k^a)] = \\sum_{c} E[Y | A=a,K^a=k^a, C=c] \\times P[c] \\tag{9.4} \\end{equation}\\] Similarly, we can define \\(G^a\\) a random variable for versions of treatment with conditional distribution \\(P[G^a=k^a| C=c]=g^{k^a,c}\\) and assuming the equation (9.2) to be true we can derive the following formula and its formal proof in appendix C.2.2: \\[\\begin{equation} E[Y(a, G^a)] = \\sum_{c,k^a} E[Y | A=a,K^a=k^a, C=c] \\times g^{k^a,c} \\times P[c] \\tag{9.5} \\end{equation}\\] In this case, to allow estimation of the right-hand side of the equation, positivity will be defined as \\(0&lt;P[A=a, K^a=k^a|C]&lt;1\\). 9.3.3 Application to precision medicine In the context of the potential outcomes framework extended to treatments with multiple versions, it is therefore possible to apply equations (9.3) and (9.5) in order to define and estimate the precision medicine causal effects previously described in section 9.2.2. \\(A=0\\) corresponds to control patients with \\(\\mathcal{K}^0=\\{k^0\\}\\) and \\(A=1\\) to patients treated with a targeted treatments. It is important to notice that from this point on we systematically restrict ourselves to patients eligible for the precision medicine algorithm, i.e., to individuals \\(j\\) such as \\(C_j \\in \\mathcal{C}^{PM}\\). 9.3.3.1 \\(\\text{CE}_1\\) estimation \\(\\text{CE}_1\\) is a comparison between the precision medicine arm and a single version control arm: \\[\\begin{equation} \\text{CE}_1 = E[Y(1, r(C)] - E[Y(0, k^0)] \\tag{9.6} \\end{equation}\\] In details, \\(E[Y(1, r(C)]\\) can be derived from equation (9.5) in the case where \\(g^{k^a,~c}=1\\) if \\(k^a=r(c)\\) and \\(g^{k^a,~c}=0\\) otherwise: \\[E[Y(1, r(C)] = \\sum_{c} E[Y | A=1,K^1=r(c), C=c] \\times P[c]\\] Then, \\(E[Y(0, k^0)]\\) and \\(E[Y(1, k^1_{ref})]\\) can be derived from equation (9.4): \\[ E[Y(0, k^0)] = \\sum_{c} E[Y | A=0, C=c] \\times P[c] \\] Alternatively, if one wants to use as control only one of the treatments used in the PM arm the previous estimate could be replaced by the following one: \\[E[Y(1, k^1_{ref})] = \\sum_{c} E[Y | A=1,K^1=k^1_{ref}, C=c] \\times P[c]\\] It should be noted that \\(\\text{CE}_1\\), like \\(\\text{CE}_2\\) and \\(\\text{CE}_3\\) presented later, depends on the PM algorithm of interest \\(r\\). \\(\\text{CE}_i\\) could therefore also be written \\(\\text{CE}_1(r)\\). 9.3.3.2 \\(\\text{CE}_2\\) estimation Then, \\(\\text{CE}_2\\) is written using \\(K^1(1)\\) the PM targeted treatment that would have been assigned to the patient by the physician if the patient had been allocated in arm \\(A=1\\) with PM targeted treatments: \\[\\begin{equation} \\text{CE}_2 = E[Y(1, r(C)] - E[Y(1, K^1(1))] \\tag{9.7} \\end{equation}\\] \\(E[Y(1, K^1(1))]\\) is derived from equation (9.3): \\[E[Y(1, K^1(1))] = \\sum_{c,w} E[Y | A=1, C=c, W=w] \\times P[c,w]\\] 9.3.3.3 \\(\\text{CE}_3\\) estimation Defining \\(G^1\\) as the random distribution of versions of treatment \\(k^1 \\in \\mathcal{K}^{1}\\), \\(\\text{CE}_3\\) expresses as: \\[\\begin{equation} \\text{CE}_3 = E[Y(1, r(C)] - E[Y(1, G^1)] \\text{ with } P[G^1=k^1_i]=\\dfrac{1}{|\\mathcal{K}^1_{PM}|}, \\tag{9.8} \\end{equation}\\] \\(|.|\\) denoting the cardinality of the set. In this formula, \\(E[Y(1, G^1)]\\) can be derived from equation @ref{distrib-treatment-effect}: \\[E[Y(1, G^1)] = \\dfrac{1}{|\\mathcal{K}^1_{PM}|} \\times \\sum_{c,k^1_i} E[Y | A=1, K^1=k^1_i, C=c] \\times P[c]\\] 9.3.4 Alternative estimation methods For the sake of simplicity and brevity, we only detailed the standardization in previous sections. However, other popular candidate methods can be used. Estimators based on the inverse probability weighting (IPW) and targeted maximum likelihood estimation (TMLE) will also be computed in the following sections. IPW has the particularity of not trying to model the outcome but rather the process of assigning treatments. Its theoretical bases have been described in section 8.2.2.2 and the details of its adaptation to multiple versions of treatment is provided in appendix C.2.3. The TMLE methods are of a different nature (Van der Laan and Rose 2011). They combine an outcome model and a treatment model in order to obtain a doubly robust estimate, i.e., an estimate that is robust to a possible misspecification of either model. Moreover, the estimation is done in several steps in order to optimize the equilibrium bias-variance, not for the overall distribution of the data but specifically for the causal effect of interest. These methods also have the particularity of being very often used with machine learning algorithms to fit the outcome or treatment models, instead of the parametric models classically used in standardization and IPW methods. A more detailed description of TMLE properties and the choices that have been made to adapt it to the problem of precision medicine are available in appendix C.2.4. 9.3.5 Code The methods detailed above have been implemented in R and applied to simulate data and PDX data. The code is provided in the form of R notebooks (simulated data and PDX data) as well as in the form of an RShiny interactive application (simulated data only). All of these files are available in the dedicated GitHub repository. 9.4 Application to simulated data The proposed methods are first tested on simulated data in order to check the performance of the estimators in finite sample sizes. 9.4.1 General settings Table 9.1: Intercepts and linear coefficients in the linear models specified to simulate data Response variable Intercept Lin. coeff. \\(Y \\sim C_1\\) Lin. coeff. \\(Y \\sim C_2\\) \\(Y(0,k^0)\\) 0 0 15 \\(Y(1,k^1_1)\\) -25 -15 10 \\(Y(1,k^1_2)\\) 0 0 -20 Using the R package lava based on latent variable models, we simulate a super-population of 10000 patients with variables \\(C\\), \\(A\\), \\(K\\) and \\(Y\\) as in Figure @ref{fig:DAG-multiple}. We first define two independent binary variables \\(C_1\\) and \\(C_2\\), representing mutational status of genomic covariates, with a mutation prevalence of 40%. By analogy with the PDX data, \\(Y\\) represents the evolution of tumor volume and a low value (a fortiori negative) corresponds to a better response. Y is therefore defined as a continuous gaussian variable. For each counterfactual variable of response \\(Y(a, k^a)\\), we specify the intercept and the linear regression coefficients regarding influence of \\(C_i\\) as described in Table 9.1. Lower intercepts correspond to better responses/more efficient drugs. Similarly, a negative regression coefficient between \\(Y(a, k^a_i)\\) and \\(C_j\\) means that the gene \\(C_j\\) improves the response to \\(k^a_i\\). So all in all, \\(k^1_1\\) has the best basal response (lowest intercept). \\(C_1\\) (resp. \\(C_2\\)) improves the response to \\(k^1_1\\) (resp. \\(k^1_2\\)). The treatment algorithm of precision medicine is in line with these settings since patients mutated in \\(C_1\\) (regardless their \\(C_2\\) status) are recommended to take \\(k^1_1\\) and patients mutated for \\(C_2\\) only are recommended to take \\(k^1_2\\). Patients without mutations are not eligible for precision medicine and not taken into account in the computations. Since \\(k^1_1\\) has the bast basal response we assume it is assigned with greater probability by the physician and implement the following distribution of observed treatments: \\[P[K=k^1_1]=0.5 \\text{ and } P[K=k^1_2]=P[K=k^0]=0.25\\] A super-population of 10000 patients is then generated. 1000 cohorts of 200 patients are sampled without replacement within this super-population which, with the prevalences defined for the mutations, corresponds to an effective sample size of about 130 patients eligible for the PM algortithm. the causal effects \\(\\text{CE}_1\\), \\(\\text{CE}_2\\) and \\(\\text{CE}_3\\) are computed based on different methods on the sub-cohort eligible for precision medicine: True effects, using all simulated counterfactuals for all patients Naive effects, using observed outcomes only for both PM and control arms Corrected effects: using observed outcome and standardized estimators (Std), inverse probability weighting (IPW) and targeted maximum likelihood estimators (TMLE). 9.4.2 Simulation results First, the distribution of data in the super-population of 10,000 patients can be observed in Figure @ref{simulation-results}A, illustrating the different relations and differences described above. In particular, \\(Y(1, k^1_1)\\) (resp. \\(Y(1, k^1_2)\\)) is lower for \\(C_1\\)-mutated (resp. \\(C_2\\)-mutated) patients. It can also be seen that the response to precision medicine (\\(Y(1, r(C))\\)) differs according to the groups: patients mutated for \\(C_1\\) only have the best response, followed by patients mutated for both \\(C_1\\) and \\(C_2\\) and patients mutated for \\(C_2\\) only. There is therefore a heterogeneity of responses to PM which encourages to take into account the groups of patients and their PM versions. The right side of Figure 9.6A shows the deterministic assignment of the recommended PM treatment (\\(r(C)\\)) to each patient profile and the unbalanced distribution of observed treatments (\\(K\\)) with a predominance of \\(k^1_1\\). Figure 9.6: Causal effects of precision medicine strategy with simulated data. (A) Main variables and relations in the simulated super-population. From left to right: categories of patient based on their mutations; responses to \\(k^0\\), \\(k^1_1\\), \\(k^1_2\\) and precision medicine \\(K=r(C)\\); repartition of patients regarding their precision medicine drug and their assigned treatment in observed data. (B) Distribution and deviation of \\(\\text{CE}_1\\) estimates based on different methods, deviation scores being computed based on mean absolute error (MAE). (C) Same for \\(\\text{CE}_2\\). (D) Same for \\(\\text{CE}_3\\). In the first target trial, true \\(\\text{CE}_1\\) estimates in the sampled cohorts are distributed around -40 (Figure 9.6B), confirming the superiority of the PM arm over the control arm as defined in the simulation parameters. Not all methods of estimating the causal effect perform equally well. The so-called naive estimate and the one based on IPW show a net bias. The over-representation of the most advantaged patients by PM tends to cause these methods to overestimate the benefit of PM, as can also be seen in the deviation plots. The same trends are observed for \\(\\text{CE}_2\\) and \\(\\text{CE}_3\\) (Figure 9.6C and D) where the differences are even more drastic. The mean absolute error of the naive method is thus divided by more than 2 when using standardized estimates or the TMLE. In order to further dissect the influence of simulation parameters on estimation performances, a slightly different simulation scenario with equal probabilities of observed treatments has been studied: \\[P[K=k^0]=P[K=k^1_1]=P[K=k^1_2]=\\dfrac{1}{3}\\] In this case, the random and balanced assignment of the observed treatments logically removes the systematic biases of the naive method by providing them with more randomized data. However, the corrections made by the proposed methods of causal inference, and in particular standardization and TMLE, still reduce the variances in the estimates due to the heterogeneity of the effects of precision medicine as a function of molecular profiles. Randomly, some sampled cohorts are indeed found with an association between \\(C\\) and observed \\(K\\), thus generating a confounding effect that the causal methods partially correct. The simulated data allow us to imagine an almost infinite number of scenarios depending on the number of biomarkers taken into account in the algorithm, the number of different treatments, the dependencies of their responses or the distribution of treatments observed. In order to allow easy exploration of these scenarios without having to master the underlying R code, an interactive RShiny application has been developped. It can be accessed by locally running the R source file or by using the online version embedded in Figure 9.7. Readers with the ability to run the application locally are encouraged to favor this option because the hosting of the online application is limited to a maximum amount of time per month. The application allows certain additional analyses not presented in this manuscript, in particular the linking of biases observed in the sampled cohorts according to their composition (prevalence of mutations, treatments, etc.). It is thus possible to trace the origin of the biases. Figure 9.7: RShiny interactive application to investigate various simulation scenarios of precision medicine evaluation. It is possible to run the application locally with the source R file or online with the version hosted on the shinyapps.io server 9.5 Application to PDX The method is then applied to public data from patient-derived xenografts (Gao et al. 2015), described in section 9.1.1 and appendix A.2. One of the major interests of this type of data in the context of this chapter is to provide access to treatment response values otherwise considered as hypothetical (or counterfactual). It is indeed possible to have the response of the same tumor (or more precisely of distinct samples from the same tumor) to different treatments, thus representing proxies for counterfactual variables, as described in Figure 9.1. Availability of these data provides a unique ground truth to assess the validity of proposed causal estimates in a pre-clinical context. Based on the analysis accompanying the published data (Gao et al. 2015), some biomarkers of treatment response have been selected and resulted in an example of treatment algorithm: binimetinib (MEK inhibitor) is recommended to KRAS/BRAF mutated tumors, and BYL719 (alpha-specific PI3K inhibitor, also known as Alpelisib) to PIK3CA mutated tumors. PTEN is also included as a covariate because of its detrimental impact on the response to these two treatments. LEE011 drug (a cell cycle inhibitor also known as Ribociclib) is chosen as the reference drug treatment (\\(k^0\\)). Among the sequenced tumors, 88 are eligible for this precision medicine algorithm (i.e., mutated for BRAF, KRAS or PIK3CA) and have been tested for all 3 drugs of interest, thus ensuring the availability of all corresponding responses. The following analyses will focus exclusively on this sub-cohort for which a descriptive analysis is provided in Figure 9.8A. As expected BRAF/KRAS-mutated tumors have a better response to binimetinib and PIK3CA-mutated tumors have a better response to BYL719 (Figure 9.8B). In addition, it can be noted that these biomarkers have deleterious cross-effects. Figure 9.8: Description of the 88 PDX models cohort. (A) Tissue of origin and prevalences of the drug biomarkers. (B) Drug response to precision medicine targetd treatments in the 88 PDX models cohort depending on the mutational status of biomarkers The analysis settings are similar to the ones used for simulated data. 1000 different cohorts of 70 tumors (out of 88) are sampled without replacement assuming each time that only the response to one of the treatments is known for each tumor, reproducing the classical clinical situation. The distribution of the observed treatments was defined randomly: \\[P[K=k^0]=P[K=k^1_1]=P[K=k^1_2]=\\dfrac{1}{3}\\] It should be noted that, contrary to analyses based on simulated data, all the statistical models used for standardization (outcome model), for the IPW (treatment model) and for the TMLE are no longer generalized linear models (GLM) but random forests (RF). Indeed, it was observed that the performance of GLM-based methods was lower than that of the naive method, supporting the importance of relevant model specification consistent with real data. The RF algorithms then allow to limit misspecification due to the largely non-linear nature of the data. Random forests were chosen for their speed and versatility, especially in view of their ability to handle multinomial classification as well. The results of estimations are then presented in Figure 9.9. In the presence of randomly assigned and balanced observed treatments, none of the methods (including the naive one) has significant systematic bias. On the other hand, more sophisticated methods, and in particular TMLE, allow to reduce the gap between estimates and true values, as visible on the mean absolute errors in Figure 9.9 right column. A similar analysis using the binary version of outcome \\(Y\\) is presented in Béal and Latouche (2020) with similar trends and conclusions, thus supporting the extension of the method to binary outcomes. In the same way as before with the simulated data, it would be possible to study the impact of non-random assignment of the observed treatments, which could systematically bias the results of the naïve methods. Figure 9.9: Causal estimates with PDX data. Distribution and deviation of \\(\\text{CE}_1\\) (A), \\(\\text{CE}_2\\) (B) and \\(\\text{CE}_3\\) (C) estimates based on different methods as in Figure 9.6B. 9.6 Limitations and perspective In synthesis, this work proposes a conceptual framework for evaluating a precision medicine algorithm, taking advantage of data already generated using adapted causal inference tools. However, in a clinical context, these data were not generated in a purely observational manner. Patients were cared for and treated by physicians who probably took into account some of their characteristics. However, the reasoning, formalized or not, behind the physicians’ decisions does not correspond to that which a new investigator might want to test. In the eyes of this new investigator, the data can therefore be considered as observational in that they do not correspond to the randomization he would have liked to have carried out. The possibility for this new investigator to estimate the impact of his PM algorithm using the proposed estimators depends, however, on the consistency, exchangeability and positivity hypotheses. The hypothesis of consistency has been made more plausible by taking into account the treatment versions, which makes it possible to explicit the heterogeneity of the molecules administered. Exchangeability remains questionable. The simulations and calculations described above underline the importance of taking into account at least the genomic covariates used in the processing algorithm. The inclusion of additional covariates is likely to be necessary in many real-world applications. Positivity, on the other hand, can be violated in a much more obvious way in certain situations. Thus, equation (9.5) requires positivity to be extended to versions of treatment: \\(0&lt;P[A=a, K^a=k^a|C]&lt;1\\). If the assignment of the observed treatments was done on a deterministic basis with respect to the variables used by the treatment algorithm, each patient’s molecular profile will have been treated with a single drug, thus preventing any subsequent causal inference within the defined framework. The eventual use, by the boards of physicians in charge of assigning the observed treatments, of variables different from those used by the algorithm could then make it possible to verify the positive condition. But these variables would represent unmeasured confounding factors. It is therefore essential to have an in-depth knowledge of the rationales at work in the assignment of the observed treatments. We developed a user-friendly application that extends the scope of the simulations and makes possible to study and quantify the impact of different situations, including possible (quasi-)violations of positivity or unmeasured confounding. It is thus a tool for empirically framing cases where this causal inference is reasonable or not. The analysis of the PDX data provides an illustration and proof of feasibility for these methods on pre-clinical data, closer to the human clinical data generally of interest. Beyond feasibility, this implementation leads to some remarks. Firstly, the improvement of causal inference methods compared to naive estimation of PM effects is conditioned in this case to the use of flexible and non-linear learning algorithms. This underlines the importance of a proper specification of the outcome and treatment models whose imperfection, especially when trained on small samples, could explain the modesty of the results compared to the simulated data. The particular nature of the PDX data design used should also be kept in mind: each tumor is tested only once for each drug, which may lead to greater variability of results due to tumor heterogeneity (Gao et al. 2015). Some studies, with smaller numbers of tumors and treatments, propose to form groups of several mice for each treatment-drug combination (Hidalgo et al. 2014). The use of these mean effects could contribute to more accurate data. In spite of these limitations, which may diminish their ability to provide values with counterfactual interpretation, PDX data are thus a dataset of interest for studying and validating methods of causal inferences about treatment response. It can also be noted that the very nature of these data, due to the multiplicity of drugs tested for each tumor, can provide a framework in which the constraints of positivity are singularly alleviated. Even if all drugs were not tested on all patients, considering each tumor-drug combination as a different unit increases the coverage of the data. It is then necessary to take into account the clustered nature of the data, each tumor being present several times. Finally, beyond the pre-clinical data presented here, the theoretical framework developed in this article should be more directly applicable to data from clinical trials if these data do not violate the requirements of positivity. If it is necessary to consider several trials, the heterogeneity of practices must be taken into account. The use of different drug lists from one trial to another or from one medical centre to another could also provide an example of confounding factor \\(W\\), included in the theoretical framework presented here but not used in applications. References "],
["conclusion.html", "Conclusion", " Conclusion The aim of this thesis was to trace a path to link the biological knowledge of cancer to the clinical impact through mechanistics models. Among the many possible orientations, it was chosen to take the opposite side of the data-intensive machine learning methods. The main approach proposed uses a qualitative logical formalism and integrates the data by interpreting them rather than by optimizing the parameters with respect to a particular objective. As a result, the resulting personalized mechanical models have proven to be more of an interpretive than a predictive tool. Their versatility and low data requirements nevertheless allow them to be applied to a wide range of questions, particularly concerning the response to treatments that their mechanistic nature facilitates. This seemingly limitless versatility can, however, prove to be a trap because, while all kinds of applications are theoretically possible, the need to rely on detailed biological knowledge and appropriate data limits its scope. In the case of mechanistic molecular signalling models, this interpretative nature of the models is confirmed by statistical analyses. The main value of these models is to provide an understandable framework for extracting relevant biological information in the context of current biological knowledge. The ability of these models to detect emerging non-linear information is also proven, but is rarer and of relatively smaller magnitude. Given the influx of biological knowledge and data, computational models of cancer, with various formalisms, are nevertheless multiplying, particularly with medical aims. In the context of cancer, their use to recommend personalised treatment for each patient is a possible horizon. The evaluation of these models could then become increasingly acute. This thesis proposes the adaptation of causal inference methods in order to simulate their evaluation in clinical trials and to come as close as possible to medical evaluation standards. In a word, cancer models still have a bright future ahead of them. Mechanistic models will continue to be attractive because of their ability not only to predict but, more importantly, to explain. However, the transparency of their mechanisms should not prevent them from being rigorously evaluated statistically. It is not enough for them to explain, they must also be well understood. "],
["appendix-datasets.html", "A About datasets A.1 Cell lines A.2 Patient-derived xenografts A.3 Patients", " A About datasets A.1 Cell lines Several analyses in previous chapters are based on data derived from cell lines. Among the different databases, the ones used in the thesis are briefly described below. Please refer to corresponding references for additional details. A.1.1 Omics profiles The omics profiles of cancer cell lines have been downloaded from Cell Model Passports (Meer et al. 2019) containing genotypic and phenotypic information about more than 1000 cell lines. Among the available data used in this thesis are the exome sequencing, copy number variations and RNA-sequencing. A.1.2 Drug screenings Information about response to treatments is retrieved from Genomics of Drug Sensitivity in Cancer Database (GDSC, Yang et al. (2012)). In order to allow detailed analyses at the level of cancer types, we will restrict ourselves here to tissues represented by at least 20 cell lines and highlighted in dark grey in Figure A.1A. Most of the 663 cell lines in this subcohort have a complete profile with all omics data (mutations, CNA and expression) and drug responses. However, not all cell lines have necessarily been tested for all drugs. Figure A.1: Distribution of cancer types and data types in GDSC-associated dataset. (A) Distribution of cell lines per cancer types, highlighting the ones selected in this thesis with more than 20 cell lines. (B) Availibility of data for the 663 selected cell lines in 17 different cancer types. The cell lines are treated with increasing concentration of drugs and the viability of the cell line relative to untreated control is measured. The dose-response relative viability curve is fitted and then used to compute the half maximal inhibitory concentration (\\(IC_{50}\\)) and the area under the dose-response curve (AUC) (Vis et al. 2016), both being represented in Figure. Since the \\(IC_{50}\\) values are often extrapolated outside the concentration range actually tested, we will focus on the AUC metric for all validation with drug screening data. AUC is a value between 0 and 1: values close to 1 mean that the relative viability has not been decreased, and lower values correspond to increased sensitivity to inhibitions. In cases where the ranges of concentrations tested for different drugs vary, comparison of their AUC values does not have a simple and straightforward interpretation. Figure A.2: Drug screening metrics in cell lines. Based on a tested drug concentration range, \\(IC_{50}\\) and area under the dose-response curve (AUC) can be computed. For a given drug, red AUC corresponds to a more sensitive cell line than blue AUC. A.1.3 CRISPR-Cas9 screening On top the previous drug response characterization, some CRISPR-Cas9 screenings have been performed on cancer cell lines. Very basically, this involves using single-guide RNAs (sgRNAs) to direct the targeted inhibition of certain genes. Conceptually, screening is not very different from drug screening since it allows the sensitivity of cell lines to the inhibition of certain targets to be studied. However, this technology makes it possible to target many more different genes since it is based on RNA guide synthesis and not on the existence of drugs with an affinity for the target of interest. Schematically, sreening is therefore broader (thousands of genes), less biased (any gene can be targeted a priori) and more precise (much lower off-target effect). Among the various databases available, the ones used in this thesis have been downloaded from Cell Model Passports and come from Sanger Institute (Behan et al. 2019) and Broad Institute (Meyers et al. 2017). Both databases present CRISPR inhibition results for thousands of genes for a few hundred cell lines among those presented in the previous section. The Sanger dataset for instance includes 324 cell lines, and 238 in common with the subcohort previously described in the previous section and in Figure A.1. Among the different metrics, the examples presented in this thesis will focus on scaled Bayesian factors to assess the effect of CRISPR targeting of genes. These scores are computed based on the fold change distribution of sgRNA (Hart and Moffat 2016). The highest values indicate that the targeted gene is essential to the cell fitness. A.2 Patient-derived xenografts Another type of data exists, halfway between cell lines and patients, and that is patient-derived xenografts (PDX). Each patient tumour is divided into pieces later implanted in several immunodeficient cloned mice treated with different drugs, thus providing access to sensitivities to several different drugs for each tumour. A.2.1 Overview of PDX data from Gao et al. (2015) The PDX dataset used in this thesis is the one published by Gao et al. (2015). The original dataset contains 281 different tumours of origin (sometimes called PDX models, in the sense of a biological model) and 63 tested drugs, not all drugs having been tested for all tumours and some drugs have been tested with tissue-specific patterns (Figure A.3). 192 of these tumours have also been characterized for their mutations, copy-number alterations and mRNA. More detailed analyses of this dataset are available in the dedicated Github repository, in the file Analysis_PDX.Rmd and its corresponding HTML report. Figure A.3: Comprehensive overview of tumours and drugs screened in PDX dataset from Gao et al. (2015). A.2.2 Drug response metrics A.2.2.1 A continuous outcome The first drug response metric used in this article is called Best Average Response. For each combination tumour/drug, the response is determined by comparing tumor volume change at time \\(t\\), \\(V_t\\) to tumor volume at time \\(t_0\\), \\(V_{t_0}\\). Several scores are computed: \\[\\text{Tumour Volume Change (\\%)} = \\Delta Vol_t = 100\\% \\times \\dfrac{V_t-V_{t_0}}{V_t}\\] \\[\\text{Best Response} = min(\\Delta Vol_t), t&gt;10d\\] \\[\\text{Average Response}_t = mean(\\Delta Vol_i, 0 \\leq i\\leq t)\\] \\[\\text{Best Average Response} = min(\\text{Average Response}_t), t&gt;10d\\] We will mainly focus on Best Average Response. This metric “captures a combination of speed, strength and durability of response into a single value” gao2015high. Qualitatively, lower values correspond to more efficient drugs. A.2.2.2 A binary outcome Thresholds of Best Response and Best Average Response are also defined, inspired by RECIST criteria (Therasse et al. 2000), in order to classify response to treatment into 4 categories: Complete Response (CR), Partial Response (PR, Stable Disease (SD) and Progressive Disease (PD). We designed a binary response status by combining the response categories (CR, PR and SD) into a single ‘responder’ category (1), opposed to the ‘non-responders’ progressive diseases (0). A.3 Patients A.3.1 METABRIC METABRIC dataset is large breast cancer dataset with more than 2000 patients (Pereira et al. 2016). Mutations, CNA, expression (transcriptomics micro-array) and clinical data are available for a majority of patients (Figure A.4A), with 1904 patients for whom all the data is available. One of the particular features of these data is to propose a very long clinical follow-up, over more than 10 years (Figure A.4B). Figure A.4: Available omics and survival in METABRIC Breast Cancer dataset. (A) Number of patients for each omics type and their combinations, depicted as a Venn diagram. (B) Overall survival probability for all patients with clinical follow-up, stratified per breast cancer PAM50 subtype; administrative censoring at 180 months. A.3.2 TCGA: Breast cancer Another reference database for breast cancer is the one from the TCGA consortium (Network and others 2012). The cohort is smaller than METABRIC and its clinical follow-up is more limited. In contrast, the omics data are more comprehensive and include RNA sequencing and relative quantification ofsproteins with RPPA technology (Figure A.5A). Figure A.5: Available omics for TCGA Breast and Prostate cancer. (A) Number of patients for each omics type and their combinations, depicted as a Venn diagram, in TCGA BRCA (Breast Invasive Carcinoma) study. (B) Same for the TCGA PRAD (Prostate Adenocarcinoma) study. A.3.3 TCGA: Prostate cancer Similarly, for prostate cancer, reference can be made to data from the TCGA study (Abeshouse et al. 2015), which has the same type of data but for a smaller number of patients than the breast cancer (Figure A.5B). References "],
["about-logical-models.html", "B About logical models B.1 Generic logical model of cancer pathways B.2 Extended logical model of cancer pathways B.3 Logical model of BRAF pathways in melanoma and colorectal cancer B.4 Logical model of prostate cancer", " B About logical models Several logical models of cancer are used in this thesis and some additional descriptive elements about them are given below. B.1 Generic logical model of cancer pathways For this thesis, a published Boolean model from (Fumia and Martins 2013) has first been used to illustrate our PROFILE methodology. This regulatory network summarizes several key players and pathways involved in cancer mechanisms such as RTKs, PI3K/AKT, WNT/\\(\\beta\\)-catenin, TGF-\\(\\beta\\)/Smads, Rb, HIF-1, p53 and ATM/ATR. An input node Acidosis has been added, along with an output node Proliferation used as a readout for the activity of any of the cyclins (CyclinA, CyclinB, CyclinD and CyclinE). This slightly extended model contains 98 nodes and 254 edges and its inputs are Acidosis, Nutrients, Growth Factors (GFs), Hypoxia, TNFalpha, ROS, PTEN, p14ARF, GLI, FOXO, APC and MAX. Its outputs are Proliferation, Apoptosis, DNA_repair, DNA_damage, VEGF, Lactic_acid, GSH, GLUT1 and COX412. Figure B.1: GINSIM representation of the logical model described in Fumia and Martins (2013). B.2 Extended logical model of cancer pathways Another logical model of similar size and scope was also used, primarily for the study of treatment responses. This model was built by Loïc Verlingue, a medical oncologist and member of the laboratory and preliminary versions of the model are described in Verlingue, Dugourd, et al. (2016) and Verlingue, Calzone, et al. (2016). One of the interests of this model is that it has been designed with a more clinical perspective, notably centred on the response to MTOR inhibitors. In addition, it presents more biological read-outs used for interpretation, and we will use mainly Proliferation (also called G1_S in the moel files to designate the associated stage of the cell cycle), Apoptosis and Quiescence in particular. In addition, being able to discuss and collaborate directly with the model autor has helped to avoid potential errors in use. Figure B.2: GINSIM representation of the ‘Verlingue’ logical model described in Verlingue, Calzone, et al. (2016). B.3 Logical model of BRAF pathways in melanoma and colorectal cancer Here are some details about the regulations represented in Figure 6.4. The MAPK pathway encompasses three families of protein kinases: RAF, MEK, ERK. If RAF is separated into two isoforms, CRAF and BRAF, the other two families MEK and ERK are represented by a single node. When BRAF is inhibited, ERK can still be activated through CRAF, and BRAF binds to and phosphorylates MEK1 and MEK2 more efficiently than CRAF (Wellbrock, Karasarides, and Marais 2004), especially in his V600E/K mutated form. When PI3K/AKT pathway is activated, through the presence of the HGF (Hepatocyte Growth Factors), EGF (Epidermal Growth Factors) and FGF (Fibroblast Growth Factors) ligands, it leads to a proliferative phenotype. The activation of this pathway results in the activation of PDPK1 and mTOR, both able to phosphorylate p70 (RPS6KB1) which then promotes cell proliferation and growth (Consortium 2019). There has been some evidence of negative regulations of these two pathways carried out by ERK itself (Lake, Corrêa, and Müller 2016): phosphorylated ERK is able to prevent the SOS-GRB2 complex formation through the activation of SPRY (Edwin et al. 2009), inhibit the EGF-dependent GAB1/PI3K association (Lehr et al. 2004) and down-regulate EGFR signal through phosphorylation (Lake, Corrêa, and Müller 2016). The model also accounts for a negative regulation of proliferation through a pathway involving p53 activation in response to DNA damage (represented by ATM); p53 hinders proliferation through the activation of both PTEN, a PI3K inhibitor, and p21 (CDKN1A) responsible for cell cycle arrest. We hypothesize that a single network is able to discriminate between melanoma and CRC cells. These differences may come from different sources. One of them is linked to the negative feedback loop from ERK to EGFR. As mentioned previously, this feedback leads to one important difference in response to treatment between melanoma and CRC: \\(BRAF^{(V600E)}\\) inhibition causes a rapid feedback activation of EGFR, which supports continued proliferation. This feedback is observed only in colorectal since melanoma cells express low levels of EGFR and are therefore not subject to this reactivation (Prahallad et al. 2012). Moreover, phosphorylation of SOX10 by ERK inhibits its transcription activity towards multiple target genes by interfering with the sumoylation of SOX10 at K55, which is essential for its transcriptional activity (Han et al. 2018). The absence of ERK releases the activity of SOX10, which is necessary and sufficient for FOXD3 induction. FOXD3 is then able to directly activate the expression of ERBB3 at the transcriptional level, enhancing the responsiveness of melanoma cells to NRG1 (the ligand for ERBB3), and thus leading to the reactivation of both MAPK and PI3K/AKT pathways (Han et al. 2018). Furthermore, it has been shown that in colorectal cells, FOXD3 inhibits EGFR signal in vitro (Li et al. 2017). Interestingly, SOX10 is highly expressed in melanoma cell lines when compared to other cancer cells. In the model, we define SOX10 as an input because of the lack of information about the regulatory mechanisms controlling its activity. The different expression levels of SOX10 have been reported to play an important role in melanoma (high expression) and colorectal (low expression) cell lines. Besides a list of formalized biological assertions, retrieved from literature, has been used during the model building to ensure the consitency of the model with some qualitative behaviours. These assertions, listed below, are all verified when the logical model is simulated (details are available on the corresponding GitHub repository): BRAF inhibition causes a feedback activation of EGFR in colorectal cancer and not in melanoma (Prahallad et al. 2012) MEK inhibition stops ERK signal but activates the PI3K/Akt pathway and increases the activity of ERBB3 (Gopal et al. 2010; Lake, Corrêa, and Müller 2016) HGF signal leads to the reactivation of the MAPK and PI3K/AKT pathways, and resistance to BRAF inhibition (Wroblewski et al. 2013) BRAF inhibition in melanoma activates the SOX10/FOXD3/ERBB3 axis, which mediates resistance through the activation of the PI3K/AKT pathway (Han et al. 2018) Overexpression/mutation of CRAF results in constitutive activation of ERK and MEK also in the presence of a BRAF inhibitor [Manzano et al. (2016); johannessen2010cot] Early resistance to BRAF inhibition may be observed in case of PTEN loss, or mutations in PI3K or AKT (Manzano et al. 2016) Experiments in melanoma cell lines support combined treatment with BRAF/MEK + PI3K/AKT inhibitors to overcome resistance (Manzano et al. 2016) BRAF inhibition (Vemurafenib) leads to the induction of PI3K/AKT pathway and inhibition of EGFR did not block this induction (Corcoran et al. 2012) Induction of PI3K/AKT pathway signaling has been associated with decreased sensitivity to MAPK inhibition (Corcoran et al. 2012) B.4 Logical model of prostate cancer In the context of the European project PRECISE (Personalized Engine for Cancer Integrative Study and Evaluation), focused on the integrative study of prostate cancer, an adapted logical model has been built. This prostate cancer model is initially based on the generic structure of the Fumia model presented in section B.1, which has been considerably enriched and extended with genes and mechanisms specific to prostate cancer such as ERG, SPOP or AR. The model contains 133 nodes and 449 edges (Figure B.3) and includes pathways like androgen receptor and growth factor signalling, several signaling pathways (Wnt, NFkB, PI3K/AKT, MAPK, mTOR, SHH), cell cycle, epithelial-mesenchymal transition (EMT), Apoptosis, DNA damage, etc. The model has 9 inputs (EGF, FGF, TGF beta, Nutrients, Hypoxia, Acidosis, Androgen, TNF alpha and Carcinogen presence) and 6 outputs (Proliferation, Apoptosis, Invasion, Migration, (bone) Metastasis and DNA repair). Figure B.3: GINSIM representation of the ‘Montagud’ logical model of prostate cancer. References "],
["about-statistics.html", "C About statistics C.1 Decomposition of \\(R^2\\) C.2 Causal inference with multiple versions of treatment", " C About statistics C.1 Decomposition of \\(R^2\\) The decomposition of \\(R^2\\) according to the method of Lindeman (1980) is detailed below. The presentation is taken directly from Grömping and others (2006). A linear model is written \\(y_i=\\beta_0+\\beta_1x_{i1}+...+\\beta_px_{ip}+e_i\\) and the corresponding \\(R^2\\) is: \\[R^2=\\dfrac{\\sum_{i=1}^{n} (\\hat{y_i}-\\bar{y_i})^2}{\\sum_{i=1}^{n} (y_i-\\bar{y_i})^2}\\] Additionally, we define \\(R^2(S)\\) for a model with regressors in set S. The additional \\(R^2\\) when adding the regressors in set \\(M\\) to a model with the regressors in set \\(S\\) is given as: \\[seqR^2(M|S)=R^2(M\\cup S)-R^2(S)\\] The order of the regressors in any model is a permutation of the available regressors \\(x_1, ..., x_p\\) and is denoted by the tuple of indices \\(r = (r_1, ..., r_p)\\). Let \\(S_k(r)\\) denote the set of regressors entered into the model before regressor \\(x_k\\) in the order \\(r\\). Then the portion of \\(R^2\\) allocated to regressor \\(x_k\\) in the order \\(r\\) can be written as \\[seqR^2(\\{x_k\\}|S_k(r))=R^2(\\{x_k\\}\\cup S_k(r))-R^2(S_k(r))\\] All in all, the \\(R^2\\) allocated to \\(x_k\\) after decomposition is: \\[R^2_{decomp}(x_k)=\\dfrac{1}{p!}\\sum_{r\\text{ permutations}}seqR^2(\\{x_k\\}|r)\\] C.2 Causal inference with multiple versions of treatment This section gathers the demonstrations of the equations present in chapter 9 when they are not present in this chapter and additional details about other estimators based on IPW and TMLE. C.2.1 Overall treatment effect with multiple versions of treatment (equation (9.3)) Here is the formal proof for equation (9.3), mostly derived from the proof of Proposition 3 in (VanderWeele and Hernan 2013). \\[\\begin{equation*} \\begin{aligned} E[ &amp; Y(a, K^a(a))] = E[Y(a)] &amp;&amp;K^a \\text{ actually received}\\\\ &amp; = \\sum_{c,w} E[Y(a)|c,w] \\times P[c,w]&amp;&amp; \\\\ &amp; = \\sum_{c,w} E[Y(a)|a,c,w] \\times P[c,w] &amp;&amp; Y(a) \\perp \\!\\!\\! \\perp A | (C,W)\\\\ &amp; = \\sum_{c,w} E[Y(a, K^a(a))|a,c,w] \\times P[c,w]&amp;&amp; \\\\ &amp; = \\sum_{c,w,k^a} E[Y(a, k^a)|a,K^a(a)=k^a,c,w]\\times P[K^a(a)=k^a|a,c,w] \\times P[c,w] &amp;&amp;\\\\ &amp; = \\sum_{c,w,k^a} E[Y(a, k^a)|a,K^a=k^a,c,w]\\times P[K^a=k^a|a,c,w] \\times P[c,w] &amp;&amp;\\text{consistency K} \\\\ &amp; = \\sum_{c,w,k^a} E[Y|a,K^a=k^a,c,w]\\times P[K^a=k^a|a,c,w] \\times P[c,w] &amp;&amp;\\text{consistency Y} \\\\ &amp; = \\sum_{c,w} E[Y|a,c,w]\\times P[c,w] \\end{aligned} \\end{equation*}\\] Then, the overall treatment effect can be defined and computed by: \\[E[Y(a, K^a(a))] - E[Y(a^*, K^{a^*}(a^*))]\\] C.2.2 Treatment effect with predefined distributions of versions of treatment (equation (9.5)) Here is the formal proof for equation (9.5), partially derived from the proof of Proposition 5 in (VanderWeele and Hernan 2013). \\[\\begin{equation*} \\begin{aligned} E[ &amp; Y(a, G^a)] = \\sum_{c} E[Y(a, G^a)|C=c] \\times P[c]\\\\ &amp; = \\sum_{c, k^a} E[Y(a, k^a)|G^a=k^a, C=c] \\times P[G^a=k^a|C=c] \\times P[c]\\\\ &amp; = \\sum_{c, k^a} E[Y(a, k^a)| C=c] \\times g^{k^a,c} \\times P[c] &amp;&amp;\\text{since } P[G^a=k^a] = g^{k^a,c}\\\\ &amp; = \\sum_{c, k^a} E[Y(a, k^a)| A=a, K^a=k^a, C=c] \\times g^{k^a,c} \\times P[c] &amp;&amp;\\text{with } Y(a,k^a) \\perp \\!\\!\\! \\perp \\{A,K\\} | C\\\\ &amp; = \\sum_{c, k^a} E[Y| A=a, K^a=k^a, C=c] \\times P[c] &amp;&amp;\\text{by consistency for Y} \\end{aligned} \\end{equation*}\\] C.2.3 Inverse probability of treatment weighted (IPW) estimators for precision medicine An extension of IPW methods described in section 8.2.2.2 to multi-valued treatments (only treatment \\(K\\) with different modalities and no \\(A\\)) has already been studied and the different formulas and estimators adapted accordingly [Imbens (2000); feng2012generalized], defining in particular a generalized propensity score: \\[f(k|c)=P[K=k|C=c]=E[I(k)|C=c]\\] \\[\\begin{equation*} \\text{with } I(k) = \\left\\{ \\begin{array}{ll} 1 &amp; \\quad \\text{if } K = k \\\\ 0 &amp; \\quad \\text{otherwise} \\end{array} \\right. \\end{equation*}\\] and a subsequent estimator: \\[E[Y(k)]=\\dfrac{\\hat{E}[I(K=k)W^{K}Y]}{\\hat{E}[I(K=k)W^K]} \\text{ with } W^K=\\dfrac{1}{f[K|C]}\\] In our precision medicine settings, to be consistent with the previously defined causal diagram (Figure 9.5), we have both \\(A\\), binary status depending on the class of drugs, and \\(K\\), the multinomial variable for versions of treatments, i.e., the precise drug. Therefore we need to define a slightly different propensity score with joint probabilities: \\[\\begin{equation*} \\begin{aligned} f(a,k|c) &amp; =P[A=a,K=k|C=c]\\\\ &amp; =P[K=k|A=a, C=c].P[A=a|C=c]\\\\ &amp; =E[I(a,k)|C=c] \\end{aligned} \\end{equation*}\\] \\[\\begin{equation*} \\text{with } I(a,k) = \\left\\{ \\begin{array}{ll} 1 &amp; \\quad \\text{if } A = a, K = k \\\\ 0 &amp; \\quad \\text{otherwise} \\end{array} \\right. \\end{equation*}\\] From this we can deduce the estimator: \\[E[Y(a, k)]=\\dfrac{\\hat{E}[I(A=a,K=k)W^{A,K}Y]}{\\hat{E}[I(A=a,K=k)W^{A,K}]} \\text{ with } W^{A,K}=\\dfrac{1}{f[A,K|C]}\\] In all the examples presented in this study and implemented in the code, \\(\\mathcal{K}^{0} \\cap \\mathcal{K}^{1} = \\emptyset\\), it is therefore possible to simplify the joint probabilities since the knowledge of K automatically results in the knowledge of A allowing \\(P[A=a, K=k|C=c]=P[K=k|C=c]\\). The above formulas with the attached probabilities are still necessary in the general case and allow for the derivation of causal effects \\(\\text{CE}_1\\), \\(\\text{CE}_2\\) and \\(\\text{CE}_3\\) previously described. C.2.4 TMLE Targeted maximum likelihood estimation is framework based on a doubly robust maximum-likelihood–based approach that includes a “targeting” step that optimizes the bias-variance trade-off for a defined target parameter. In particular, this method is perfectly compatible with the use of machine learning algorithms for outcome or treatment models. A detailed description of the method and its implementations can be found in Van der Laan and Rose (2011). The implementation proposed in this article is very similar to the one proposed in a recent tutorial concerning the application to binary processing (Luque-Fernandez et al. 2018). The specific characteristics of the problem of precision medicine studied here lead to modify this approach. In particular, the outcome and treatment models used in the first steps are modified in the same way as the one explained for the standardized estimators (outcome model) and for the IPW estimators (treatment model). The step of updating the estimates is done on a model similar to Luque-Fernandez et al. (2018). The algorithm used for the models internal to the TMLE are, as much as possible, the same as those used for the standardised and IPW estimators: For simulated data: generalized linear models in all cases except multinomial classification performed through the function in package. For PDX data: random forests for all models. Use of SuperLearner (Van der Laan, Polley, and Hubbard 2007) is made possible by simple modifications to the code but significantly slows down its execution. References "],
["references.html", "References", " References "]
]
